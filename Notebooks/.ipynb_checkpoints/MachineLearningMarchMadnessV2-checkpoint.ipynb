{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #dataframes\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np # n-dim object support\n",
    "# do ploting inline instead of in a separate window\n",
    "%matplotlib inline\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_school = pd.read_csv(\"../Scraper/school_records.csv\")\n",
    "df_ps_game = pd.read_csv(\"../Scraper/post_season_game_records.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3125, 21)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_school.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(441, 7)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ps_game.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>team_name</th>\n",
       "      <th>fg_pg</th>\n",
       "      <th>ft_pg</th>\n",
       "      <th>three_pt_pg</th>\n",
       "      <th>orb_pg</th>\n",
       "      <th>drb_pg</th>\n",
       "      <th>ast_pg</th>\n",
       "      <th>stl_pg</th>\n",
       "      <th>blk_pg</th>\n",
       "      <th>...</th>\n",
       "      <th>pf_pg</th>\n",
       "      <th>pt_pg</th>\n",
       "      <th>opnt_pt_pg</th>\n",
       "      <th>fg_pct</th>\n",
       "      <th>three_p_pct</th>\n",
       "      <th>ft_pct</th>\n",
       "      <th>wl_pct</th>\n",
       "      <th>conf_wl_pct</th>\n",
       "      <th>srs</th>\n",
       "      <th>sos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010.0</td>\n",
       "      <td>Air Force</td>\n",
       "      <td>20.387097</td>\n",
       "      <td>10.741935</td>\n",
       "      <td>5.677419</td>\n",
       "      <td>7.096774</td>\n",
       "      <td>27.222685</td>\n",
       "      <td>12.548387</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.645161</td>\n",
       "      <td>...</td>\n",
       "      <td>17.645161</td>\n",
       "      <td>57.193548</td>\n",
       "      <td>63.129032</td>\n",
       "      <td>0.443</td>\n",
       "      <td>0.313</td>\n",
       "      <td>0.635</td>\n",
       "      <td>0.323</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>-4.90</td>\n",
       "      <td>3.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010.0</td>\n",
       "      <td>Akron</td>\n",
       "      <td>25.057143</td>\n",
       "      <td>13.800000</td>\n",
       "      <td>6.714286</td>\n",
       "      <td>13.342857</td>\n",
       "      <td>35.875918</td>\n",
       "      <td>13.514286</td>\n",
       "      <td>6.085714</td>\n",
       "      <td>3.257143</td>\n",
       "      <td>...</td>\n",
       "      <td>19.485714</td>\n",
       "      <td>70.628571</td>\n",
       "      <td>65.514286</td>\n",
       "      <td>0.433</td>\n",
       "      <td>0.339</td>\n",
       "      <td>0.657</td>\n",
       "      <td>0.686</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>2.82</td>\n",
       "      <td>-1.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010.0</td>\n",
       "      <td>Alabama A&amp;M</td>\n",
       "      <td>22.185185</td>\n",
       "      <td>17.481481</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>13.925926</td>\n",
       "      <td>36.669410</td>\n",
       "      <td>10.666667</td>\n",
       "      <td>9.222222</td>\n",
       "      <td>5.296296</td>\n",
       "      <td>...</td>\n",
       "      <td>20.370370</td>\n",
       "      <td>65.851852</td>\n",
       "      <td>69.666667</td>\n",
       "      <td>0.382</td>\n",
       "      <td>0.291</td>\n",
       "      <td>0.635</td>\n",
       "      <td>0.407</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>-20.19</td>\n",
       "      <td>-13.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010.0</td>\n",
       "      <td>Alabama-Birmingham</td>\n",
       "      <td>22.441176</td>\n",
       "      <td>16.852941</td>\n",
       "      <td>5.205882</td>\n",
       "      <td>12.352941</td>\n",
       "      <td>36.342561</td>\n",
       "      <td>11.470588</td>\n",
       "      <td>6.558824</td>\n",
       "      <td>2.676471</td>\n",
       "      <td>...</td>\n",
       "      <td>17.970588</td>\n",
       "      <td>66.941176</td>\n",
       "      <td>60.382353</td>\n",
       "      <td>0.422</td>\n",
       "      <td>0.311</td>\n",
       "      <td>0.694</td>\n",
       "      <td>0.735</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>9.46</td>\n",
       "      <td>2.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010.0</td>\n",
       "      <td>Alabama State</td>\n",
       "      <td>21.516129</td>\n",
       "      <td>15.290323</td>\n",
       "      <td>6.129032</td>\n",
       "      <td>12.903226</td>\n",
       "      <td>35.099896</td>\n",
       "      <td>12.903226</td>\n",
       "      <td>7.354839</td>\n",
       "      <td>4.161290</td>\n",
       "      <td>...</td>\n",
       "      <td>20.451613</td>\n",
       "      <td>64.451613</td>\n",
       "      <td>65.903226</td>\n",
       "      <td>0.404</td>\n",
       "      <td>0.324</td>\n",
       "      <td>0.641</td>\n",
       "      <td>0.516</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>-14.41</td>\n",
       "      <td>-12.02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     year           team_name      fg_pg      ft_pg  three_pt_pg     orb_pg  \\\n",
       "0  2010.0           Air Force  20.387097  10.741935     5.677419   7.096774   \n",
       "1  2010.0               Akron  25.057143  13.800000     6.714286  13.342857   \n",
       "2  2010.0         Alabama A&M  22.185185  17.481481     4.000000  13.925926   \n",
       "3  2010.0  Alabama-Birmingham  22.441176  16.852941     5.205882  12.352941   \n",
       "4  2010.0       Alabama State  21.516129  15.290323     6.129032  12.903226   \n",
       "\n",
       "      drb_pg     ast_pg    stl_pg    blk_pg  ...      pf_pg      pt_pg  \\\n",
       "0  27.222685  12.548387  5.000000  1.645161  ...  17.645161  57.193548   \n",
       "1  35.875918  13.514286  6.085714  3.257143  ...  19.485714  70.628571   \n",
       "2  36.669410  10.666667  9.222222  5.296296  ...  20.370370  65.851852   \n",
       "3  36.342561  11.470588  6.558824  2.676471  ...  17.970588  66.941176   \n",
       "4  35.099896  12.903226  7.354839  4.161290  ...  20.451613  64.451613   \n",
       "\n",
       "   opnt_pt_pg  fg_pct  three_p_pct  ft_pct  wl_pct  conf_wl_pct    srs    sos  \n",
       "0   63.129032   0.443        0.313   0.635   0.323     0.062500  -4.90   3.13  \n",
       "1   65.514286   0.433        0.339   0.657   0.686     0.750000   2.82  -1.50  \n",
       "2   69.666667   0.382        0.291   0.635   0.407     0.444444 -20.19 -13.71  \n",
       "3   60.382353   0.422        0.311   0.694   0.735     0.687500   9.46   2.90  \n",
       "4   65.903226   0.404        0.324   0.641   0.516     0.666667 -14.41 -12.02  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_school.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>team_1_name</th>\n",
       "      <th>team_1_score</th>\n",
       "      <th>team_1_seed</th>\n",
       "      <th>team_2_name</th>\n",
       "      <th>team_2_score</th>\n",
       "      <th>team_2_seed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011</td>\n",
       "      <td>UTSA</td>\n",
       "      <td>46.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>Ohio State</td>\n",
       "      <td>75.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011</td>\n",
       "      <td>George Mason</td>\n",
       "      <td>61.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Villanova</td>\n",
       "      <td>57.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011</td>\n",
       "      <td>Clemson</td>\n",
       "      <td>76.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>West Virginia</td>\n",
       "      <td>84.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011</td>\n",
       "      <td>Princeton</td>\n",
       "      <td>57.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Kentucky</td>\n",
       "      <td>59.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011</td>\n",
       "      <td>Marquette</td>\n",
       "      <td>66.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>Xavier</td>\n",
       "      <td>55.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year   team_1_name  team_1_score  team_1_seed    team_2_name  team_2_score  \\\n",
       "0  2011          UTSA          46.0         16.0     Ohio State          75.0   \n",
       "1  2011  George Mason          61.0          8.0      Villanova          57.0   \n",
       "2  2011       Clemson          76.0         12.0  West Virginia          84.0   \n",
       "3  2011     Princeton          57.0         13.0       Kentucky          59.0   \n",
       "4  2011     Marquette          66.0         11.0         Xavier          55.0   \n",
       "\n",
       "   team_2_seed  \n",
       "0          1.0  \n",
       "1          9.0  \n",
       "2          5.0  \n",
       "3          4.0  \n",
       "4          6.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ps_game.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_school.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ps_game.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_team_name(team_name):\n",
    "    #Apply hard-coded corrections to team names\n",
    "    team_name_dict = {'Colorado-Colorado Springs':'Colorado',\n",
    "                     'Colorado College': 'Colorado',\n",
    "                     'UNC':'North Carolina',\n",
    "                     'UConn':'Connecticut',\n",
    "                     'LIU-Brooklyn':'Long Island University',\n",
    "                     'UTSA':'Texas-San Antonio',\n",
    "                     'Pitt':'Pittsburgh',\n",
    "                     'BYU':'Brigham Young',\n",
    "                     \"St. Peter's\": \"Saint Peter's\",\n",
    "                     'VCU':'Virginia Commonwealth',\n",
    "                     'Southern Miss':'Southern Mississippi',\n",
    "                     'Detroit': 'Detroit Mercy',\n",
    "                     'UNLV':'Nevada-Las Vegas',\n",
    "                     'Ole Miss':'Mississippi',\n",
    "                     \"St. Joseph's\":\"Saint Joseph's\",\n",
    "                     'UCSB':'UC-Santa Barbara',\n",
    "                     'SMU': 'Southern Methodist',\n",
    "                     'USC':'South Carolina',\n",
    "                     'LSU':'Louisiana State',\n",
    "                     'UMass':'Massachusetts',\n",
    "                     'ETSU':'East Tennessee State'}\n",
    "    # TODO: for V2 add more corrections to the team_name_dict\n",
    "    if(team_name in team_name_dict):\n",
    "        return team_name_dict[team_name]\n",
    "    return team_name\n",
    "def get_school_stats(year, team_name):\n",
    "    return df_school[(df_school['year'] == year) & (df_school['team_name'] == team_name)]\n",
    "def get_vals(t_stats_list, key):\n",
    "    ret = []\n",
    "    for t_stat in t_stats_list:\n",
    "        ret.append(t_stat[key].squeeze())\n",
    "    return ret\n",
    "def get_team_stats_dict_with_t1_win(t1_stats, t2_stats, t1_wins):\n",
    "    return {'team_name_1':get_vals(t1_stats,'team_name'),'fg_pg_1':get_vals(t1_stats,'fg_pg'),'ft_pg_1':get_vals(t1_stats,'ft_pg'),\n",
    "            'three_pt_pg_1':get_vals(t1_stats,'three_pt_pg'),'orb_pg_1':get_vals(t1_stats,'orb_pg'),'drb_pg_1':get_vals(t1_stats,'drb_pg'),\n",
    "            'ast_pg_1':get_vals(t1_stats,'ast_pg'),'stl_pg_1':get_vals(t1_stats,'stl_pg'),'blk_pg_1':get_vals(t1_stats,'blk_pg'),\n",
    "            'tov_pg_1':get_vals(t1_stats,'tov_pg'),'pf_pg_1':get_vals(t1_stats,'pf_pg'), 'pt_pg_1':get_vals(t1_stats,'pt_pg'),\n",
    "            'opnt_pt_pg_1':get_vals(t1_stats,'opnt_pt_pg'),'fg_pct_1':get_vals(t1_stats,'fg_pct'),'three_p_pct_1':get_vals(t1_stats,'three_p_pct'),\n",
    "            'ft_pct_1':get_vals(t1_stats,'ft_pct'),'wl_pct_1':get_vals(t1_stats,'wl_pct'),'conf_wl_pct_1':get_vals(t1_stats,'conf_wl_pct'),\n",
    "            'srs_1':get_vals(t1_stats,'srs'),'sos_1':get_vals(t1_stats,'sos'),\n",
    "            'team_name_2':get_vals(t2_stats,'team_name'),'fg_pg_2':get_vals(t2_stats,'fg_pg'),'ft_pg_2':get_vals(t2_stats,'ft_pg'),\n",
    "            'three_pt_pg_2':get_vals(t2_stats,'three_pt_pg'),'orb_pg_2':get_vals(t2_stats,'orb_pg'),'drb_pg_2':get_vals(t2_stats,'drb_pg'),\n",
    "            'ast_pg_2':get_vals(t2_stats,'ast_pg'),'stl_pg_2':get_vals(t2_stats,'stl_pg'),'blk_pg_2':get_vals(t2_stats,'blk_pg'),\n",
    "            'tov_pg_2':get_vals(t2_stats,'tov_pg'),'pf_pg_2':get_vals(t2_stats,'pf_pg'), 'pt_pg_2':get_vals(t2_stats,'pt_pg'),\n",
    "            'opnt_pt_pg_2':get_vals(t2_stats,'opnt_pt_pg'),'fg_pct_2':get_vals(t2_stats,'fg_pct'),'three_p_pct_2':get_vals(t2_stats,'three_p_pct'),\n",
    "            'ft_pct_2':get_vals(t2_stats,'ft_pct'),'wl_pct_2':get_vals(t2_stats,'wl_pct'),'conf_wl_pct_2':get_vals(t2_stats,'conf_wl_pct'),\n",
    "            'srs_2':get_vals(t2_stats,'srs'),'sos_2':get_vals(t2_stats,'sos'),\n",
    "            't1_win':t1_wins}\n",
    "def get_team_stats_dict(t1_stats, t2_stats):\n",
    "    return {'team_name_1':get_vals(t1_stats,'team_name'),'fg_pg_1':get_vals(t1_stats,'fg_pg'),'ft_pg_1':get_vals(t1_stats,'ft_pg'),\n",
    "            'three_pt_pg_1':get_vals(t1_stats,'three_pt_pg'),'orb_pg_1':get_vals(t1_stats,'orb_pg'),'drb_pg_1':get_vals(t1_stats,'drb_pg'),\n",
    "            'ast_pg_1':get_vals(t1_stats,'ast_pg'),'stl_pg_1':get_vals(t1_stats,'stl_pg'),'blk_pg_1':get_vals(t1_stats,'blk_pg'),\n",
    "            'tov_pg_1':get_vals(t1_stats,'tov_pg'),'pf_pg_1':get_vals(t1_stats,'pf_pg'), 'pt_pg_1':get_vals(t1_stats,'pt_pg'),\n",
    "            'opnt_pt_pg_1':get_vals(t1_stats,'opnt_pt_pg'),'fg_pct_1':get_vals(t1_stats,'fg_pct'),'three_p_pct_1':get_vals(t1_stats,'three_p_pct'),\n",
    "            'ft_pct_1':get_vals(t1_stats,'ft_pct'),'wl_pct_1':get_vals(t1_stats,'wl_pct'),'conf_wl_pct_1':get_vals(t1_stats,'conf_wl_pct'),\n",
    "            'srs_1':get_vals(t1_stats,'srs'),'sos_1':get_vals(t1_stats,'sos'),\n",
    "            'team_name_2':get_vals(t2_stats,'team_name'),'fg_pg_2':get_vals(t2_stats,'fg_pg'),'ft_pg_2':get_vals(t2_stats,'ft_pg'),\n",
    "            'three_pt_pg_2':get_vals(t2_stats,'three_pt_pg'),'orb_pg_2':get_vals(t2_stats,'orb_pg'),'drb_pg_2':get_vals(t2_stats,'drb_pg'),\n",
    "            'ast_pg_2':get_vals(t2_stats,'ast_pg'),'stl_pg_2':get_vals(t2_stats,'stl_pg'),'blk_pg_2':get_vals(t2_stats,'blk_pg'),\n",
    "            'tov_pg_2':get_vals(t2_stats,'tov_pg'),'pf_pg_2':get_vals(t2_stats,'pf_pg'), 'pt_pg_2':get_vals(t2_stats,'pt_pg'),\n",
    "            'opnt_pt_pg_2':get_vals(t2_stats,'opnt_pt_pg'),'fg_pct_2':get_vals(t2_stats,'fg_pct'),'three_p_pct_2':get_vals(t2_stats,'three_p_pct'),\n",
    "            'ft_pct_2':get_vals(t2_stats,'ft_pct'),'wl_pct_2':get_vals(t2_stats,'wl_pct'),'conf_wl_pct_2':get_vals(t2_stats,'conf_wl_pct'),\n",
    "            'srs_2':get_vals(t2_stats,'srs'),'sos_2':get_vals(t2_stats,'sos')}\n",
    "def get_team_stats_dict_ps(t1_stats, t2_stats, t1_seeds, t2_seeds):\n",
    "    return {'team_name_1':get_vals(t1_stats,'team_name'),'fg_pg_1':get_vals(t1_stats,'fg_pg'),'ft_pg_1':get_vals(t1_stats,'ft_pg'),\n",
    "            'three_pt_pg_1':get_vals(t1_stats,'three_pt_pg'),'orb_pg_1':get_vals(t1_stats,'orb_pg'),'drb_pg_1':get_vals(t1_stats,'drb_pg'),\n",
    "            'ast_pg_1':get_vals(t1_stats,'ast_pg'),'stl_pg_1':get_vals(t1_stats,'stl_pg'),'blk_pg_1':get_vals(t1_stats,'blk_pg'),\n",
    "            'tov_pg_1':get_vals(t1_stats,'tov_pg'),'pf_pg_1':get_vals(t1_stats,'pf_pg'), 'pt_pg_1':get_vals(t1_stats,'pt_pg'),\n",
    "            'opnt_pt_pg_1':get_vals(t1_stats,'opnt_pt_pg'),'fg_pct_1':get_vals(t1_stats,'fg_pct'),'three_p_pct_1':get_vals(t1_stats,'three_p_pct'),\n",
    "            'ft_pct_1':get_vals(t1_stats,'ft_pct'),'wl_pct_1':get_vals(t1_stats,'wl_pct'),'conf_wl_pct_1':get_vals(t1_stats,'conf_wl_pct'),\n",
    "            'srs_1':get_vals(t1_stats,'srs'),'sos_1':get_vals(t1_stats,'sos'),\n",
    "            'team_name_2':get_vals(t2_stats,'team_name'),'fg_pg_2':get_vals(t2_stats,'fg_pg'),'ft_pg_2':get_vals(t2_stats,'ft_pg'),\n",
    "            'three_pt_pg_2':get_vals(t2_stats,'three_pt_pg'),'orb_pg_2':get_vals(t2_stats,'orb_pg'),'drb_pg_2':get_vals(t2_stats,'drb_pg'),\n",
    "            'ast_pg_2':get_vals(t2_stats,'ast_pg'),'stl_pg_2':get_vals(t2_stats,'stl_pg'),'blk_pg_2':get_vals(t2_stats,'blk_pg'),\n",
    "            'tov_pg_2':get_vals(t2_stats,'tov_pg'),'pf_pg_2':get_vals(t2_stats,'pf_pg'), 'pt_pg_2':get_vals(t2_stats,'pt_pg'),\n",
    "            'opnt_pt_pg_2':get_vals(t2_stats,'opnt_pt_pg'),'fg_pct_2':get_vals(t2_stats,'fg_pct'),'three_p_pct_2':get_vals(t2_stats,'three_p_pct'),\n",
    "            'ft_pct_2':get_vals(t2_stats,'ft_pct'),'wl_pct_2':get_vals(t2_stats,'wl_pct'),'conf_wl_pct_2':get_vals(t2_stats,'conf_wl_pct'),\n",
    "            'srs_2':get_vals(t2_stats,'srs'),'sos_2':get_vals(t2_stats,'sos'), 'team_1_seed':t1_seeds,\n",
    "            'team_2_seed':t2_seeds}\n",
    "def create_team_stats_df_w_t1_win(indeces_w_stats, t1_stats_list, t2_stats_list,t1_wins):\n",
    "    # Adds column for wether team 1 wins or not\n",
    "    # Assumes all lists are of the same length\n",
    "    return pd.DataFrame(get_team_stats_dict_with_t1_win(t1_stats_list, t2_stats_list,t1_wins), index = indeces_w_stats)\n",
    "def create_team_stats_df(indeces_w_stats, t1_stats_list, t2_stats_list):\n",
    "    # Assumes all lists are of the same length\n",
    "    return pd.DataFrame(get_team_stats_dict(t1_stats_list, t2_stats_list), index = indeces_w_stats)\n",
    "def create_team_stats_df_ps(indeces_w_stats, t1_stats_list, t2_stats_list, t1_seeds, t2_seeds):\n",
    "    # Only uses post season stats => inclu\n",
    "    # Assumes all lists are of the same length\n",
    "    return pd.DataFrame(get_team_stats_dict_ps(t1_stats_list, t2_stats_list, t1_seeds, t2_seeds), index = indeces_w_stats)\n",
    "def get_team_stats_df(game_df, should_print=False):\n",
    "    indeces_w_stats = []\n",
    "    t1_stats_list = []\n",
    "    t2_stats_list = []\n",
    "    t1_wins_list = []\n",
    "    for index, row in game_df.iterrows():\n",
    "        year = row['year']\n",
    "        team_1 = row['team_1_name']\n",
    "        team_2 = row['team_2_name']\n",
    "        team_1_score = row['team_1_score']\n",
    "        team_2_score = row['team_2_score']\n",
    "        t1_stats = get_school_stats(year, resolve_team_name(team_1))\n",
    "        t2_stats = get_school_stats(year, resolve_team_name(team_2))\n",
    "\n",
    "        if(len(t1_stats) > 0 and len(t2_stats) > 0):  \n",
    "            indeces_w_stats.append(index)\n",
    "            t1_stats_list.append(t1_stats)\n",
    "            t2_stats_list.append(t2_stats)\n",
    "            t1_wins_list.append(team_1_score > team_2_score)\n",
    "        else:         \n",
    "            if(should_print):\n",
    "                print(year)\n",
    "                if(len(t1_stats) < 1):\n",
    "                    print(team_1)\n",
    "                if(len(t2_stats) < 1):\n",
    "                    print(team_2)\n",
    "            \n",
    "    print(len(indeces_w_stats))\n",
    "    team_stats_df = create_team_stats_df_w_t1_win(indeces_w_stats, t1_stats_list, t2_stats_list, t1_wins_list)\n",
    "    return team_stats_df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "441\n"
     ]
    }
   ],
   "source": [
    "ps_team_stats_df = get_team_stats_df(df_ps_game, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_game_w_team_stats = pd.concat([df_ps_game, ps_team_stats_df], axis=1, join='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>team_1_name</th>\n",
       "      <th>team_1_score</th>\n",
       "      <th>team_1_seed</th>\n",
       "      <th>team_2_name</th>\n",
       "      <th>team_2_score</th>\n",
       "      <th>team_2_seed</th>\n",
       "      <th>team_name_1</th>\n",
       "      <th>fg_pg_1</th>\n",
       "      <th>ft_pg_1</th>\n",
       "      <th>...</th>\n",
       "      <th>pt_pg_2</th>\n",
       "      <th>opnt_pt_pg_2</th>\n",
       "      <th>fg_pct_2</th>\n",
       "      <th>three_p_pct_2</th>\n",
       "      <th>ft_pct_2</th>\n",
       "      <th>wl_pct_2</th>\n",
       "      <th>conf_wl_pct_2</th>\n",
       "      <th>srs_2</th>\n",
       "      <th>sos_2</th>\n",
       "      <th>t1_win</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011</td>\n",
       "      <td>UTSA</td>\n",
       "      <td>46.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>Ohio State</td>\n",
       "      <td>75.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Texas-San Antonio</td>\n",
       "      <td>23.588235</td>\n",
       "      <td>16.058824</td>\n",
       "      <td>...</td>\n",
       "      <td>77.135135</td>\n",
       "      <td>59.675676</td>\n",
       "      <td>0.494</td>\n",
       "      <td>0.423</td>\n",
       "      <td>0.701</td>\n",
       "      <td>0.919</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>25.84</td>\n",
       "      <td>8.38</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011</td>\n",
       "      <td>George Mason</td>\n",
       "      <td>61.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Villanova</td>\n",
       "      <td>57.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>George Mason</td>\n",
       "      <td>25.764706</td>\n",
       "      <td>14.558824</td>\n",
       "      <td>...</td>\n",
       "      <td>72.242424</td>\n",
       "      <td>65.424242</td>\n",
       "      <td>0.438</td>\n",
       "      <td>0.348</td>\n",
       "      <td>0.757</td>\n",
       "      <td>0.636</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>15.05</td>\n",
       "      <td>8.23</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011</td>\n",
       "      <td>Clemson</td>\n",
       "      <td>76.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>West Virginia</td>\n",
       "      <td>84.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Clemson</td>\n",
       "      <td>23.823529</td>\n",
       "      <td>14.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>69.787879</td>\n",
       "      <td>64.666667</td>\n",
       "      <td>0.429</td>\n",
       "      <td>0.337</td>\n",
       "      <td>0.711</td>\n",
       "      <td>0.636</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>16.15</td>\n",
       "      <td>11.03</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   year   team_1_name  team_1_score  team_1_seed    team_2_name  team_2_score  \\\n",
       "0  2011          UTSA          46.0         16.0     Ohio State          75.0   \n",
       "1  2011  George Mason          61.0          8.0      Villanova          57.0   \n",
       "2  2011       Clemson          76.0         12.0  West Virginia          84.0   \n",
       "\n",
       "   team_2_seed        team_name_1    fg_pg_1    ft_pg_1  ...    pt_pg_2  \\\n",
       "0          1.0  Texas-San Antonio  23.588235  16.058824  ...  77.135135   \n",
       "1          9.0       George Mason  25.764706  14.558824  ...  72.242424   \n",
       "2          5.0            Clemson  23.823529  14.500000  ...  69.787879   \n",
       "\n",
       "   opnt_pt_pg_2  fg_pct_2  three_p_pct_2  ft_pct_2  wl_pct_2  conf_wl_pct_2  \\\n",
       "0     59.675676     0.494          0.423     0.701     0.919       0.888889   \n",
       "1     65.424242     0.438          0.348     0.757     0.636       0.500000   \n",
       "2     64.666667     0.429          0.337     0.711     0.636       0.611111   \n",
       "\n",
       "   srs_2  sos_2  t1_win  \n",
       "0  25.84   8.38   False  \n",
       "1  15.05   8.23    True  \n",
       "2  16.15  11.03   False  \n",
       "\n",
       "[3 rows x 48 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps_game_w_team_stats.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(441, 48)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps_game_w_team_stats.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check team 1 winning true/false ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of True cases: 231 (52.38%)\n",
      "Number of False cases: 210 (47.62%)\n"
     ]
    }
   ],
   "source": [
    "t1_win_map = {True:1, False:0}\n",
    "ps_game_w_team_stats['t1_win'] = ps_game_w_team_stats['t1_win'].map(t1_win_map)\n",
    "num_true = len(ps_game_w_team_stats.loc[ps_game_w_team_stats['t1_win'] == True])\n",
    "num_false = len(ps_game_w_team_stats.loc[ps_game_w_team_stats['t1_win'] == False])\n",
    "print(\"Number of True cases: {0} ({1:2.2f}%)\".format(num_true, (num_true/(num_true+num_false))*100))\n",
    "print(\"Number of False cases: {0} ({1:2.2f}%)\".format(num_false, (num_false/(num_true+num_false))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.model_selection\n",
    "ps_feature_col_names = ['team_1_seed', 'team_2_seed','fg_pg_1','ft_pg_1',\n",
    "            'three_pt_pg_1','orb_pg_1','drb_pg_1',\n",
    "            'ast_pg_1','stl_pg_1','blk_pg_1',\n",
    "            'tov_pg_1','pf_pg_1', 'pt_pg_1',\n",
    "            'opnt_pt_pg_1','fg_pct_1','three_p_pct_1',\n",
    "            'ft_pct_1','wl_pct_1','conf_wl_pct_1',\n",
    "            'srs_1','sos_1',\n",
    "            'fg_pg_2','ft_pg_2',\n",
    "            'three_pt_pg_2','orb_pg_2','drb_pg_2',\n",
    "            'ast_pg_2','stl_pg_2','blk_pg_2',\n",
    "            'tov_pg_2','pf_pg_2', 'pt_pg_2',\n",
    "            'opnt_pt_pg_2','fg_pct_2','three_p_pct_2',\n",
    "            'ft_pct_2','wl_pct_2','conf_wl_pct_2',\n",
    "            'srs_2','sos_2'\n",
    "            ]\n",
    "ps_predict_class_names = ['t1_win']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_features(data, col_names):\n",
    "    scaled_features = {}\n",
    "    for col_name in col_names:\n",
    "        mean, std = data[col_name].values.mean(), data[col_name].values.std()\n",
    "        scaled_features[col_name] = [mean, std]\n",
    "        data.loc[:, col_name] = (data[col_name].values - mean)/std\n",
    "    return scaled_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "scale_features(ps_game_w_team_stats, ps_feature_col_names)\n",
    "ps_x = ps_game_w_team_stats[ps_feature_col_names].values\n",
    "ps_y = ps_game_w_team_stats[ps_predict_class_names].values\n",
    "print(type(ps_x))\n",
    "split_test_size = 0.25\n",
    "ps_x_train, ps_x_test, ps_y_train, ps_y_test = sklearn.model_selection.train_test_split(ps_x, ps_y, test_size=split_test_size, random_state=42)\n",
    "split_valid_size = 0.333\n",
    "ps_x_train, ps_x_val, ps_y_train, ps_y_val = sklearn.model_selection.train_test_split(ps_x_train, ps_y_train, test_size=split_valid_size, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.89% in training set\n",
      "25.17% in test set\n",
      "24.94% in test set\n"
     ]
    }
   ],
   "source": [
    "print(\"{0:0.2f}% in training set\".format((len(ps_x_train)/len(ps_game_w_team_stats.index))*100))\n",
    "print(\"{0:0.2f}% in test set\".format((len(ps_x_test)/len(ps_game_w_team_stats.index))*100))\n",
    "print(\"{0:0.2f}% in test set\".format((len(ps_x_val)/len(ps_game_w_team_stats.index))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.impute\n",
    "\n",
    "#Impute with mean all 0 readings\n",
    "fill_0 = sklearn.impute.SimpleImputer(missing_values=0, strategy=\"mean\")\n",
    "\n",
    "ps_x_train = fill_0.fit_transform(ps_x_train)\n",
    "ps_x_test = fill_0.fit_transform(ps_x_test)\n",
    "ps_x_val = fill_0.fit_transform(ps_x_val)\n",
    "# TODO : impute incorrect negative values such anything other than (SOS and SRS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "print(len(ps_x_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[-0.8663, -1.2717,  1.0014, -0.4597,  1.3055, -0.4140,  0.4707,  0.8538,\n",
      "         -0.2324,  2.0430, -0.0753, -0.9153,  0.9982,  0.1564,  0.9477,  0.7227,\n",
      "          0.0420,  1.0910,  1.2749,  0.8389,  0.5079,  2.0011, -0.1542,  1.5838,\n",
      "          0.3544,  1.0425,  1.1607,  0.2799,  0.3289,  0.6034,  0.2011,  1.9187,\n",
      "          1.5560,  1.2947,  1.6882, -0.9417,  1.1696,  1.2119,  1.2497,  1.3320],\n",
      "        [-0.8663,  1.6681, -0.9790,  0.2877, -2.0177,  0.3602, -1.2628, -0.2302,\n",
      "         -0.1468, -0.4616,  1.1402, -0.2907, -1.2397, -0.5344, -0.1032, -2.3768,\n",
      "          0.5640, -0.0244,  0.4275, -0.0752,  0.5506, -0.6995,  0.7169,  0.5353,\n",
      "         -1.3976, -1.6311, -0.1627, -0.8204, -1.0138, -0.6475,  0.1611, -0.1383,\n",
      "         -0.5670,  0.0914,  0.3067,  2.5229,  0.1497,  1.6310, -1.1775, -2.1604],\n",
      "        [-0.6509,  1.4420, -1.8601,  0.9976,  0.3973, -1.3084, -0.1459, -1.9353,\n",
      "         -1.4899,  0.2428,  0.3753, -0.3868, -1.0327, -0.3662, -0.8475,  0.5751,\n",
      "          1.3181,  0.5928,  0.4275, -0.1726,  0.3475, -0.6995, -0.9386,  0.0623,\n",
      "          0.2256,  0.6389, -0.6681, -0.4934,  0.4322,  0.4217, -0.4477, -0.8915,\n",
      "         -1.1323, -0.1863,  0.7804, -1.0305,  0.7765,  0.6355, -1.3238, -1.8374]]), tensor([[1.],\n",
      "        [1.],\n",
      "        [1.]])]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "dropout = 0.3\n",
    "model = nn.Sequential(nn.Linear(40, 30),\n",
    "                     nn.ReLU(),\n",
    "                     nn.Dropout(dropout),\n",
    "                     nn.Linear(30, 30),\n",
    "                     nn.ReLU(),\n",
    "                     nn.Dropout(dropout),\n",
    "                     nn.Linear(30, 10),\n",
    "                     nn.ReLU(),\n",
    "                     nn.Dropout(dropout),\n",
    "                     nn.Linear(10, 1),\n",
    "                     nn.Sigmoid())\n",
    "trainset = TensorDataset(torch.from_numpy(ps_x_train).float(), torch.from_numpy(ps_y_train).float())\n",
    "trainloader = DataLoader(trainset, batch_size=3, shuffle=True)\n",
    "testset = TensorDataset(torch.from_numpy(ps_x_test).float(), torch.from_numpy(ps_y_test).float())\n",
    "testloader = DataLoader(testset, batch_size=3, shuffle=True)\n",
    "valset = TensorDataset(torch.from_numpy(ps_x_val).float(), torch.from_numpy(ps_y_val).float())\n",
    "valloader = DataLoader(valset, batch_size=3, shuffle=True)\n",
    "print(next(iter(testloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: True\n"
     ]
    }
   ],
   "source": [
    "# check if CUDA is available\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print ('Using CUDA: {}'.format(use_cuda))\n",
    "if use_cuda:\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_epochs, train_loader, model, optimizer, criterion, use_cuda, save_path):\n",
    "    \"\"\"returns trained model\"\"\"\n",
    "    # initialize tracker for minimum validation loss\n",
    "    # TODO: fix target tensors always zero for some reason\n",
    "    valid_loss_min = np.Inf \n",
    "    print_loss_count = 40\n",
    "    cuda_refresh_count = 5\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        # initialize variables to monitor training and validation loss\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train()\n",
    "        train_load_iter = iter(train_loader)\n",
    "        for i in range(len(train_loader)):\n",
    "            data, target = next(train_load_iter)\n",
    "            # print('data {}'.format(data.shape))\n",
    "            # print('target {}'.format(target))\n",
    "            # move to GPU\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "\n",
    "            optimizer.zero_grad()    \n",
    "            output = model(data)\n",
    "            # print('target {}'.format(target))\n",
    "            # print('output {}'.format(output))\n",
    "            # _, argmax = output.max(-1)\n",
    "            # print('argmax', argmax)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "                \n",
    "            train_loss = train_loss + ((1 / (i + 1)) * (loss.data - train_loss))\n",
    "            if i % print_loss_count == 0:\n",
    "                print('Epoch %d, Batch %d Loss %.6f' % (epoch, i + 1, train_loss))\n",
    "            if i % cuda_refresh_count == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "\n",
    "    # return trained model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 1 Loss 0.656183\n",
      "Epoch 1, Batch 21 Loss 0.696060\n",
      "Epoch 1, Batch 41 Loss 0.683744\n",
      "Epoch 1, Batch 61 Loss 0.695740\n",
      "Epoch 1, Batch 81 Loss 0.695903\n",
      "Epoch 1, Batch 101 Loss 0.689590\n",
      "Epoch 2, Batch 1 Loss 0.670350\n",
      "Epoch 2, Batch 21 Loss 0.681764\n",
      "Epoch 2, Batch 41 Loss 0.683111\n",
      "Epoch 2, Batch 61 Loss 0.686538\n",
      "Epoch 2, Batch 81 Loss 0.689973\n",
      "Epoch 2, Batch 101 Loss 0.686561\n",
      "Epoch 3, Batch 1 Loss 0.752274\n",
      "Epoch 3, Batch 21 Loss 0.711087\n",
      "Epoch 3, Batch 41 Loss 0.699204\n",
      "Epoch 3, Batch 61 Loss 0.699687\n",
      "Epoch 3, Batch 81 Loss 0.696100\n",
      "Epoch 3, Batch 101 Loss 0.691675\n",
      "Epoch 4, Batch 1 Loss 0.588947\n",
      "Epoch 4, Batch 21 Loss 0.669261\n",
      "Epoch 4, Batch 41 Loss 0.678836\n",
      "Epoch 4, Batch 61 Loss 0.688483\n",
      "Epoch 4, Batch 81 Loss 0.691241\n",
      "Epoch 4, Batch 101 Loss 0.690208\n",
      "Epoch 5, Batch 1 Loss 0.659773\n",
      "Epoch 5, Batch 21 Loss 0.687240\n",
      "Epoch 5, Batch 41 Loss 0.689732\n",
      "Epoch 5, Batch 61 Loss 0.695063\n",
      "Epoch 5, Batch 81 Loss 0.688919\n",
      "Epoch 5, Batch 101 Loss 0.689564\n",
      "Epoch 6, Batch 1 Loss 0.722019\n",
      "Epoch 6, Batch 21 Loss 0.674288\n",
      "Epoch 6, Batch 41 Loss 0.685091\n",
      "Epoch 6, Batch 61 Loss 0.691427\n",
      "Epoch 6, Batch 81 Loss 0.686319\n",
      "Epoch 6, Batch 101 Loss 0.684431\n",
      "Epoch 7, Batch 1 Loss 0.748019\n",
      "Epoch 7, Batch 21 Loss 0.690813\n",
      "Epoch 7, Batch 41 Loss 0.675225\n",
      "Epoch 7, Batch 61 Loss 0.679940\n",
      "Epoch 7, Batch 81 Loss 0.687318\n",
      "Epoch 7, Batch 101 Loss 0.687451\n",
      "Epoch 8, Batch 1 Loss 0.741710\n",
      "Epoch 8, Batch 21 Loss 0.673018\n",
      "Epoch 8, Batch 41 Loss 0.683515\n",
      "Epoch 8, Batch 61 Loss 0.681679\n",
      "Epoch 8, Batch 81 Loss 0.681109\n",
      "Epoch 8, Batch 101 Loss 0.688085\n",
      "Epoch 9, Batch 1 Loss 0.733610\n",
      "Epoch 9, Batch 21 Loss 0.692827\n",
      "Epoch 9, Batch 41 Loss 0.680314\n",
      "Epoch 9, Batch 61 Loss 0.683290\n",
      "Epoch 9, Batch 81 Loss 0.682996\n",
      "Epoch 9, Batch 101 Loss 0.686132\n",
      "Epoch 10, Batch 1 Loss 0.725370\n",
      "Epoch 10, Batch 21 Loss 0.693884\n",
      "Epoch 10, Batch 41 Loss 0.684668\n",
      "Epoch 10, Batch 61 Loss 0.690911\n",
      "Epoch 10, Batch 81 Loss 0.683289\n",
      "Epoch 10, Batch 101 Loss 0.685209\n",
      "Epoch 11, Batch 1 Loss 0.581793\n",
      "Epoch 11, Batch 21 Loss 0.680049\n",
      "Epoch 11, Batch 41 Loss 0.683438\n",
      "Epoch 11, Batch 61 Loss 0.683446\n",
      "Epoch 11, Batch 81 Loss 0.688779\n",
      "Epoch 11, Batch 101 Loss 0.683057\n",
      "Epoch 12, Batch 1 Loss 0.657466\n",
      "Epoch 12, Batch 21 Loss 0.680500\n",
      "Epoch 12, Batch 41 Loss 0.690220\n",
      "Epoch 12, Batch 61 Loss 0.697456\n",
      "Epoch 12, Batch 81 Loss 0.689727\n",
      "Epoch 12, Batch 101 Loss 0.687900\n",
      "Epoch 13, Batch 1 Loss 0.805194\n",
      "Epoch 13, Batch 21 Loss 0.706901\n",
      "Epoch 13, Batch 41 Loss 0.692945\n",
      "Epoch 13, Batch 61 Loss 0.693391\n",
      "Epoch 13, Batch 81 Loss 0.685026\n",
      "Epoch 13, Batch 101 Loss 0.684569\n",
      "Epoch 14, Batch 1 Loss 0.599149\n",
      "Epoch 14, Batch 21 Loss 0.685345\n",
      "Epoch 14, Batch 41 Loss 0.689374\n",
      "Epoch 14, Batch 61 Loss 0.682792\n",
      "Epoch 14, Batch 81 Loss 0.683243\n",
      "Epoch 14, Batch 101 Loss 0.687081\n",
      "Epoch 15, Batch 1 Loss 0.654455\n",
      "Epoch 15, Batch 21 Loss 0.662533\n",
      "Epoch 15, Batch 41 Loss 0.679109\n",
      "Epoch 15, Batch 61 Loss 0.676542\n",
      "Epoch 15, Batch 81 Loss 0.682386\n",
      "Epoch 15, Batch 101 Loss 0.684186\n",
      "Epoch 16, Batch 1 Loss 0.749286\n",
      "Epoch 16, Batch 21 Loss 0.690080\n",
      "Epoch 16, Batch 41 Loss 0.690335\n",
      "Epoch 16, Batch 61 Loss 0.684995\n",
      "Epoch 16, Batch 81 Loss 0.686037\n",
      "Epoch 16, Batch 101 Loss 0.685845\n",
      "Epoch 17, Batch 1 Loss 0.728024\n",
      "Epoch 17, Batch 21 Loss 0.664925\n",
      "Epoch 17, Batch 41 Loss 0.670978\n",
      "Epoch 17, Batch 61 Loss 0.667464\n",
      "Epoch 17, Batch 81 Loss 0.674400\n",
      "Epoch 17, Batch 101 Loss 0.681684\n",
      "Epoch 18, Batch 1 Loss 0.731780\n",
      "Epoch 18, Batch 21 Loss 0.698904\n",
      "Epoch 18, Batch 41 Loss 0.695920\n",
      "Epoch 18, Batch 61 Loss 0.689552\n",
      "Epoch 18, Batch 81 Loss 0.687007\n",
      "Epoch 18, Batch 101 Loss 0.686140\n",
      "Epoch 19, Batch 1 Loss 0.575171\n",
      "Epoch 19, Batch 21 Loss 0.697257\n",
      "Epoch 19, Batch 41 Loss 0.682333\n",
      "Epoch 19, Batch 61 Loss 0.682057\n",
      "Epoch 19, Batch 81 Loss 0.686137\n",
      "Epoch 19, Batch 101 Loss 0.685379\n",
      "Epoch 20, Batch 1 Loss 0.759332\n",
      "Epoch 20, Batch 21 Loss 0.699681\n",
      "Epoch 20, Batch 41 Loss 0.693528\n",
      "Epoch 20, Batch 61 Loss 0.686555\n",
      "Epoch 20, Batch 81 Loss 0.685859\n",
      "Epoch 20, Batch 101 Loss 0.684876\n",
      "Epoch 21, Batch 1 Loss 0.709904\n",
      "Epoch 21, Batch 21 Loss 0.673693\n",
      "Epoch 21, Batch 41 Loss 0.679396\n",
      "Epoch 21, Batch 61 Loss 0.677041\n",
      "Epoch 21, Batch 81 Loss 0.683887\n",
      "Epoch 21, Batch 101 Loss 0.683603\n",
      "Epoch 22, Batch 1 Loss 0.615203\n",
      "Epoch 22, Batch 21 Loss 0.676261\n",
      "Epoch 22, Batch 41 Loss 0.687229\n",
      "Epoch 22, Batch 61 Loss 0.683374\n",
      "Epoch 22, Batch 81 Loss 0.683051\n",
      "Epoch 22, Batch 101 Loss 0.682182\n",
      "Epoch 23, Batch 1 Loss 0.669456\n",
      "Epoch 23, Batch 21 Loss 0.698962\n",
      "Epoch 23, Batch 41 Loss 0.684736\n",
      "Epoch 23, Batch 61 Loss 0.684420\n",
      "Epoch 23, Batch 81 Loss 0.686177\n",
      "Epoch 23, Batch 101 Loss 0.683202\n",
      "Epoch 24, Batch 1 Loss 0.664352\n",
      "Epoch 24, Batch 21 Loss 0.689780\n",
      "Epoch 24, Batch 41 Loss 0.692096\n",
      "Epoch 24, Batch 61 Loss 0.694128\n",
      "Epoch 24, Batch 81 Loss 0.684786\n",
      "Epoch 24, Batch 101 Loss 0.683993\n",
      "Epoch 25, Batch 1 Loss 0.785070\n",
      "Epoch 25, Batch 21 Loss 0.672264\n",
      "Epoch 25, Batch 41 Loss 0.682618\n",
      "Epoch 25, Batch 61 Loss 0.687814\n",
      "Epoch 25, Batch 81 Loss 0.687832\n",
      "Epoch 25, Batch 101 Loss 0.686923\n",
      "Epoch 26, Batch 1 Loss 0.803138\n",
      "Epoch 26, Batch 21 Loss 0.701316\n",
      "Epoch 26, Batch 41 Loss 0.695064\n",
      "Epoch 26, Batch 61 Loss 0.690945\n",
      "Epoch 26, Batch 81 Loss 0.688145\n",
      "Epoch 26, Batch 101 Loss 0.686580\n",
      "Epoch 27, Batch 1 Loss 0.661817\n",
      "Epoch 27, Batch 21 Loss 0.675818\n",
      "Epoch 27, Batch 41 Loss 0.689783\n",
      "Epoch 27, Batch 61 Loss 0.684664\n",
      "Epoch 27, Batch 81 Loss 0.681992\n",
      "Epoch 27, Batch 101 Loss 0.679733\n",
      "Epoch 28, Batch 1 Loss 0.715445\n",
      "Epoch 28, Batch 21 Loss 0.691256\n",
      "Epoch 28, Batch 41 Loss 0.686638\n",
      "Epoch 28, Batch 61 Loss 0.681764\n",
      "Epoch 28, Batch 81 Loss 0.687500\n",
      "Epoch 28, Batch 101 Loss 0.686202\n",
      "Epoch 29, Batch 1 Loss 0.589913\n",
      "Epoch 29, Batch 21 Loss 0.675774\n",
      "Epoch 29, Batch 41 Loss 0.671090\n",
      "Epoch 29, Batch 61 Loss 0.674322\n",
      "Epoch 29, Batch 81 Loss 0.682518\n",
      "Epoch 29, Batch 101 Loss 0.682745\n",
      "Epoch 30, Batch 1 Loss 0.714095\n",
      "Epoch 30, Batch 21 Loss 0.684926\n",
      "Epoch 30, Batch 41 Loss 0.683030\n",
      "Epoch 30, Batch 61 Loss 0.683343\n",
      "Epoch 30, Batch 81 Loss 0.685945\n",
      "Epoch 30, Batch 101 Loss 0.686578\n",
      "Epoch 31, Batch 1 Loss 0.657851\n",
      "Epoch 31, Batch 21 Loss 0.667865\n",
      "Epoch 31, Batch 41 Loss 0.674069\n",
      "Epoch 31, Batch 61 Loss 0.679450\n",
      "Epoch 31, Batch 81 Loss 0.682517\n",
      "Epoch 31, Batch 101 Loss 0.682461\n",
      "Epoch 32, Batch 1 Loss 0.691772\n",
      "Epoch 32, Batch 21 Loss 0.684379\n",
      "Epoch 32, Batch 41 Loss 0.677328\n",
      "Epoch 32, Batch 61 Loss 0.679141\n",
      "Epoch 32, Batch 81 Loss 0.684087\n",
      "Epoch 32, Batch 101 Loss 0.682609\n",
      "Epoch 33, Batch 1 Loss 0.642697\n",
      "Epoch 33, Batch 21 Loss 0.677729\n",
      "Epoch 33, Batch 41 Loss 0.672999\n",
      "Epoch 33, Batch 61 Loss 0.675992\n",
      "Epoch 33, Batch 81 Loss 0.673296\n",
      "Epoch 33, Batch 101 Loss 0.682729\n",
      "Epoch 34, Batch 1 Loss 0.639796\n",
      "Epoch 34, Batch 21 Loss 0.684714\n",
      "Epoch 34, Batch 41 Loss 0.681410\n",
      "Epoch 34, Batch 61 Loss 0.676683\n",
      "Epoch 34, Batch 81 Loss 0.682779\n",
      "Epoch 34, Batch 101 Loss 0.681264\n",
      "Epoch 35, Batch 1 Loss 0.793622\n",
      "Epoch 35, Batch 21 Loss 0.714046\n",
      "Epoch 35, Batch 41 Loss 0.689708\n",
      "Epoch 35, Batch 61 Loss 0.681869\n",
      "Epoch 35, Batch 81 Loss 0.686988\n",
      "Epoch 35, Batch 101 Loss 0.683654\n",
      "Epoch 36, Batch 1 Loss 0.664719\n",
      "Epoch 36, Batch 21 Loss 0.667551\n",
      "Epoch 36, Batch 41 Loss 0.673605\n",
      "Epoch 36, Batch 61 Loss 0.674231\n",
      "Epoch 36, Batch 81 Loss 0.676614\n",
      "Epoch 36, Batch 101 Loss 0.680787\n",
      "Epoch 37, Batch 1 Loss 0.771289\n",
      "Epoch 37, Batch 21 Loss 0.679570\n",
      "Epoch 37, Batch 41 Loss 0.693670\n",
      "Epoch 37, Batch 61 Loss 0.688741\n",
      "Epoch 37, Batch 81 Loss 0.686503\n",
      "Epoch 37, Batch 101 Loss 0.682098\n",
      "Epoch 38, Batch 1 Loss 0.661516\n",
      "Epoch 38, Batch 21 Loss 0.687476\n",
      "Epoch 38, Batch 41 Loss 0.679995\n",
      "Epoch 38, Batch 61 Loss 0.684023\n",
      "Epoch 38, Batch 81 Loss 0.677937\n",
      "Epoch 38, Batch 101 Loss 0.680878\n",
      "Epoch 39, Batch 1 Loss 0.635583\n",
      "Epoch 39, Batch 21 Loss 0.658510\n",
      "Epoch 39, Batch 41 Loss 0.657629\n",
      "Epoch 39, Batch 61 Loss 0.677967\n",
      "Epoch 39, Batch 81 Loss 0.676704\n",
      "Epoch 39, Batch 101 Loss 0.679720\n",
      "Epoch 40, Batch 1 Loss 0.700703\n",
      "Epoch 40, Batch 21 Loss 0.681655\n",
      "Epoch 40, Batch 41 Loss 0.683801\n",
      "Epoch 40, Batch 61 Loss 0.685229\n",
      "Epoch 40, Batch 81 Loss 0.681901\n",
      "Epoch 40, Batch 101 Loss 0.682845\n",
      "Epoch 41, Batch 1 Loss 0.722061\n",
      "Epoch 41, Batch 21 Loss 0.664546\n",
      "Epoch 41, Batch 41 Loss 0.677203\n",
      "Epoch 41, Batch 61 Loss 0.686717\n",
      "Epoch 41, Batch 81 Loss 0.681683\n",
      "Epoch 41, Batch 101 Loss 0.682040\n",
      "Epoch 42, Batch 1 Loss 0.718063\n",
      "Epoch 42, Batch 21 Loss 0.671678\n",
      "Epoch 42, Batch 41 Loss 0.670027\n",
      "Epoch 42, Batch 61 Loss 0.675294\n",
      "Epoch 42, Batch 81 Loss 0.675728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42, Batch 101 Loss 0.678259\n",
      "Epoch 43, Batch 1 Loss 0.710078\n",
      "Epoch 43, Batch 21 Loss 0.682415\n",
      "Epoch 43, Batch 41 Loss 0.682605\n",
      "Epoch 43, Batch 61 Loss 0.686627\n",
      "Epoch 43, Batch 81 Loss 0.681415\n",
      "Epoch 43, Batch 101 Loss 0.679869\n",
      "Epoch 44, Batch 1 Loss 0.654303\n",
      "Epoch 44, Batch 21 Loss 0.676725\n",
      "Epoch 44, Batch 41 Loss 0.673338\n",
      "Epoch 44, Batch 61 Loss 0.672356\n",
      "Epoch 44, Batch 81 Loss 0.672844\n",
      "Epoch 44, Batch 101 Loss 0.676941\n",
      "Epoch 45, Batch 1 Loss 0.636916\n",
      "Epoch 45, Batch 21 Loss 0.676980\n",
      "Epoch 45, Batch 41 Loss 0.677438\n",
      "Epoch 45, Batch 61 Loss 0.686261\n",
      "Epoch 45, Batch 81 Loss 0.685718\n",
      "Epoch 45, Batch 101 Loss 0.681750\n",
      "Epoch 46, Batch 1 Loss 0.747935\n",
      "Epoch 46, Batch 21 Loss 0.678408\n",
      "Epoch 46, Batch 41 Loss 0.677965\n",
      "Epoch 46, Batch 61 Loss 0.676074\n",
      "Epoch 46, Batch 81 Loss 0.676535\n",
      "Epoch 46, Batch 101 Loss 0.678015\n",
      "Epoch 47, Batch 1 Loss 0.707553\n",
      "Epoch 47, Batch 21 Loss 0.687424\n",
      "Epoch 47, Batch 41 Loss 0.686503\n",
      "Epoch 47, Batch 61 Loss 0.680731\n",
      "Epoch 47, Batch 81 Loss 0.679981\n",
      "Epoch 47, Batch 101 Loss 0.677268\n",
      "Epoch 48, Batch 1 Loss 0.639185\n",
      "Epoch 48, Batch 21 Loss 0.669883\n",
      "Epoch 48, Batch 41 Loss 0.675543\n",
      "Epoch 48, Batch 61 Loss 0.672482\n",
      "Epoch 48, Batch 81 Loss 0.669462\n",
      "Epoch 48, Batch 101 Loss 0.672925\n",
      "Epoch 49, Batch 1 Loss 0.662743\n",
      "Epoch 49, Batch 21 Loss 0.681406\n",
      "Epoch 49, Batch 41 Loss 0.678044\n",
      "Epoch 49, Batch 61 Loss 0.673841\n",
      "Epoch 49, Batch 81 Loss 0.674257\n",
      "Epoch 49, Batch 101 Loss 0.676569\n",
      "Epoch 50, Batch 1 Loss 0.739470\n",
      "Epoch 50, Batch 21 Loss 0.674281\n",
      "Epoch 50, Batch 41 Loss 0.668624\n",
      "Epoch 50, Batch 61 Loss 0.673641\n",
      "Epoch 50, Batch 81 Loss 0.678686\n",
      "Epoch 50, Batch 101 Loss 0.673602\n",
      "Epoch 51, Batch 1 Loss 0.719105\n",
      "Epoch 51, Batch 21 Loss 0.681252\n",
      "Epoch 51, Batch 41 Loss 0.681047\n",
      "Epoch 51, Batch 61 Loss 0.677856\n",
      "Epoch 51, Batch 81 Loss 0.678847\n",
      "Epoch 51, Batch 101 Loss 0.678375\n",
      "Epoch 52, Batch 1 Loss 0.717698\n",
      "Epoch 52, Batch 21 Loss 0.686106\n",
      "Epoch 52, Batch 41 Loss 0.675414\n",
      "Epoch 52, Batch 61 Loss 0.682821\n",
      "Epoch 52, Batch 81 Loss 0.680305\n",
      "Epoch 52, Batch 101 Loss 0.676672\n",
      "Epoch 53, Batch 1 Loss 0.790536\n",
      "Epoch 53, Batch 21 Loss 0.670013\n",
      "Epoch 53, Batch 41 Loss 0.680775\n",
      "Epoch 53, Batch 61 Loss 0.674031\n",
      "Epoch 53, Batch 81 Loss 0.671470\n",
      "Epoch 53, Batch 101 Loss 0.674041\n",
      "Epoch 54, Batch 1 Loss 0.646921\n",
      "Epoch 54, Batch 21 Loss 0.691288\n",
      "Epoch 54, Batch 41 Loss 0.681724\n",
      "Epoch 54, Batch 61 Loss 0.677333\n",
      "Epoch 54, Batch 81 Loss 0.676805\n",
      "Epoch 54, Batch 101 Loss 0.674772\n",
      "Epoch 55, Batch 1 Loss 0.642241\n",
      "Epoch 55, Batch 21 Loss 0.685395\n",
      "Epoch 55, Batch 41 Loss 0.659062\n",
      "Epoch 55, Batch 61 Loss 0.664459\n",
      "Epoch 55, Batch 81 Loss 0.669519\n",
      "Epoch 55, Batch 101 Loss 0.674152\n",
      "Epoch 56, Batch 1 Loss 0.715679\n",
      "Epoch 56, Batch 21 Loss 0.663731\n",
      "Epoch 56, Batch 41 Loss 0.666497\n",
      "Epoch 56, Batch 61 Loss 0.673230\n",
      "Epoch 56, Batch 81 Loss 0.675028\n",
      "Epoch 56, Batch 101 Loss 0.675927\n",
      "Epoch 57, Batch 1 Loss 0.569123\n",
      "Epoch 57, Batch 21 Loss 0.664562\n",
      "Epoch 57, Batch 41 Loss 0.660390\n",
      "Epoch 57, Batch 61 Loss 0.666259\n",
      "Epoch 57, Batch 81 Loss 0.668560\n",
      "Epoch 57, Batch 101 Loss 0.671803\n",
      "Epoch 58, Batch 1 Loss 0.749263\n",
      "Epoch 58, Batch 21 Loss 0.673030\n",
      "Epoch 58, Batch 41 Loss 0.675876\n",
      "Epoch 58, Batch 61 Loss 0.675408\n",
      "Epoch 58, Batch 81 Loss 0.675200\n",
      "Epoch 58, Batch 101 Loss 0.670911\n",
      "Epoch 59, Batch 1 Loss 0.735616\n",
      "Epoch 59, Batch 21 Loss 0.674822\n",
      "Epoch 59, Batch 41 Loss 0.679076\n",
      "Epoch 59, Batch 61 Loss 0.683261\n",
      "Epoch 59, Batch 81 Loss 0.680725\n",
      "Epoch 59, Batch 101 Loss 0.674619\n",
      "Epoch 60, Batch 1 Loss 0.638665\n",
      "Epoch 60, Batch 21 Loss 0.665257\n",
      "Epoch 60, Batch 41 Loss 0.675376\n",
      "Epoch 60, Batch 61 Loss 0.666576\n",
      "Epoch 60, Batch 81 Loss 0.665814\n",
      "Epoch 60, Batch 101 Loss 0.669694\n",
      "Epoch 61, Batch 1 Loss 0.624571\n",
      "Epoch 61, Batch 21 Loss 0.663508\n",
      "Epoch 61, Batch 41 Loss 0.667419\n",
      "Epoch 61, Batch 61 Loss 0.671557\n",
      "Epoch 61, Batch 81 Loss 0.667773\n",
      "Epoch 61, Batch 101 Loss 0.672181\n",
      "Epoch 62, Batch 1 Loss 0.716991\n",
      "Epoch 62, Batch 21 Loss 0.660811\n",
      "Epoch 62, Batch 41 Loss 0.674341\n",
      "Epoch 62, Batch 61 Loss 0.672036\n",
      "Epoch 62, Batch 81 Loss 0.667323\n",
      "Epoch 62, Batch 101 Loss 0.671172\n",
      "Epoch 63, Batch 1 Loss 0.675375\n",
      "Epoch 63, Batch 21 Loss 0.683682\n",
      "Epoch 63, Batch 41 Loss 0.671039\n",
      "Epoch 63, Batch 61 Loss 0.674814\n",
      "Epoch 63, Batch 81 Loss 0.671677\n",
      "Epoch 63, Batch 101 Loss 0.669154\n",
      "Epoch 64, Batch 1 Loss 0.749998\n",
      "Epoch 64, Batch 21 Loss 0.666463\n",
      "Epoch 64, Batch 41 Loss 0.663610\n",
      "Epoch 64, Batch 61 Loss 0.671681\n",
      "Epoch 64, Batch 81 Loss 0.672851\n",
      "Epoch 64, Batch 101 Loss 0.670016\n",
      "Epoch 65, Batch 1 Loss 0.608111\n",
      "Epoch 65, Batch 21 Loss 0.674310\n",
      "Epoch 65, Batch 41 Loss 0.669369\n",
      "Epoch 65, Batch 61 Loss 0.673940\n",
      "Epoch 65, Batch 81 Loss 0.671010\n",
      "Epoch 65, Batch 101 Loss 0.668058\n",
      "Epoch 66, Batch 1 Loss 0.664671\n",
      "Epoch 66, Batch 21 Loss 0.669837\n",
      "Epoch 66, Batch 41 Loss 0.670956\n",
      "Epoch 66, Batch 61 Loss 0.677262\n",
      "Epoch 66, Batch 81 Loss 0.669060\n",
      "Epoch 66, Batch 101 Loss 0.671637\n",
      "Epoch 67, Batch 1 Loss 0.615745\n",
      "Epoch 67, Batch 21 Loss 0.677451\n",
      "Epoch 67, Batch 41 Loss 0.675433\n",
      "Epoch 67, Batch 61 Loss 0.677506\n",
      "Epoch 67, Batch 81 Loss 0.672579\n",
      "Epoch 67, Batch 101 Loss 0.666882\n",
      "Epoch 68, Batch 1 Loss 0.735384\n",
      "Epoch 68, Batch 21 Loss 0.691259\n",
      "Epoch 68, Batch 41 Loss 0.679127\n",
      "Epoch 68, Batch 61 Loss 0.662484\n",
      "Epoch 68, Batch 81 Loss 0.666317\n",
      "Epoch 68, Batch 101 Loss 0.667024\n",
      "Epoch 69, Batch 1 Loss 0.751900\n",
      "Epoch 69, Batch 21 Loss 0.653856\n",
      "Epoch 69, Batch 41 Loss 0.654655\n",
      "Epoch 69, Batch 61 Loss 0.658750\n",
      "Epoch 69, Batch 81 Loss 0.662439\n",
      "Epoch 69, Batch 101 Loss 0.664648\n",
      "Epoch 70, Batch 1 Loss 0.743254\n",
      "Epoch 70, Batch 21 Loss 0.674504\n",
      "Epoch 70, Batch 41 Loss 0.666914\n",
      "Epoch 70, Batch 61 Loss 0.668383\n",
      "Epoch 70, Batch 81 Loss 0.662721\n",
      "Epoch 70, Batch 101 Loss 0.665601\n",
      "Epoch 71, Batch 1 Loss 0.593654\n",
      "Epoch 71, Batch 21 Loss 0.627501\n",
      "Epoch 71, Batch 41 Loss 0.649902\n",
      "Epoch 71, Batch 61 Loss 0.659963\n",
      "Epoch 71, Batch 81 Loss 0.663702\n",
      "Epoch 71, Batch 101 Loss 0.663461\n",
      "Epoch 72, Batch 1 Loss 0.539649\n",
      "Epoch 72, Batch 21 Loss 0.665644\n",
      "Epoch 72, Batch 41 Loss 0.662290\n",
      "Epoch 72, Batch 61 Loss 0.664010\n",
      "Epoch 72, Batch 81 Loss 0.666324\n",
      "Epoch 72, Batch 101 Loss 0.663218\n",
      "Epoch 73, Batch 1 Loss 0.660188\n",
      "Epoch 73, Batch 21 Loss 0.669750\n",
      "Epoch 73, Batch 41 Loss 0.659047\n",
      "Epoch 73, Batch 61 Loss 0.662686\n",
      "Epoch 73, Batch 81 Loss 0.661309\n",
      "Epoch 73, Batch 101 Loss 0.661772\n",
      "Epoch 74, Batch 1 Loss 0.605595\n",
      "Epoch 74, Batch 21 Loss 0.671836\n",
      "Epoch 74, Batch 41 Loss 0.679955\n",
      "Epoch 74, Batch 61 Loss 0.678681\n",
      "Epoch 74, Batch 81 Loss 0.664961\n",
      "Epoch 74, Batch 101 Loss 0.660480\n",
      "Epoch 75, Batch 1 Loss 0.832785\n",
      "Epoch 75, Batch 21 Loss 0.657900\n",
      "Epoch 75, Batch 41 Loss 0.660662\n",
      "Epoch 75, Batch 61 Loss 0.655052\n",
      "Epoch 75, Batch 81 Loss 0.662733\n",
      "Epoch 75, Batch 101 Loss 0.659162\n",
      "Epoch 76, Batch 1 Loss 0.684541\n",
      "Epoch 76, Batch 21 Loss 0.632411\n",
      "Epoch 76, Batch 41 Loss 0.648729\n",
      "Epoch 76, Batch 61 Loss 0.660419\n",
      "Epoch 76, Batch 81 Loss 0.664111\n",
      "Epoch 76, Batch 101 Loss 0.661095\n",
      "Epoch 77, Batch 1 Loss 0.662817\n",
      "Epoch 77, Batch 21 Loss 0.663113\n",
      "Epoch 77, Batch 41 Loss 0.654671\n",
      "Epoch 77, Batch 61 Loss 0.651212\n",
      "Epoch 77, Batch 81 Loss 0.656485\n",
      "Epoch 77, Batch 101 Loss 0.658511\n",
      "Epoch 78, Batch 1 Loss 0.835557\n",
      "Epoch 78, Batch 21 Loss 0.657423\n",
      "Epoch 78, Batch 41 Loss 0.662946\n",
      "Epoch 78, Batch 61 Loss 0.656502\n",
      "Epoch 78, Batch 81 Loss 0.658020\n",
      "Epoch 78, Batch 101 Loss 0.653752\n",
      "Epoch 79, Batch 1 Loss 0.614664\n",
      "Epoch 79, Batch 21 Loss 0.628158\n",
      "Epoch 79, Batch 41 Loss 0.659231\n",
      "Epoch 79, Batch 61 Loss 0.660585\n",
      "Epoch 79, Batch 81 Loss 0.656303\n",
      "Epoch 79, Batch 101 Loss 0.652792\n",
      "Epoch 80, Batch 1 Loss 0.652245\n",
      "Epoch 80, Batch 21 Loss 0.642436\n",
      "Epoch 80, Batch 41 Loss 0.647127\n",
      "Epoch 80, Batch 61 Loss 0.649336\n",
      "Epoch 80, Batch 81 Loss 0.650999\n",
      "Epoch 80, Batch 101 Loss 0.654154\n",
      "Epoch 81, Batch 1 Loss 0.641800\n",
      "Epoch 81, Batch 21 Loss 0.625326\n",
      "Epoch 81, Batch 41 Loss 0.644169\n",
      "Epoch 81, Batch 61 Loss 0.647674\n",
      "Epoch 81, Batch 81 Loss 0.653573\n",
      "Epoch 81, Batch 101 Loss 0.655858\n",
      "Epoch 82, Batch 1 Loss 0.655184\n",
      "Epoch 82, Batch 21 Loss 0.677094\n",
      "Epoch 82, Batch 41 Loss 0.665263\n",
      "Epoch 82, Batch 61 Loss 0.653069\n",
      "Epoch 82, Batch 81 Loss 0.655736\n",
      "Epoch 82, Batch 101 Loss 0.651024\n",
      "Epoch 83, Batch 1 Loss 0.562272\n",
      "Epoch 83, Batch 21 Loss 0.634526\n",
      "Epoch 83, Batch 41 Loss 0.644165\n",
      "Epoch 83, Batch 61 Loss 0.645294\n",
      "Epoch 83, Batch 81 Loss 0.644060\n",
      "Epoch 83, Batch 101 Loss 0.648073\n",
      "Epoch 84, Batch 1 Loss 0.645298\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84, Batch 21 Loss 0.630964\n",
      "Epoch 84, Batch 41 Loss 0.639658\n",
      "Epoch 84, Batch 61 Loss 0.652089\n",
      "Epoch 84, Batch 81 Loss 0.648112\n",
      "Epoch 84, Batch 101 Loss 0.648277\n",
      "Epoch 85, Batch 1 Loss 0.563326\n",
      "Epoch 85, Batch 21 Loss 0.651121\n",
      "Epoch 85, Batch 41 Loss 0.645383\n",
      "Epoch 85, Batch 61 Loss 0.645806\n",
      "Epoch 85, Batch 81 Loss 0.648356\n",
      "Epoch 85, Batch 101 Loss 0.649668\n",
      "Epoch 86, Batch 1 Loss 0.605701\n",
      "Epoch 86, Batch 21 Loss 0.645764\n",
      "Epoch 86, Batch 41 Loss 0.643341\n",
      "Epoch 86, Batch 61 Loss 0.638013\n",
      "Epoch 86, Batch 81 Loss 0.639607\n",
      "Epoch 86, Batch 101 Loss 0.644252\n",
      "Epoch 87, Batch 1 Loss 0.658130\n",
      "Epoch 87, Batch 21 Loss 0.646890\n",
      "Epoch 87, Batch 41 Loss 0.646056\n",
      "Epoch 87, Batch 61 Loss 0.642771\n",
      "Epoch 87, Batch 81 Loss 0.647529\n",
      "Epoch 87, Batch 101 Loss 0.645295\n",
      "Epoch 88, Batch 1 Loss 0.615119\n",
      "Epoch 88, Batch 21 Loss 0.629598\n",
      "Epoch 88, Batch 41 Loss 0.639721\n",
      "Epoch 88, Batch 61 Loss 0.648607\n",
      "Epoch 88, Batch 81 Loss 0.640286\n",
      "Epoch 88, Batch 101 Loss 0.646607\n",
      "Epoch 89, Batch 1 Loss 0.688011\n",
      "Epoch 89, Batch 21 Loss 0.640392\n",
      "Epoch 89, Batch 41 Loss 0.635906\n",
      "Epoch 89, Batch 61 Loss 0.642404\n",
      "Epoch 89, Batch 81 Loss 0.643625\n",
      "Epoch 89, Batch 101 Loss 0.642321\n",
      "Epoch 90, Batch 1 Loss 0.671005\n",
      "Epoch 90, Batch 21 Loss 0.636932\n",
      "Epoch 90, Batch 41 Loss 0.636154\n",
      "Epoch 90, Batch 61 Loss 0.631948\n",
      "Epoch 90, Batch 81 Loss 0.634927\n",
      "Epoch 90, Batch 101 Loss 0.636547\n",
      "Epoch 91, Batch 1 Loss 0.555701\n",
      "Epoch 91, Batch 21 Loss 0.633149\n",
      "Epoch 91, Batch 41 Loss 0.634740\n",
      "Epoch 91, Batch 61 Loss 0.624915\n",
      "Epoch 91, Batch 81 Loss 0.635706\n",
      "Epoch 91, Batch 101 Loss 0.637512\n",
      "Epoch 92, Batch 1 Loss 0.659296\n",
      "Epoch 92, Batch 21 Loss 0.669667\n",
      "Epoch 92, Batch 41 Loss 0.645995\n",
      "Epoch 92, Batch 61 Loss 0.640905\n",
      "Epoch 92, Batch 81 Loss 0.632821\n",
      "Epoch 92, Batch 101 Loss 0.635948\n",
      "Epoch 93, Batch 1 Loss 0.664886\n",
      "Epoch 93, Batch 21 Loss 0.649451\n",
      "Epoch 93, Batch 41 Loss 0.647923\n",
      "Epoch 93, Batch 61 Loss 0.642974\n",
      "Epoch 93, Batch 81 Loss 0.635901\n",
      "Epoch 93, Batch 101 Loss 0.634631\n",
      "Epoch 94, Batch 1 Loss 0.551808\n",
      "Epoch 94, Batch 21 Loss 0.630833\n",
      "Epoch 94, Batch 41 Loss 0.633837\n",
      "Epoch 94, Batch 61 Loss 0.637330\n",
      "Epoch 94, Batch 81 Loss 0.632699\n",
      "Epoch 94, Batch 101 Loss 0.632716\n",
      "Epoch 95, Batch 1 Loss 0.678159\n",
      "Epoch 95, Batch 21 Loss 0.645112\n",
      "Epoch 95, Batch 41 Loss 0.635436\n",
      "Epoch 95, Batch 61 Loss 0.630736\n",
      "Epoch 95, Batch 81 Loss 0.633890\n",
      "Epoch 95, Batch 101 Loss 0.631556\n",
      "Epoch 96, Batch 1 Loss 0.638311\n",
      "Epoch 96, Batch 21 Loss 0.630408\n",
      "Epoch 96, Batch 41 Loss 0.637109\n",
      "Epoch 96, Batch 61 Loss 0.637266\n",
      "Epoch 96, Batch 81 Loss 0.629635\n",
      "Epoch 96, Batch 101 Loss 0.631118\n",
      "Epoch 97, Batch 1 Loss 0.773672\n",
      "Epoch 97, Batch 21 Loss 0.637639\n",
      "Epoch 97, Batch 41 Loss 0.631742\n",
      "Epoch 97, Batch 61 Loss 0.629734\n",
      "Epoch 97, Batch 81 Loss 0.627183\n",
      "Epoch 97, Batch 101 Loss 0.622864\n",
      "Epoch 98, Batch 1 Loss 0.741878\n",
      "Epoch 98, Batch 21 Loss 0.620732\n",
      "Epoch 98, Batch 41 Loss 0.630794\n",
      "Epoch 98, Batch 61 Loss 0.631025\n",
      "Epoch 98, Batch 81 Loss 0.627883\n",
      "Epoch 98, Batch 101 Loss 0.628150\n",
      "Epoch 99, Batch 1 Loss 0.533975\n",
      "Epoch 99, Batch 21 Loss 0.599745\n",
      "Epoch 99, Batch 41 Loss 0.617284\n",
      "Epoch 99, Batch 61 Loss 0.623644\n",
      "Epoch 99, Batch 81 Loss 0.625291\n",
      "Epoch 99, Batch 101 Loss 0.628422\n",
      "Epoch 100, Batch 1 Loss 0.626012\n",
      "Epoch 100, Batch 21 Loss 0.622182\n",
      "Epoch 100, Batch 41 Loss 0.620776\n",
      "Epoch 100, Batch 61 Loss 0.622099\n",
      "Epoch 100, Batch 81 Loss 0.615270\n",
      "Epoch 100, Batch 101 Loss 0.618310\n",
      "Epoch 101, Batch 1 Loss 0.587748\n",
      "Epoch 101, Batch 21 Loss 0.618284\n",
      "Epoch 101, Batch 41 Loss 0.625403\n",
      "Epoch 101, Batch 61 Loss 0.623339\n",
      "Epoch 101, Batch 81 Loss 0.618439\n",
      "Epoch 101, Batch 101 Loss 0.616293\n",
      "Epoch 102, Batch 1 Loss 0.522573\n",
      "Epoch 102, Batch 21 Loss 0.634838\n",
      "Epoch 102, Batch 41 Loss 0.599349\n",
      "Epoch 102, Batch 61 Loss 0.602811\n",
      "Epoch 102, Batch 81 Loss 0.613666\n",
      "Epoch 102, Batch 101 Loss 0.614518\n",
      "Epoch 103, Batch 1 Loss 0.754956\n",
      "Epoch 103, Batch 21 Loss 0.588260\n",
      "Epoch 103, Batch 41 Loss 0.610826\n",
      "Epoch 103, Batch 61 Loss 0.608803\n",
      "Epoch 103, Batch 81 Loss 0.618547\n",
      "Epoch 103, Batch 101 Loss 0.614075\n",
      "Epoch 104, Batch 1 Loss 0.560631\n",
      "Epoch 104, Batch 21 Loss 0.611118\n",
      "Epoch 104, Batch 41 Loss 0.610112\n",
      "Epoch 104, Batch 61 Loss 0.606039\n",
      "Epoch 104, Batch 81 Loss 0.599896\n",
      "Epoch 104, Batch 101 Loss 0.603681\n",
      "Epoch 105, Batch 1 Loss 0.715588\n",
      "Epoch 105, Batch 21 Loss 0.640759\n",
      "Epoch 105, Batch 41 Loss 0.622513\n",
      "Epoch 105, Batch 61 Loss 0.616527\n",
      "Epoch 105, Batch 81 Loss 0.606993\n",
      "Epoch 105, Batch 101 Loss 0.610364\n",
      "Epoch 106, Batch 1 Loss 0.476579\n",
      "Epoch 106, Batch 21 Loss 0.619513\n",
      "Epoch 106, Batch 41 Loss 0.616709\n",
      "Epoch 106, Batch 61 Loss 0.613956\n",
      "Epoch 106, Batch 81 Loss 0.611718\n",
      "Epoch 106, Batch 101 Loss 0.612729\n",
      "Epoch 107, Batch 1 Loss 0.775266\n",
      "Epoch 107, Batch 21 Loss 0.659597\n",
      "Epoch 107, Batch 41 Loss 0.620058\n",
      "Epoch 107, Batch 61 Loss 0.600563\n",
      "Epoch 107, Batch 81 Loss 0.613210\n",
      "Epoch 107, Batch 101 Loss 0.604649\n",
      "Epoch 108, Batch 1 Loss 0.518906\n",
      "Epoch 108, Batch 21 Loss 0.566804\n",
      "Epoch 108, Batch 41 Loss 0.575501\n",
      "Epoch 108, Batch 61 Loss 0.591355\n",
      "Epoch 108, Batch 81 Loss 0.598031\n",
      "Epoch 108, Batch 101 Loss 0.602788\n",
      "Epoch 109, Batch 1 Loss 0.500640\n",
      "Epoch 109, Batch 21 Loss 0.606846\n",
      "Epoch 109, Batch 41 Loss 0.612161\n",
      "Epoch 109, Batch 61 Loss 0.616009\n",
      "Epoch 109, Batch 81 Loss 0.606465\n",
      "Epoch 109, Batch 101 Loss 0.598404\n",
      "Epoch 110, Batch 1 Loss 0.656088\n",
      "Epoch 110, Batch 21 Loss 0.589069\n",
      "Epoch 110, Batch 41 Loss 0.588201\n",
      "Epoch 110, Batch 61 Loss 0.602192\n",
      "Epoch 110, Batch 81 Loss 0.598274\n",
      "Epoch 110, Batch 101 Loss 0.592110\n",
      "Epoch 111, Batch 1 Loss 0.629498\n",
      "Epoch 111, Batch 21 Loss 0.572297\n",
      "Epoch 111, Batch 41 Loss 0.589857\n",
      "Epoch 111, Batch 61 Loss 0.602172\n",
      "Epoch 111, Batch 81 Loss 0.598888\n",
      "Epoch 111, Batch 101 Loss 0.596124\n",
      "Epoch 112, Batch 1 Loss 0.670194\n",
      "Epoch 112, Batch 21 Loss 0.597678\n",
      "Epoch 112, Batch 41 Loss 0.593831\n",
      "Epoch 112, Batch 61 Loss 0.591928\n",
      "Epoch 112, Batch 81 Loss 0.596272\n",
      "Epoch 112, Batch 101 Loss 0.591500\n",
      "Epoch 113, Batch 1 Loss 0.804353\n",
      "Epoch 113, Batch 21 Loss 0.609577\n",
      "Epoch 113, Batch 41 Loss 0.572643\n",
      "Epoch 113, Batch 61 Loss 0.572016\n",
      "Epoch 113, Batch 81 Loss 0.573203\n",
      "Epoch 113, Batch 101 Loss 0.587363\n",
      "Epoch 114, Batch 1 Loss 0.748890\n",
      "Epoch 114, Batch 21 Loss 0.582666\n",
      "Epoch 114, Batch 41 Loss 0.609143\n",
      "Epoch 114, Batch 61 Loss 0.602366\n",
      "Epoch 114, Batch 81 Loss 0.599810\n",
      "Epoch 114, Batch 101 Loss 0.587336\n",
      "Epoch 115, Batch 1 Loss 0.480684\n",
      "Epoch 115, Batch 21 Loss 0.610686\n",
      "Epoch 115, Batch 41 Loss 0.580065\n",
      "Epoch 115, Batch 61 Loss 0.584148\n",
      "Epoch 115, Batch 81 Loss 0.586409\n",
      "Epoch 115, Batch 101 Loss 0.580270\n",
      "Epoch 116, Batch 1 Loss 0.615330\n",
      "Epoch 116, Batch 21 Loss 0.576917\n",
      "Epoch 116, Batch 41 Loss 0.573872\n",
      "Epoch 116, Batch 61 Loss 0.574716\n",
      "Epoch 116, Batch 81 Loss 0.581839\n",
      "Epoch 116, Batch 101 Loss 0.583827\n",
      "Epoch 117, Batch 1 Loss 0.469146\n",
      "Epoch 117, Batch 21 Loss 0.605731\n",
      "Epoch 117, Batch 41 Loss 0.590867\n",
      "Epoch 117, Batch 61 Loss 0.580843\n",
      "Epoch 117, Batch 81 Loss 0.571770\n",
      "Epoch 117, Batch 101 Loss 0.573371\n",
      "Epoch 118, Batch 1 Loss 0.347798\n",
      "Epoch 118, Batch 21 Loss 0.593327\n",
      "Epoch 118, Batch 41 Loss 0.582768\n",
      "Epoch 118, Batch 61 Loss 0.569046\n",
      "Epoch 118, Batch 81 Loss 0.564972\n",
      "Epoch 118, Batch 101 Loss 0.567863\n",
      "Epoch 119, Batch 1 Loss 0.674358\n",
      "Epoch 119, Batch 21 Loss 0.586424\n",
      "Epoch 119, Batch 41 Loss 0.605733\n",
      "Epoch 119, Batch 61 Loss 0.585819\n",
      "Epoch 119, Batch 81 Loss 0.589609\n",
      "Epoch 119, Batch 101 Loss 0.574314\n",
      "Epoch 120, Batch 1 Loss 0.681684\n",
      "Epoch 120, Batch 21 Loss 0.589644\n",
      "Epoch 120, Batch 41 Loss 0.571076\n",
      "Epoch 120, Batch 61 Loss 0.575278\n",
      "Epoch 120, Batch 81 Loss 0.573610\n",
      "Epoch 120, Batch 101 Loss 0.566036\n",
      "Epoch 121, Batch 1 Loss 0.349997\n",
      "Epoch 121, Batch 21 Loss 0.583678\n",
      "Epoch 121, Batch 41 Loss 0.584088\n",
      "Epoch 121, Batch 61 Loss 0.568182\n",
      "Epoch 121, Batch 81 Loss 0.577551\n",
      "Epoch 121, Batch 101 Loss 0.571280\n",
      "Epoch 122, Batch 1 Loss 0.551901\n",
      "Epoch 122, Batch 21 Loss 0.585225\n",
      "Epoch 122, Batch 41 Loss 0.598549\n",
      "Epoch 122, Batch 61 Loss 0.586068\n",
      "Epoch 122, Batch 81 Loss 0.582843\n",
      "Epoch 122, Batch 101 Loss 0.563861\n",
      "Epoch 123, Batch 1 Loss 0.723048\n",
      "Epoch 123, Batch 21 Loss 0.584053\n",
      "Epoch 123, Batch 41 Loss 0.554043\n",
      "Epoch 123, Batch 61 Loss 0.552008\n",
      "Epoch 123, Batch 81 Loss 0.552412\n",
      "Epoch 123, Batch 101 Loss 0.554399\n",
      "Epoch 124, Batch 1 Loss 0.506960\n",
      "Epoch 124, Batch 21 Loss 0.573708\n",
      "Epoch 124, Batch 41 Loss 0.548735\n",
      "Epoch 124, Batch 61 Loss 0.570322\n",
      "Epoch 124, Batch 81 Loss 0.565024\n",
      "Epoch 124, Batch 101 Loss 0.564577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 125, Batch 1 Loss 0.611813\n",
      "Epoch 125, Batch 21 Loss 0.565911\n",
      "Epoch 125, Batch 41 Loss 0.548222\n",
      "Epoch 125, Batch 61 Loss 0.548310\n",
      "Epoch 125, Batch 81 Loss 0.552598\n",
      "Epoch 125, Batch 101 Loss 0.556805\n",
      "Epoch 126, Batch 1 Loss 0.755248\n",
      "Epoch 126, Batch 21 Loss 0.516311\n",
      "Epoch 126, Batch 41 Loss 0.556062\n",
      "Epoch 126, Batch 61 Loss 0.545460\n",
      "Epoch 126, Batch 81 Loss 0.543198\n",
      "Epoch 126, Batch 101 Loss 0.552148\n",
      "Epoch 127, Batch 1 Loss 0.692938\n",
      "Epoch 127, Batch 21 Loss 0.567518\n",
      "Epoch 127, Batch 41 Loss 0.541455\n",
      "Epoch 127, Batch 61 Loss 0.542675\n",
      "Epoch 127, Batch 81 Loss 0.558530\n",
      "Epoch 127, Batch 101 Loss 0.556083\n",
      "Epoch 128, Batch 1 Loss 0.447227\n",
      "Epoch 128, Batch 21 Loss 0.552419\n",
      "Epoch 128, Batch 41 Loss 0.556843\n",
      "Epoch 128, Batch 61 Loss 0.552673\n",
      "Epoch 128, Batch 81 Loss 0.548692\n",
      "Epoch 128, Batch 101 Loss 0.547460\n",
      "Epoch 129, Batch 1 Loss 0.412357\n",
      "Epoch 129, Batch 21 Loss 0.544755\n",
      "Epoch 129, Batch 41 Loss 0.566181\n",
      "Epoch 129, Batch 61 Loss 0.548254\n",
      "Epoch 129, Batch 81 Loss 0.559784\n",
      "Epoch 129, Batch 101 Loss 0.550503\n",
      "Epoch 130, Batch 1 Loss 0.657238\n",
      "Epoch 130, Batch 21 Loss 0.537334\n",
      "Epoch 130, Batch 41 Loss 0.518660\n",
      "Epoch 130, Batch 61 Loss 0.518297\n",
      "Epoch 130, Batch 81 Loss 0.522418\n",
      "Epoch 130, Batch 101 Loss 0.539201\n",
      "Epoch 131, Batch 1 Loss 0.718553\n",
      "Epoch 131, Batch 21 Loss 0.521050\n",
      "Epoch 131, Batch 41 Loss 0.556013\n",
      "Epoch 131, Batch 61 Loss 0.541083\n",
      "Epoch 131, Batch 81 Loss 0.538203\n",
      "Epoch 131, Batch 101 Loss 0.537881\n",
      "Epoch 132, Batch 1 Loss 0.576258\n",
      "Epoch 132, Batch 21 Loss 0.486000\n",
      "Epoch 132, Batch 41 Loss 0.499222\n",
      "Epoch 132, Batch 61 Loss 0.528556\n",
      "Epoch 132, Batch 81 Loss 0.523201\n",
      "Epoch 132, Batch 101 Loss 0.533446\n",
      "Epoch 133, Batch 1 Loss 0.482413\n",
      "Epoch 133, Batch 21 Loss 0.550237\n",
      "Epoch 133, Batch 41 Loss 0.538036\n",
      "Epoch 133, Batch 61 Loss 0.541412\n",
      "Epoch 133, Batch 81 Loss 0.541175\n",
      "Epoch 133, Batch 101 Loss 0.534416\n",
      "Epoch 134, Batch 1 Loss 0.543911\n",
      "Epoch 134, Batch 21 Loss 0.571014\n",
      "Epoch 134, Batch 41 Loss 0.557506\n",
      "Epoch 134, Batch 61 Loss 0.557716\n",
      "Epoch 134, Batch 81 Loss 0.556370\n",
      "Epoch 134, Batch 101 Loss 0.539206\n",
      "Epoch 135, Batch 1 Loss 0.522507\n",
      "Epoch 135, Batch 21 Loss 0.537713\n",
      "Epoch 135, Batch 41 Loss 0.547620\n",
      "Epoch 135, Batch 61 Loss 0.525374\n",
      "Epoch 135, Batch 81 Loss 0.524338\n",
      "Epoch 135, Batch 101 Loss 0.527781\n",
      "Epoch 136, Batch 1 Loss 0.779721\n",
      "Epoch 136, Batch 21 Loss 0.554519\n",
      "Epoch 136, Batch 41 Loss 0.569508\n",
      "Epoch 136, Batch 61 Loss 0.545697\n",
      "Epoch 136, Batch 81 Loss 0.550119\n",
      "Epoch 136, Batch 101 Loss 0.531036\n",
      "Epoch 137, Batch 1 Loss 0.520248\n",
      "Epoch 137, Batch 21 Loss 0.496948\n",
      "Epoch 137, Batch 41 Loss 0.527109\n",
      "Epoch 137, Batch 61 Loss 0.516737\n",
      "Epoch 137, Batch 81 Loss 0.523466\n",
      "Epoch 137, Batch 101 Loss 0.522489\n",
      "Epoch 138, Batch 1 Loss 0.660232\n",
      "Epoch 138, Batch 21 Loss 0.552336\n",
      "Epoch 138, Batch 41 Loss 0.517354\n",
      "Epoch 138, Batch 61 Loss 0.497788\n",
      "Epoch 138, Batch 81 Loss 0.497929\n",
      "Epoch 138, Batch 101 Loss 0.509786\n",
      "Epoch 139, Batch 1 Loss 0.387829\n",
      "Epoch 139, Batch 21 Loss 0.578267\n",
      "Epoch 139, Batch 41 Loss 0.549405\n",
      "Epoch 139, Batch 61 Loss 0.506902\n",
      "Epoch 139, Batch 81 Loss 0.518068\n",
      "Epoch 139, Batch 101 Loss 0.511057\n",
      "Epoch 140, Batch 1 Loss 0.610747\n",
      "Epoch 140, Batch 21 Loss 0.520973\n",
      "Epoch 140, Batch 41 Loss 0.509228\n",
      "Epoch 140, Batch 61 Loss 0.519746\n",
      "Epoch 140, Batch 81 Loss 0.521041\n",
      "Epoch 140, Batch 101 Loss 0.516736\n"
     ]
    }
   ],
   "source": [
    "model = train(140, trainloader, model, optimizer, criterion,\n",
    "                      use_cuda, 'model_nn1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(loader, model, criterion, use_cuda, num_classes = 2):\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "    # monitor test loss and accuracy\n",
    "    test_loss = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "    model.eval()\n",
    "    load_iter = iter(loader)\n",
    "    for i in range(len(loader)):\n",
    "        data, target = next(load_iter)\n",
    "        # move to GPU\n",
    "        if use_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        \n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # update average test loss \n",
    "        test_loss = test_loss + ((1 / (i + 1)) * (loss.data - test_loss))\n",
    "        # compare predictions to true label\n",
    "        for j, tensor in enumerate(output):\n",
    "            if (tensor.item() > .5 and target[j] == 1) or (tensor.item() <= .5 and target[j] == 0):\n",
    "                correct += 1\n",
    "        total += data.size(0)\n",
    "       \n",
    "    print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "    print('\\nTest Accuracy: %2d%% (%2d/%2d)' % (\n",
    "        100. * correct / total, correct, total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.533384\n",
      "\n",
      "\n",
      "Test Accuracy: 72% (81/111)\n"
     ]
    }
   ],
   "source": [
    "test(testloader, model, criterion, use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup This years bracket regions\n",
    "# TODO: automate this with the data received from the scraper\n",
    "school_names_south = [\n",
    "    # south region\n",
    "    ('Virginia', 1),('Maryland-Baltimore County', 16),\n",
    "    ('Creighton', 8), ('Kansas State', 9),\n",
    "    ('Kentucky',5), ('Davidson', 12),\n",
    "    ('Arizona',4), ('Buffalo', 13),\n",
    "    ('Miami (FL)', 6), ('Loyola (IL)', 11),\n",
    "    ('Tennessee',3), ('Wright State',14),\n",
    "    ('Nevada',7),('Texas',10),\n",
    "    ('Cincinnati',2), ('Georgia State',15)\n",
    "    ]\n",
    "school_names_west = [\n",
    "    # west region\n",
    "    ('Xavier', 1),('North Carolina Central',16), #or 'Texas Southern',\n",
    "    ('Missouri', 8),('Florida State', 9),\n",
    "    ('Ohio State',5), ('South Dakota State', 12),\n",
    "    ('Gonzaga',4), ('North Carolina-Greensboro',13),\n",
    "    ('Houston',6),('San Diego State',11),\n",
    "    ('Michigan', 3),('Montana', 14),\n",
    "    ('Texas A&M',7),('Providence',10),\n",
    "    ('North Carolina',2),('Lipscomb',15)\n",
    "    ]\n",
    "school_names_east = [\n",
    "    # east region\n",
    "    ('Villanova',1),('Long Island University',16), # or 'Radford',\n",
    "    ('Virginia Tech',8), ('Alabama',9),\n",
    "    ('West Virginia',5), ('Murray State',12),\n",
    "    ('Wichita State',4), ('Marshall',13),\n",
    "    ('Florida',6), ('St. Bonaventure',11), # or 'UCLA',\n",
    "    ('Texas Tech',3), ('Stephen F. Austin',14),\n",
    "    ('Arkansas',7), ('Butler',10),\n",
    "    ('Purdue', 2), ('Cal State Fullerton',15)\n",
    "    ]\n",
    "school_names_midwest = [\n",
    "    # mid-west region\n",
    "    ('Kansas', 1), ('Pennsylvania',16),\n",
    "    ('Seton Hall', 8), ('North Carolina State',9),\n",
    "    ('Clemson', 5), ('New Mexico State',12),\n",
    "    ('Auburn',4), ('College of Charleston',13),\n",
    "    ('Texas Christian',6), ('Arizona State',11), # or 'Syracuse',\n",
    "    ('Michigan State',3), ('Bucknell',14),\n",
    "    ('Rhode Island',7), ('Oklahoma',10),\n",
    "    ('Duke', 2), ('Iona' ,15) \n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Methods to add evaluating the predicted winners of matchups and subbrackets (A region or Final Four)\n",
    "    To change the predictive model used, just change the model handed to \"evaluate_winner(schools,sub_bracket_name, model)\"\n",
    "    found later in the notebook\n",
    "'''\n",
    "def get_matchups_stats(schools, post_season):    \n",
    "    \n",
    "    i = 0 \n",
    "    t1_stats = []\n",
    "    t2_stats = []\n",
    "    t1_seeds = []\n",
    "    t2_seeds = []\n",
    "    if(not is_power_of_two(len(schools))):\n",
    "        print('ERROR: invalid number of school names')\n",
    "        return False\n",
    "    while i < len(schools):\n",
    "        t1_name, t1_seed = schools[i]\n",
    "        t2_name, t2_seed = schools[i + 1]\n",
    "        t1_seeds.append(t1_seed)\n",
    "        t2_seeds.append(t2_seed)\n",
    "        #print(t1_name, t2_name\n",
    "        t1_stats.append(get_school_stats(2018, t1_name))\n",
    "        t2_stats.append(get_school_stats(2018, t2_name))\n",
    "        i = i + 2\n",
    "    if(post_season):\n",
    "        matchup_stats = create_team_stats_df_ps(range(0,int(len(schools)/2)), t1_stats, t2_stats, t1_seeds, t2_seeds)\n",
    "    else:\n",
    "        matchup_stats = create_team_stats_df(range(0,int(len(schools)/2)), t1_stats, t2_stats)\n",
    "    return matchup_stats\n",
    "def is_power_of_two(num):\n",
    "    return ((num & (num - 1)) == 0) and num != 0\n",
    "def get_matchup_winners(matchup_stats, schools, model, post_season, use_cuda):\n",
    "\n",
    "    x_tourney = matchup_stats[ps_feature_col_names].values\n",
    "    x_tourney = torch.from_numpy(x_tourney).float()\n",
    "    # print(x_tourney)\n",
    "    if use_cuda:\n",
    "        x_tourney = x_tourney.cuda()\n",
    "    y_tourney = model(x_tourney)\n",
    "    #print(y_tourney)\n",
    "    i = 0\n",
    "    winners = []\n",
    "    for y_val in y_tourney:\n",
    "        t1_name, t1_seed = schools[i]\n",
    "        t2_name, t2_seed = schools[i + 1]\n",
    "        t1_won = y_val.item() > .5\n",
    "        print(t1_name,t1_seed,' vs. ', t2_name,t2_seed,'(team 1 won=', t1_won,')')\n",
    "        if(t1_won):\n",
    "            winners.append((t1_name,t1_seed))\n",
    "        else:\n",
    "            winners.append((t2_name, t2_seed))\n",
    "        i = i + 2\n",
    "    return winners\n",
    "def evaluate_winner(schools,sub_bracket_name, model, use_cuda):        \n",
    "    remaining_teams = schools\n",
    "    i = 1\n",
    "    while(len(remaining_teams) > 1):\n",
    "        #Add a random factor\n",
    "        rand = random.randrange(0,1)\n",
    "        post_season_stats = True\n",
    "        print(\"---\",sub_bracket_name,\" round \",i,\"---\")\n",
    "        matchup_stats = get_matchups_stats(remaining_teams, post_season_stats)\n",
    "        remaining_teams = get_matchup_winners(matchup_stats,remaining_teams, model, post_season_stats, use_cuda)\n",
    "        i = i + 1\n",
    "    winner = remaining_teams[0]\n",
    "    print('Winner of ',sub_bracket_name,':',winner)\n",
    "    return winner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- South  round  1 ---\n",
      "Virginia 1  vs.  Maryland-Baltimore County 16 (team 1 won= True )\n",
      "Creighton 8  vs.  Kansas State 9 (team 1 won= False )\n",
      "Kentucky 5  vs.  Davidson 12 (team 1 won= True )\n",
      "Arizona 4  vs.  Buffalo 13 (team 1 won= True )\n",
      "Miami (FL) 6  vs.  Loyola (IL) 11 (team 1 won= True )\n",
      "Tennessee 3  vs.  Wright State 14 (team 1 won= True )\n",
      "Nevada 7  vs.  Texas 10 (team 1 won= False )\n",
      "Cincinnati 2  vs.  Georgia State 15 (team 1 won= True )\n",
      "--- South  round  2 ---\n",
      "Virginia 1  vs.  Kansas State 9 (team 1 won= True )\n",
      "Kentucky 5  vs.  Arizona 4 (team 1 won= True )\n",
      "Miami (FL) 6  vs.  Tennessee 3 (team 1 won= False )\n",
      "Texas 10  vs.  Cincinnati 2 (team 1 won= False )\n",
      "--- South  round  3 ---\n",
      "Virginia 1  vs.  Kentucky 5 (team 1 won= True )\n",
      "Tennessee 3  vs.  Cincinnati 2 (team 1 won= False )\n",
      "--- South  round  4 ---\n",
      "Virginia 1  vs.  Cincinnati 2 (team 1 won= True )\n",
      "Winner of  South : ('Virginia', 1)\n",
      "--- West  round  1 ---\n",
      "Xavier 1  vs.  North Carolina Central 16 (team 1 won= True )\n",
      "Missouri 8  vs.  Florida State 9 (team 1 won= False )\n",
      "Ohio State 5  vs.  South Dakota State 12 (team 1 won= True )\n",
      "Gonzaga 4  vs.  North Carolina-Greensboro 13 (team 1 won= True )\n",
      "Houston 6  vs.  San Diego State 11 (team 1 won= True )\n",
      "Michigan 3  vs.  Montana 14 (team 1 won= True )\n",
      "Texas A&M 7  vs.  Providence 10 (team 1 won= True )\n",
      "North Carolina 2  vs.  Lipscomb 15 (team 1 won= True )\n",
      "--- West  round  2 ---\n",
      "Xavier 1  vs.  Florida State 9 (team 1 won= True )\n",
      "Ohio State 5  vs.  Gonzaga 4 (team 1 won= True )\n",
      "Houston 6  vs.  Michigan 3 (team 1 won= False )\n",
      "Texas A&M 7  vs.  North Carolina 2 (team 1 won= False )\n",
      "--- West  round  3 ---\n",
      "Xavier 1  vs.  Ohio State 5 (team 1 won= False )\n",
      "Michigan 3  vs.  North Carolina 2 (team 1 won= False )\n",
      "--- West  round  4 ---\n",
      "Ohio State 5  vs.  North Carolina 2 (team 1 won= False )\n",
      "Winner of  West : ('North Carolina', 2)\n",
      "--- East  round  1 ---\n",
      "Villanova 1  vs.  Long Island University 16 (team 1 won= True )\n",
      "Virginia Tech 8  vs.  Alabama 9 (team 1 won= False )\n",
      "West Virginia 5  vs.  Murray State 12 (team 1 won= True )\n",
      "Wichita State 4  vs.  Marshall 13 (team 1 won= True )\n",
      "Florida 6  vs.  St. Bonaventure 11 (team 1 won= True )\n",
      "Texas Tech 3  vs.  Stephen F. Austin 14 (team 1 won= True )\n",
      "Arkansas 7  vs.  Butler 10 (team 1 won= False )\n",
      "Purdue 2  vs.  Cal State Fullerton 15 (team 1 won= True )\n",
      "--- East  round  2 ---\n",
      "Villanova 1  vs.  Alabama 9 (team 1 won= True )\n",
      "West Virginia 5  vs.  Wichita State 4 (team 1 won= True )\n",
      "Florida 6  vs.  Texas Tech 3 (team 1 won= False )\n",
      "Butler 10  vs.  Purdue 2 (team 1 won= False )\n",
      "--- East  round  3 ---\n",
      "Villanova 1  vs.  West Virginia 5 (team 1 won= True )\n",
      "Texas Tech 3  vs.  Purdue 2 (team 1 won= False )\n",
      "--- East  round  4 ---\n",
      "Villanova 1  vs.  Purdue 2 (team 1 won= False )\n",
      "Winner of  East : ('Purdue', 2)\n",
      "--- MidWest  round  1 ---\n",
      "Kansas 1  vs.  Pennsylvania 16 (team 1 won= True )\n",
      "Seton Hall 8  vs.  North Carolina State 9 (team 1 won= True )\n",
      "Clemson 5  vs.  New Mexico State 12 (team 1 won= True )\n",
      "Auburn 4  vs.  College of Charleston 13 (team 1 won= True )\n",
      "Texas Christian 6  vs.  Arizona State 11 (team 1 won= True )\n",
      "Michigan State 3  vs.  Bucknell 14 (team 1 won= True )\n",
      "Rhode Island 7  vs.  Oklahoma 10 (team 1 won= False )\n",
      "Duke 2  vs.  Iona 15 (team 1 won= True )\n",
      "--- MidWest  round  2 ---\n",
      "Kansas 1  vs.  Seton Hall 8 (team 1 won= True )\n",
      "Clemson 5  vs.  Auburn 4 (team 1 won= False )\n",
      "Texas Christian 6  vs.  Michigan State 3 (team 1 won= False )\n",
      "Oklahoma 10  vs.  Duke 2 (team 1 won= False )\n",
      "--- MidWest  round  3 ---\n",
      "Kansas 1  vs.  Auburn 4 (team 1 won= True )\n",
      "Michigan State 3  vs.  Duke 2 (team 1 won= False )\n",
      "--- MidWest  round  4 ---\n",
      "Kansas 1  vs.  Duke 2 (team 1 won= False )\n",
      "Winner of  MidWest : ('Duke', 2)\n"
     ]
    }
   ],
   "source": [
    "# Get predicted final four\n",
    "\n",
    "final_four = [evaluate_winner(school_names_south, \"South\",model, use_cuda), evaluate_winner(school_names_west,\"West\",model, use_cuda),\n",
    "              evaluate_winner(school_names_east, \"East\", model, use_cuda), evaluate_winner(school_names_midwest, \"MidWest\",model, use_cuda)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Virginia', 1), ('North Carolina', 2), ('Purdue', 2), ('Duke', 2)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_four"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- FinalFour  round  1 ---\n",
      "Virginia 1  vs.  North Carolina 2 (team 1 won= False )\n",
      "Purdue 2  vs.  Duke 2 (team 1 won= False )\n",
      "--- FinalFour  round  2 ---\n",
      "North Carolina 2  vs.  Duke 2 (team 1 won= False )\n",
      "Winner of  FinalFour : ('Duke', 2)\n"
     ]
    }
   ],
   "source": [
    "champ = evaluate_winner(final_four, \"FinalFour\", model, use_cuda)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
