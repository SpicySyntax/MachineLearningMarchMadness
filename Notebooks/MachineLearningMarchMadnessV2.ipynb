{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #dataframes\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np # n-dim object support\n",
    "# do ploting inline instead of in a separate window\n",
    "%matplotlib inline\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_school = pd.read_csv(\"../Scraper/school_records.csv\")\n",
    "df_ps_game = pd.read_csv(\"../Scraper/post_season_game_records.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3125, 21)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_school.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(441, 7)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ps_game.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>team_name</th>\n",
       "      <th>fg_pg</th>\n",
       "      <th>ft_pg</th>\n",
       "      <th>three_pt_pg</th>\n",
       "      <th>orb_pg</th>\n",
       "      <th>drb_pg</th>\n",
       "      <th>ast_pg</th>\n",
       "      <th>stl_pg</th>\n",
       "      <th>blk_pg</th>\n",
       "      <th>...</th>\n",
       "      <th>pf_pg</th>\n",
       "      <th>pt_pg</th>\n",
       "      <th>opnt_pt_pg</th>\n",
       "      <th>fg_pct</th>\n",
       "      <th>three_p_pct</th>\n",
       "      <th>ft_pct</th>\n",
       "      <th>wl_pct</th>\n",
       "      <th>conf_wl_pct</th>\n",
       "      <th>srs</th>\n",
       "      <th>sos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010.0</td>\n",
       "      <td>Air Force</td>\n",
       "      <td>20.387097</td>\n",
       "      <td>10.741935</td>\n",
       "      <td>5.677419</td>\n",
       "      <td>7.096774</td>\n",
       "      <td>27.222685</td>\n",
       "      <td>12.548387</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.645161</td>\n",
       "      <td>...</td>\n",
       "      <td>17.645161</td>\n",
       "      <td>57.193548</td>\n",
       "      <td>63.129032</td>\n",
       "      <td>0.443</td>\n",
       "      <td>0.313</td>\n",
       "      <td>0.635</td>\n",
       "      <td>0.323</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>-4.90</td>\n",
       "      <td>3.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010.0</td>\n",
       "      <td>Akron</td>\n",
       "      <td>25.057143</td>\n",
       "      <td>13.800000</td>\n",
       "      <td>6.714286</td>\n",
       "      <td>13.342857</td>\n",
       "      <td>35.875918</td>\n",
       "      <td>13.514286</td>\n",
       "      <td>6.085714</td>\n",
       "      <td>3.257143</td>\n",
       "      <td>...</td>\n",
       "      <td>19.485714</td>\n",
       "      <td>70.628571</td>\n",
       "      <td>65.514286</td>\n",
       "      <td>0.433</td>\n",
       "      <td>0.339</td>\n",
       "      <td>0.657</td>\n",
       "      <td>0.686</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>2.82</td>\n",
       "      <td>-1.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010.0</td>\n",
       "      <td>Alabama A&amp;M</td>\n",
       "      <td>22.185185</td>\n",
       "      <td>17.481481</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>13.925926</td>\n",
       "      <td>36.669410</td>\n",
       "      <td>10.666667</td>\n",
       "      <td>9.222222</td>\n",
       "      <td>5.296296</td>\n",
       "      <td>...</td>\n",
       "      <td>20.370370</td>\n",
       "      <td>65.851852</td>\n",
       "      <td>69.666667</td>\n",
       "      <td>0.382</td>\n",
       "      <td>0.291</td>\n",
       "      <td>0.635</td>\n",
       "      <td>0.407</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>-20.19</td>\n",
       "      <td>-13.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010.0</td>\n",
       "      <td>Alabama-Birmingham</td>\n",
       "      <td>22.441176</td>\n",
       "      <td>16.852941</td>\n",
       "      <td>5.205882</td>\n",
       "      <td>12.352941</td>\n",
       "      <td>36.342561</td>\n",
       "      <td>11.470588</td>\n",
       "      <td>6.558824</td>\n",
       "      <td>2.676471</td>\n",
       "      <td>...</td>\n",
       "      <td>17.970588</td>\n",
       "      <td>66.941176</td>\n",
       "      <td>60.382353</td>\n",
       "      <td>0.422</td>\n",
       "      <td>0.311</td>\n",
       "      <td>0.694</td>\n",
       "      <td>0.735</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>9.46</td>\n",
       "      <td>2.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010.0</td>\n",
       "      <td>Alabama State</td>\n",
       "      <td>21.516129</td>\n",
       "      <td>15.290323</td>\n",
       "      <td>6.129032</td>\n",
       "      <td>12.903226</td>\n",
       "      <td>35.099896</td>\n",
       "      <td>12.903226</td>\n",
       "      <td>7.354839</td>\n",
       "      <td>4.161290</td>\n",
       "      <td>...</td>\n",
       "      <td>20.451613</td>\n",
       "      <td>64.451613</td>\n",
       "      <td>65.903226</td>\n",
       "      <td>0.404</td>\n",
       "      <td>0.324</td>\n",
       "      <td>0.641</td>\n",
       "      <td>0.516</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>-14.41</td>\n",
       "      <td>-12.02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     year           team_name      fg_pg      ft_pg  three_pt_pg     orb_pg  \\\n",
       "0  2010.0           Air Force  20.387097  10.741935     5.677419   7.096774   \n",
       "1  2010.0               Akron  25.057143  13.800000     6.714286  13.342857   \n",
       "2  2010.0         Alabama A&M  22.185185  17.481481     4.000000  13.925926   \n",
       "3  2010.0  Alabama-Birmingham  22.441176  16.852941     5.205882  12.352941   \n",
       "4  2010.0       Alabama State  21.516129  15.290323     6.129032  12.903226   \n",
       "\n",
       "      drb_pg     ast_pg    stl_pg    blk_pg  ...      pf_pg      pt_pg  \\\n",
       "0  27.222685  12.548387  5.000000  1.645161  ...  17.645161  57.193548   \n",
       "1  35.875918  13.514286  6.085714  3.257143  ...  19.485714  70.628571   \n",
       "2  36.669410  10.666667  9.222222  5.296296  ...  20.370370  65.851852   \n",
       "3  36.342561  11.470588  6.558824  2.676471  ...  17.970588  66.941176   \n",
       "4  35.099896  12.903226  7.354839  4.161290  ...  20.451613  64.451613   \n",
       "\n",
       "   opnt_pt_pg  fg_pct  three_p_pct  ft_pct  wl_pct  conf_wl_pct    srs    sos  \n",
       "0   63.129032   0.443        0.313   0.635   0.323     0.062500  -4.90   3.13  \n",
       "1   65.514286   0.433        0.339   0.657   0.686     0.750000   2.82  -1.50  \n",
       "2   69.666667   0.382        0.291   0.635   0.407     0.444444 -20.19 -13.71  \n",
       "3   60.382353   0.422        0.311   0.694   0.735     0.687500   9.46   2.90  \n",
       "4   65.903226   0.404        0.324   0.641   0.516     0.666667 -14.41 -12.02  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_school.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>team_1_name</th>\n",
       "      <th>team_1_score</th>\n",
       "      <th>team_1_seed</th>\n",
       "      <th>team_2_name</th>\n",
       "      <th>team_2_score</th>\n",
       "      <th>team_2_seed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011</td>\n",
       "      <td>UTSA</td>\n",
       "      <td>46.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>Ohio State</td>\n",
       "      <td>75.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011</td>\n",
       "      <td>George Mason</td>\n",
       "      <td>61.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Villanova</td>\n",
       "      <td>57.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011</td>\n",
       "      <td>Clemson</td>\n",
       "      <td>76.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>West Virginia</td>\n",
       "      <td>84.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011</td>\n",
       "      <td>Princeton</td>\n",
       "      <td>57.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Kentucky</td>\n",
       "      <td>59.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011</td>\n",
       "      <td>Marquette</td>\n",
       "      <td>66.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>Xavier</td>\n",
       "      <td>55.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year   team_1_name  team_1_score  team_1_seed    team_2_name  team_2_score  \\\n",
       "0  2011          UTSA          46.0         16.0     Ohio State          75.0   \n",
       "1  2011  George Mason          61.0          8.0      Villanova          57.0   \n",
       "2  2011       Clemson          76.0         12.0  West Virginia          84.0   \n",
       "3  2011     Princeton          57.0         13.0       Kentucky          59.0   \n",
       "4  2011     Marquette          66.0         11.0         Xavier          55.0   \n",
       "\n",
       "   team_2_seed  \n",
       "0          1.0  \n",
       "1          9.0  \n",
       "2          5.0  \n",
       "3          4.0  \n",
       "4          6.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ps_game.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_school.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ps_game.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_team_name(team_name):\n",
    "    #Apply hard-coded corrections to team names\n",
    "    team_name_dict = {'Colorado-Colorado Springs':'Colorado',\n",
    "                     'Colorado College': 'Colorado',\n",
    "                     'UNC':'North Carolina',\n",
    "                     'UConn':'Connecticut',\n",
    "                     'LIU-Brooklyn':'Long Island University',\n",
    "                     'UTSA':'Texas-San Antonio',\n",
    "                     'Pitt':'Pittsburgh',\n",
    "                     'BYU':'Brigham Young',\n",
    "                     \"St. Peter's\": \"Saint Peter's\",\n",
    "                     'VCU':'Virginia Commonwealth',\n",
    "                     'Southern Miss':'Southern Mississippi',\n",
    "                     'Detroit': 'Detroit Mercy',\n",
    "                     'UNLV':'Nevada-Las Vegas',\n",
    "                     'Ole Miss':'Mississippi',\n",
    "                     \"St. Joseph's\":\"Saint Joseph's\",\n",
    "                     'UCSB':'UC-Santa Barbara',\n",
    "                     'SMU': 'Southern Methodist',\n",
    "                     'USC':'South Carolina',\n",
    "                     'LSU':'Louisiana State',\n",
    "                     'UMass':'Massachusetts',\n",
    "                     'ETSU':'East Tennessee State'}\n",
    "    # TODO: for V2 add more corrections to the team_name_dict\n",
    "    if(team_name in team_name_dict):\n",
    "        return team_name_dict[team_name]\n",
    "    return team_name\n",
    "def get_school_stats(year, team_name):\n",
    "    return df_school[(df_school['year'] == year) & (df_school['team_name'] == team_name)]\n",
    "def get_vals(t_stats_list, key):\n",
    "    ret = []\n",
    "    for t_stat in t_stats_list:\n",
    "        ret.append(t_stat[key].squeeze())\n",
    "    return ret\n",
    "def get_team_stats_dict_with_t1_win(t1_stats, t2_stats, t1_wins):\n",
    "    return {'team_name_1':get_vals(t1_stats,'team_name'),'fg_pg_1':get_vals(t1_stats,'fg_pg'),'ft_pg_1':get_vals(t1_stats,'ft_pg'),\n",
    "            'three_pt_pg_1':get_vals(t1_stats,'three_pt_pg'),'orb_pg_1':get_vals(t1_stats,'orb_pg'),'drb_pg_1':get_vals(t1_stats,'drb_pg'),\n",
    "            'ast_pg_1':get_vals(t1_stats,'ast_pg'),'stl_pg_1':get_vals(t1_stats,'stl_pg'),'blk_pg_1':get_vals(t1_stats,'blk_pg'),\n",
    "            'tov_pg_1':get_vals(t1_stats,'tov_pg'),'pf_pg_1':get_vals(t1_stats,'pf_pg'), 'pt_pg_1':get_vals(t1_stats,'pt_pg'),\n",
    "            'opnt_pt_pg_1':get_vals(t1_stats,'opnt_pt_pg'),'fg_pct_1':get_vals(t1_stats,'fg_pct'),'three_p_pct_1':get_vals(t1_stats,'three_p_pct'),\n",
    "            'ft_pct_1':get_vals(t1_stats,'ft_pct'),'wl_pct_1':get_vals(t1_stats,'wl_pct'),'conf_wl_pct_1':get_vals(t1_stats,'conf_wl_pct'),\n",
    "            'srs_1':get_vals(t1_stats,'srs'),'sos_1':get_vals(t1_stats,'sos'),\n",
    "            'team_name_2':get_vals(t2_stats,'team_name'),'fg_pg_2':get_vals(t2_stats,'fg_pg'),'ft_pg_2':get_vals(t2_stats,'ft_pg'),\n",
    "            'three_pt_pg_2':get_vals(t2_stats,'three_pt_pg'),'orb_pg_2':get_vals(t2_stats,'orb_pg'),'drb_pg_2':get_vals(t2_stats,'drb_pg'),\n",
    "            'ast_pg_2':get_vals(t2_stats,'ast_pg'),'stl_pg_2':get_vals(t2_stats,'stl_pg'),'blk_pg_2':get_vals(t2_stats,'blk_pg'),\n",
    "            'tov_pg_2':get_vals(t2_stats,'tov_pg'),'pf_pg_2':get_vals(t2_stats,'pf_pg'), 'pt_pg_2':get_vals(t2_stats,'pt_pg'),\n",
    "            'opnt_pt_pg_2':get_vals(t2_stats,'opnt_pt_pg'),'fg_pct_2':get_vals(t2_stats,'fg_pct'),'three_p_pct_2':get_vals(t2_stats,'three_p_pct'),\n",
    "            'ft_pct_2':get_vals(t2_stats,'ft_pct'),'wl_pct_2':get_vals(t2_stats,'wl_pct'),'conf_wl_pct_2':get_vals(t2_stats,'conf_wl_pct'),\n",
    "            'srs_2':get_vals(t2_stats,'srs'),'sos_2':get_vals(t2_stats,'sos'),\n",
    "            't1_win':t1_wins}\n",
    "def get_team_stats_dict(t1_stats, t2_stats):\n",
    "    return {'team_name_1':get_vals(t1_stats,'team_name'),'fg_pg_1':get_vals(t1_stats,'fg_pg'),'ft_pg_1':get_vals(t1_stats,'ft_pg'),\n",
    "            'three_pt_pg_1':get_vals(t1_stats,'three_pt_pg'),'orb_pg_1':get_vals(t1_stats,'orb_pg'),'drb_pg_1':get_vals(t1_stats,'drb_pg'),\n",
    "            'ast_pg_1':get_vals(t1_stats,'ast_pg'),'stl_pg_1':get_vals(t1_stats,'stl_pg'),'blk_pg_1':get_vals(t1_stats,'blk_pg'),\n",
    "            'tov_pg_1':get_vals(t1_stats,'tov_pg'),'pf_pg_1':get_vals(t1_stats,'pf_pg'), 'pt_pg_1':get_vals(t1_stats,'pt_pg'),\n",
    "            'opnt_pt_pg_1':get_vals(t1_stats,'opnt_pt_pg'),'fg_pct_1':get_vals(t1_stats,'fg_pct'),'three_p_pct_1':get_vals(t1_stats,'three_p_pct'),\n",
    "            'ft_pct_1':get_vals(t1_stats,'ft_pct'),'wl_pct_1':get_vals(t1_stats,'wl_pct'),'conf_wl_pct_1':get_vals(t1_stats,'conf_wl_pct'),\n",
    "            'srs_1':get_vals(t1_stats,'srs'),'sos_1':get_vals(t1_stats,'sos'),\n",
    "            'team_name_2':get_vals(t2_stats,'team_name'),'fg_pg_2':get_vals(t2_stats,'fg_pg'),'ft_pg_2':get_vals(t2_stats,'ft_pg'),\n",
    "            'three_pt_pg_2':get_vals(t2_stats,'three_pt_pg'),'orb_pg_2':get_vals(t2_stats,'orb_pg'),'drb_pg_2':get_vals(t2_stats,'drb_pg'),\n",
    "            'ast_pg_2':get_vals(t2_stats,'ast_pg'),'stl_pg_2':get_vals(t2_stats,'stl_pg'),'blk_pg_2':get_vals(t2_stats,'blk_pg'),\n",
    "            'tov_pg_2':get_vals(t2_stats,'tov_pg'),'pf_pg_2':get_vals(t2_stats,'pf_pg'), 'pt_pg_2':get_vals(t2_stats,'pt_pg'),\n",
    "            'opnt_pt_pg_2':get_vals(t2_stats,'opnt_pt_pg'),'fg_pct_2':get_vals(t2_stats,'fg_pct'),'three_p_pct_2':get_vals(t2_stats,'three_p_pct'),\n",
    "            'ft_pct_2':get_vals(t2_stats,'ft_pct'),'wl_pct_2':get_vals(t2_stats,'wl_pct'),'conf_wl_pct_2':get_vals(t2_stats,'conf_wl_pct'),\n",
    "            'srs_2':get_vals(t2_stats,'srs'),'sos_2':get_vals(t2_stats,'sos')}\n",
    "def get_team_stats_dict_ps(t1_stats, t2_stats, t1_seeds, t2_seeds):\n",
    "    return {'team_name_1':get_vals(t1_stats,'team_name'),'fg_pg_1':get_vals(t1_stats,'fg_pg'),'ft_pg_1':get_vals(t1_stats,'ft_pg'),\n",
    "            'three_pt_pg_1':get_vals(t1_stats,'three_pt_pg'),'orb_pg_1':get_vals(t1_stats,'orb_pg'),'drb_pg_1':get_vals(t1_stats,'drb_pg'),\n",
    "            'ast_pg_1':get_vals(t1_stats,'ast_pg'),'stl_pg_1':get_vals(t1_stats,'stl_pg'),'blk_pg_1':get_vals(t1_stats,'blk_pg'),\n",
    "            'tov_pg_1':get_vals(t1_stats,'tov_pg'),'pf_pg_1':get_vals(t1_stats,'pf_pg'), 'pt_pg_1':get_vals(t1_stats,'pt_pg'),\n",
    "            'opnt_pt_pg_1':get_vals(t1_stats,'opnt_pt_pg'),'fg_pct_1':get_vals(t1_stats,'fg_pct'),'three_p_pct_1':get_vals(t1_stats,'three_p_pct'),\n",
    "            'ft_pct_1':get_vals(t1_stats,'ft_pct'),'wl_pct_1':get_vals(t1_stats,'wl_pct'),'conf_wl_pct_1':get_vals(t1_stats,'conf_wl_pct'),\n",
    "            'srs_1':get_vals(t1_stats,'srs'),'sos_1':get_vals(t1_stats,'sos'),\n",
    "            'team_name_2':get_vals(t2_stats,'team_name'),'fg_pg_2':get_vals(t2_stats,'fg_pg'),'ft_pg_2':get_vals(t2_stats,'ft_pg'),\n",
    "            'three_pt_pg_2':get_vals(t2_stats,'three_pt_pg'),'orb_pg_2':get_vals(t2_stats,'orb_pg'),'drb_pg_2':get_vals(t2_stats,'drb_pg'),\n",
    "            'ast_pg_2':get_vals(t2_stats,'ast_pg'),'stl_pg_2':get_vals(t2_stats,'stl_pg'),'blk_pg_2':get_vals(t2_stats,'blk_pg'),\n",
    "            'tov_pg_2':get_vals(t2_stats,'tov_pg'),'pf_pg_2':get_vals(t2_stats,'pf_pg'), 'pt_pg_2':get_vals(t2_stats,'pt_pg'),\n",
    "            'opnt_pt_pg_2':get_vals(t2_stats,'opnt_pt_pg'),'fg_pct_2':get_vals(t2_stats,'fg_pct'),'three_p_pct_2':get_vals(t2_stats,'three_p_pct'),\n",
    "            'ft_pct_2':get_vals(t2_stats,'ft_pct'),'wl_pct_2':get_vals(t2_stats,'wl_pct'),'conf_wl_pct_2':get_vals(t2_stats,'conf_wl_pct'),\n",
    "            'srs_2':get_vals(t2_stats,'srs'),'sos_2':get_vals(t2_stats,'sos'), 'team_1_seed':t1_seeds,\n",
    "            'team_2_seed':t2_seeds}\n",
    "def create_team_stats_df_w_t1_win(indeces_w_stats, t1_stats_list, t2_stats_list,t1_wins):\n",
    "    # Adds column for wether team 1 wins or not\n",
    "    # Assumes all lists are of the same length\n",
    "    return pd.DataFrame(get_team_stats_dict_with_t1_win(t1_stats_list, t2_stats_list,t1_wins), index = indeces_w_stats)\n",
    "def create_team_stats_df(indeces_w_stats, t1_stats_list, t2_stats_list):\n",
    "    # Assumes all lists are of the same length\n",
    "    return pd.DataFrame(get_team_stats_dict(t1_stats_list, t2_stats_list), index = indeces_w_stats)\n",
    "def create_team_stats_df_ps(indeces_w_stats, t1_stats_list, t2_stats_list, t1_seeds, t2_seeds):\n",
    "    # Only uses post season stats => inclu\n",
    "    # Assumes all lists are of the same length\n",
    "    return pd.DataFrame(get_team_stats_dict_ps(t1_stats_list, t2_stats_list, t1_seeds, t2_seeds), index = indeces_w_stats)\n",
    "def get_team_stats_df(game_df, should_print=False):\n",
    "    indeces_w_stats = []\n",
    "    t1_stats_list = []\n",
    "    t2_stats_list = []\n",
    "    t1_wins_list = []\n",
    "    for index, row in game_df.iterrows():\n",
    "        year = row['year']\n",
    "        team_1 = row['team_1_name']\n",
    "        team_2 = row['team_2_name']\n",
    "        team_1_score = row['team_1_score']\n",
    "        team_2_score = row['team_2_score']\n",
    "        t1_stats = get_school_stats(year, resolve_team_name(team_1))\n",
    "        t2_stats = get_school_stats(year, resolve_team_name(team_2))\n",
    "\n",
    "        if(len(t1_stats) > 0 and len(t2_stats) > 0):  \n",
    "            indeces_w_stats.append(index)\n",
    "            t1_stats_list.append(t1_stats)\n",
    "            t2_stats_list.append(t2_stats)\n",
    "            t1_wins_list.append(team_1_score > team_2_score)\n",
    "        else:         \n",
    "            if(should_print):\n",
    "                print(year)\n",
    "                if(len(t1_stats) < 1):\n",
    "                    print(team_1)\n",
    "                if(len(t2_stats) < 1):\n",
    "                    print(team_2)\n",
    "            \n",
    "    print(len(indeces_w_stats))\n",
    "    team_stats_df = create_team_stats_df_w_t1_win(indeces_w_stats, t1_stats_list, t2_stats_list, t1_wins_list)\n",
    "    return team_stats_df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "441\n"
     ]
    }
   ],
   "source": [
    "ps_team_stats_df = get_team_stats_df(df_ps_game, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_game_w_team_stats = pd.concat([df_ps_game, ps_team_stats_df], axis=1, join='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>team_1_name</th>\n",
       "      <th>team_1_score</th>\n",
       "      <th>team_1_seed</th>\n",
       "      <th>team_2_name</th>\n",
       "      <th>team_2_score</th>\n",
       "      <th>team_2_seed</th>\n",
       "      <th>team_name_1</th>\n",
       "      <th>fg_pg_1</th>\n",
       "      <th>ft_pg_1</th>\n",
       "      <th>...</th>\n",
       "      <th>pt_pg_2</th>\n",
       "      <th>opnt_pt_pg_2</th>\n",
       "      <th>fg_pct_2</th>\n",
       "      <th>three_p_pct_2</th>\n",
       "      <th>ft_pct_2</th>\n",
       "      <th>wl_pct_2</th>\n",
       "      <th>conf_wl_pct_2</th>\n",
       "      <th>srs_2</th>\n",
       "      <th>sos_2</th>\n",
       "      <th>t1_win</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011</td>\n",
       "      <td>UTSA</td>\n",
       "      <td>46.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>Ohio State</td>\n",
       "      <td>75.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Texas-San Antonio</td>\n",
       "      <td>23.588235</td>\n",
       "      <td>16.058824</td>\n",
       "      <td>...</td>\n",
       "      <td>77.135135</td>\n",
       "      <td>59.675676</td>\n",
       "      <td>0.494</td>\n",
       "      <td>0.423</td>\n",
       "      <td>0.701</td>\n",
       "      <td>0.919</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>25.84</td>\n",
       "      <td>8.38</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011</td>\n",
       "      <td>George Mason</td>\n",
       "      <td>61.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Villanova</td>\n",
       "      <td>57.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>George Mason</td>\n",
       "      <td>25.764706</td>\n",
       "      <td>14.558824</td>\n",
       "      <td>...</td>\n",
       "      <td>72.242424</td>\n",
       "      <td>65.424242</td>\n",
       "      <td>0.438</td>\n",
       "      <td>0.348</td>\n",
       "      <td>0.757</td>\n",
       "      <td>0.636</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>15.05</td>\n",
       "      <td>8.23</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011</td>\n",
       "      <td>Clemson</td>\n",
       "      <td>76.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>West Virginia</td>\n",
       "      <td>84.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Clemson</td>\n",
       "      <td>23.823529</td>\n",
       "      <td>14.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>69.787879</td>\n",
       "      <td>64.666667</td>\n",
       "      <td>0.429</td>\n",
       "      <td>0.337</td>\n",
       "      <td>0.711</td>\n",
       "      <td>0.636</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>16.15</td>\n",
       "      <td>11.03</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   year   team_1_name  team_1_score  team_1_seed    team_2_name  team_2_score  \\\n",
       "0  2011          UTSA          46.0         16.0     Ohio State          75.0   \n",
       "1  2011  George Mason          61.0          8.0      Villanova          57.0   \n",
       "2  2011       Clemson          76.0         12.0  West Virginia          84.0   \n",
       "\n",
       "   team_2_seed        team_name_1    fg_pg_1    ft_pg_1  ...    pt_pg_2  \\\n",
       "0          1.0  Texas-San Antonio  23.588235  16.058824  ...  77.135135   \n",
       "1          9.0       George Mason  25.764706  14.558824  ...  72.242424   \n",
       "2          5.0            Clemson  23.823529  14.500000  ...  69.787879   \n",
       "\n",
       "   opnt_pt_pg_2  fg_pct_2  three_p_pct_2  ft_pct_2  wl_pct_2  conf_wl_pct_2  \\\n",
       "0     59.675676     0.494          0.423     0.701     0.919       0.888889   \n",
       "1     65.424242     0.438          0.348     0.757     0.636       0.500000   \n",
       "2     64.666667     0.429          0.337     0.711     0.636       0.611111   \n",
       "\n",
       "   srs_2  sos_2  t1_win  \n",
       "0  25.84   8.38   False  \n",
       "1  15.05   8.23    True  \n",
       "2  16.15  11.03   False  \n",
       "\n",
       "[3 rows x 48 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps_game_w_team_stats.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(441, 48)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps_game_w_team_stats.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check team 1 winning true/false ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of True cases: 231 (52.38%)\n",
      "Number of False cases: 210 (47.62%)\n"
     ]
    }
   ],
   "source": [
    "t1_win_map = {True:1, False:0}\n",
    "ps_game_w_team_stats['t1_win'] = ps_game_w_team_stats['t1_win'].map(t1_win_map)\n",
    "num_true = len(ps_game_w_team_stats.loc[ps_game_w_team_stats['t1_win'] == True])\n",
    "num_false = len(ps_game_w_team_stats.loc[ps_game_w_team_stats['t1_win'] == False])\n",
    "print(\"Number of True cases: {0} ({1:2.2f}%)\".format(num_true, (num_true/(num_true+num_false))*100))\n",
    "print(\"Number of False cases: {0} ({1:2.2f}%)\".format(num_false, (num_false/(num_true+num_false))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.model_selection\n",
    "ps_feature_col_names = ['team_1_seed', 'team_2_seed','fg_pg_1','ft_pg_1',\n",
    "            'three_pt_pg_1','orb_pg_1','drb_pg_1',\n",
    "            'ast_pg_1','stl_pg_1','blk_pg_1',\n",
    "            'tov_pg_1','pf_pg_1', 'pt_pg_1',\n",
    "            'opnt_pt_pg_1','fg_pct_1','three_p_pct_1',\n",
    "            'ft_pct_1','wl_pct_1','conf_wl_pct_1',\n",
    "            'srs_1','sos_1',\n",
    "            'fg_pg_2','ft_pg_2',\n",
    "            'three_pt_pg_2','orb_pg_2','drb_pg_2',\n",
    "            'ast_pg_2','stl_pg_2','blk_pg_2',\n",
    "            'tov_pg_2','pf_pg_2', 'pt_pg_2',\n",
    "            'opnt_pt_pg_2','fg_pct_2','three_p_pct_2',\n",
    "            'ft_pct_2','wl_pct_2','conf_wl_pct_2',\n",
    "            'srs_2','sos_2'\n",
    "            ]\n",
    "ps_predict_class_names = ['t1_win']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_features(data, col_names):\n",
    "    scaled_features = {}\n",
    "    for col_name in col_names:\n",
    "        mean, std = data[col_name].values.mean(), data[col_name].values.std()\n",
    "        scaled_features[col_name] = [mean, std]\n",
    "        data.loc[:, col_name] = (data[col_name].values - mean)/std\n",
    "    return scaled_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "scale_features(ps_game_w_team_stats, ps_feature_col_names)\n",
    "ps_x = ps_game_w_team_stats[ps_feature_col_names].values\n",
    "ps_y = ps_game_w_team_stats[ps_predict_class_names].values\n",
    "print(type(ps_x))\n",
    "split_test_size = 0.20\n",
    "ps_x_train, ps_x_test, ps_y_train, ps_y_test = sklearn.model_selection.train_test_split(ps_x, ps_y, test_size=split_test_size, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79.82% in training set\n",
      "20.18% in test set\n"
     ]
    }
   ],
   "source": [
    "print(\"{0:0.2f}% in training set\".format((len(ps_x_train)/len(ps_game_w_team_stats.index))*100))\n",
    "print(\"{0:0.2f}% in test set\".format((len(ps_x_test)/len(ps_game_w_team_stats.index))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.impute\n",
    "\n",
    "#Impute with mean all 0 readings\n",
    "fill_0 = sklearn.impute.SimpleImputer(missing_values=0, strategy=\"mean\")\n",
    "\n",
    "ps_x_train = fill_0.fit_transform(ps_x_train)\n",
    "ps_x_test = fill_0.fit_transform(ps_x_test)\n",
    "# TODO : impute incorrect negative values such anything other than (SOS and SRS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "print(len(ps_x_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[-1.0816,  0.0851,  1.0144, -0.4947,  0.1144,  1.2015,  1.4245, -0.3647,\n",
      "         -0.0889,  1.2608, -0.5043,  1.0103,  0.6717,  0.2003, -0.1032, -0.1998,\n",
      "         -0.7121, -0.1110, -0.4200,  1.0647,  1.1001,  0.2217, -1.2754,  2.1200,\n",
      "         -2.3871, -2.1503, -0.1875, -0.6372, -1.1266, -1.8976, -1.2824,  0.2317,\n",
      "          0.3694,  1.0633,  0.9383,  2.0491, -0.7109, -1.3030,  0.5437,  0.8732],\n",
      "        [-1.2970, -0.5933,  0.6044, -0.9893, -0.4207,  0.6199, -1.2488,  0.5748,\n",
      "          1.7752,  2.5122, -0.8299, -1.0444,  0.0125, -0.8831,  0.3347, -0.4212,\n",
      "         -0.3641,  1.8815,  1.6986,  0.8822,  0.4309, -2.0038, -1.8080,  0.8467,\n",
      "         -0.9764, -1.6102, -1.3214, -1.2420, -0.5490, -2.0893, -1.7203, -2.0458,\n",
      "         -2.4815, -1.6210,  0.2673,  0.8646, -0.3072, -0.4647,  0.5795,  0.4433],\n",
      "        [-1.0816,  0.7635, -0.6064,  1.3003,  0.8983, -0.2188,  0.5937,  1.3085,\n",
      "         -1.6721, -0.2411, -0.7012, -1.2169,  0.2457,  0.0977,  0.0282,  0.9441,\n",
      "          0.3900,  0.5279,  0.4275,  0.4833,  0.5186, -0.8542, -0.5247, -0.5054,\n",
      "          0.6715,  1.1957, -1.0050,  1.4917,  1.6200,  2.9165,  0.9621, -1.0009,\n",
      "         -0.5734, -1.1582, -1.0747, -1.3266, -0.7959, -0.3075, -0.2505,  0.0446]]), tensor([[0.],\n",
      "        [1.],\n",
      "        [0.]])]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "model = nn.Sequential(nn.Linear(40, 30),\n",
    "                     nn.ReLU(),\n",
    "                     nn.Linear(30, 30),\n",
    "                     nn.ReLU(),\n",
    "                     nn.Linear(30, 10),\n",
    "                     nn.ReLU(),\n",
    "                     nn.Linear(10, 1),\n",
    "                     nn.Sigmoid())\n",
    "trainset = TensorDataset(torch.from_numpy(ps_x_train).float(), torch.from_numpy(ps_y_train).float())\n",
    "trainloader = DataLoader(trainset, batch_size=3, shuffle=True)\n",
    "testset = TensorDataset(torch.from_numpy(ps_x_test).float(), torch.from_numpy(ps_y_test).float())\n",
    "testloader = DataLoader(testset, batch_size=3, shuffle=True)\n",
    "print(next(iter(testloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: True\n"
     ]
    }
   ],
   "source": [
    "# check if CUDA is available\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print ('Using CUDA: {}'.format(use_cuda))\n",
    "if use_cuda:\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_epochs, train_loader, model, optimizer, criterion, use_cuda, save_path):\n",
    "    \"\"\"returns trained model\"\"\"\n",
    "    # initialize tracker for minimum validation loss\n",
    "    # TODO: fix target tensors always zero for some reason\n",
    "    valid_loss_min = np.Inf \n",
    "    print_loss_count = 20\n",
    "    cuda_refresh_count = 5\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        # initialize variables to monitor training and validation loss\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train()\n",
    "        train_load_iter = iter(train_loader)\n",
    "        for i in range(len(train_loader)):\n",
    "            data, target = next(train_load_iter)\n",
    "            # print('data {}'.format(data.shape))\n",
    "            # print('target {}'.format(target))\n",
    "            # move to GPU\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "\n",
    "            optimizer.zero_grad()    \n",
    "            output = model(data)\n",
    "            # print('target {}'.format(target))\n",
    "            # print('output {}'.format(output))\n",
    "            # _, argmax = output.max(-1)\n",
    "            # print('argmax', argmax)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "                \n",
    "            train_loss = train_loss + ((1 / (i + 1)) * (loss.data - train_loss))\n",
    "            if i % print_loss_count == 0:\n",
    "                print('Epoch %d, Batch %d Loss %.6f' % (epoch, i + 1, train_loss))\n",
    "            if i % cuda_refresh_count == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "\n",
    "    # return trained model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 1 Loss 0.676714\n",
      "Epoch 1, Batch 21 Loss 0.697408\n",
      "Epoch 1, Batch 41 Loss 0.692889\n",
      "Epoch 1, Batch 61 Loss 0.691939\n",
      "Epoch 1, Batch 81 Loss 0.691744\n",
      "Epoch 1, Batch 101 Loss 0.692130\n",
      "Epoch 2, Batch 1 Loss 0.678610\n",
      "Epoch 2, Batch 21 Loss 0.686374\n",
      "Epoch 2, Batch 41 Loss 0.689130\n",
      "Epoch 2, Batch 61 Loss 0.693023\n",
      "Epoch 2, Batch 81 Loss 0.692332\n",
      "Epoch 2, Batch 101 Loss 0.691015\n",
      "Epoch 3, Batch 1 Loss 0.691336\n",
      "Epoch 3, Batch 21 Loss 0.687350\n",
      "Epoch 3, Batch 41 Loss 0.690566\n",
      "Epoch 3, Batch 61 Loss 0.691350\n",
      "Epoch 3, Batch 81 Loss 0.689946\n",
      "Epoch 3, Batch 101 Loss 0.690885\n",
      "Epoch 4, Batch 1 Loss 0.703332\n",
      "Epoch 4, Batch 21 Loss 0.686845\n",
      "Epoch 4, Batch 41 Loss 0.687674\n",
      "Epoch 4, Batch 61 Loss 0.689289\n",
      "Epoch 4, Batch 81 Loss 0.689807\n",
      "Epoch 4, Batch 101 Loss 0.689221\n",
      "Epoch 5, Batch 1 Loss 0.698609\n",
      "Epoch 5, Batch 21 Loss 0.685305\n",
      "Epoch 5, Batch 41 Loss 0.685491\n",
      "Epoch 5, Batch 61 Loss 0.686644\n",
      "Epoch 5, Batch 81 Loss 0.686567\n",
      "Epoch 5, Batch 101 Loss 0.688496\n",
      "Epoch 6, Batch 1 Loss 0.700044\n",
      "Epoch 6, Batch 21 Loss 0.684129\n",
      "Epoch 6, Batch 41 Loss 0.687498\n",
      "Epoch 6, Batch 61 Loss 0.688606\n",
      "Epoch 6, Batch 81 Loss 0.688757\n",
      "Epoch 6, Batch 101 Loss 0.688408\n",
      "Epoch 7, Batch 1 Loss 0.699345\n",
      "Epoch 7, Batch 21 Loss 0.686972\n",
      "Epoch 7, Batch 41 Loss 0.686888\n",
      "Epoch 7, Batch 61 Loss 0.685273\n",
      "Epoch 7, Batch 81 Loss 0.687117\n",
      "Epoch 7, Batch 101 Loss 0.687047\n",
      "Epoch 8, Batch 1 Loss 0.701126\n",
      "Epoch 8, Batch 21 Loss 0.687466\n",
      "Epoch 8, Batch 41 Loss 0.684710\n",
      "Epoch 8, Batch 61 Loss 0.686186\n",
      "Epoch 8, Batch 81 Loss 0.685333\n",
      "Epoch 8, Batch 101 Loss 0.684922\n",
      "Epoch 9, Batch 1 Loss 0.682885\n",
      "Epoch 9, Batch 21 Loss 0.692387\n",
      "Epoch 9, Batch 41 Loss 0.687563\n",
      "Epoch 9, Batch 61 Loss 0.686923\n",
      "Epoch 9, Batch 81 Loss 0.685630\n",
      "Epoch 9, Batch 101 Loss 0.685892\n",
      "Epoch 10, Batch 1 Loss 0.667482\n",
      "Epoch 10, Batch 21 Loss 0.682937\n",
      "Epoch 10, Batch 41 Loss 0.685327\n",
      "Epoch 10, Batch 61 Loss 0.685122\n",
      "Epoch 10, Batch 81 Loss 0.683545\n",
      "Epoch 10, Batch 101 Loss 0.683776\n",
      "Epoch 11, Batch 1 Loss 0.635306\n",
      "Epoch 11, Batch 21 Loss 0.681647\n",
      "Epoch 11, Batch 41 Loss 0.682554\n",
      "Epoch 11, Batch 61 Loss 0.684897\n",
      "Epoch 11, Batch 81 Loss 0.682998\n",
      "Epoch 11, Batch 101 Loss 0.683112\n",
      "Epoch 12, Batch 1 Loss 0.719012\n",
      "Epoch 12, Batch 21 Loss 0.681635\n",
      "Epoch 12, Batch 41 Loss 0.685502\n",
      "Epoch 12, Batch 61 Loss 0.686510\n",
      "Epoch 12, Batch 81 Loss 0.684708\n",
      "Epoch 12, Batch 101 Loss 0.683582\n",
      "Epoch 13, Batch 1 Loss 0.690932\n",
      "Epoch 13, Batch 21 Loss 0.679601\n",
      "Epoch 13, Batch 41 Loss 0.679895\n",
      "Epoch 13, Batch 61 Loss 0.679677\n",
      "Epoch 13, Batch 81 Loss 0.683474\n",
      "Epoch 13, Batch 101 Loss 0.682364\n",
      "Epoch 14, Batch 1 Loss 0.623996\n",
      "Epoch 14, Batch 21 Loss 0.680695\n",
      "Epoch 14, Batch 41 Loss 0.680594\n",
      "Epoch 14, Batch 61 Loss 0.681387\n",
      "Epoch 14, Batch 81 Loss 0.679009\n",
      "Epoch 14, Batch 101 Loss 0.679260\n",
      "Epoch 15, Batch 1 Loss 0.608614\n",
      "Epoch 15, Batch 21 Loss 0.673611\n",
      "Epoch 15, Batch 41 Loss 0.679812\n",
      "Epoch 15, Batch 61 Loss 0.682049\n",
      "Epoch 15, Batch 81 Loss 0.678921\n",
      "Epoch 15, Batch 101 Loss 0.679147\n",
      "Epoch 16, Batch 1 Loss 0.654337\n",
      "Epoch 16, Batch 21 Loss 0.677770\n",
      "Epoch 16, Batch 41 Loss 0.678554\n",
      "Epoch 16, Batch 61 Loss 0.683992\n",
      "Epoch 16, Batch 81 Loss 0.680402\n",
      "Epoch 16, Batch 101 Loss 0.679150\n",
      "Epoch 17, Batch 1 Loss 0.683798\n",
      "Epoch 17, Batch 21 Loss 0.675427\n",
      "Epoch 17, Batch 41 Loss 0.675718\n",
      "Epoch 17, Batch 61 Loss 0.679057\n",
      "Epoch 17, Batch 81 Loss 0.679621\n",
      "Epoch 17, Batch 101 Loss 0.679364\n",
      "Epoch 18, Batch 1 Loss 0.617719\n",
      "Epoch 18, Batch 21 Loss 0.679371\n",
      "Epoch 18, Batch 41 Loss 0.684151\n",
      "Epoch 18, Batch 61 Loss 0.679339\n",
      "Epoch 18, Batch 81 Loss 0.679792\n",
      "Epoch 18, Batch 101 Loss 0.675978\n",
      "Epoch 19, Batch 1 Loss 0.665733\n",
      "Epoch 19, Batch 21 Loss 0.676195\n",
      "Epoch 19, Batch 41 Loss 0.682731\n",
      "Epoch 19, Batch 61 Loss 0.677427\n",
      "Epoch 19, Batch 81 Loss 0.676517\n",
      "Epoch 19, Batch 101 Loss 0.677271\n",
      "Epoch 20, Batch 1 Loss 0.704108\n",
      "Epoch 20, Batch 21 Loss 0.678290\n",
      "Epoch 20, Batch 41 Loss 0.670790\n",
      "Epoch 20, Batch 61 Loss 0.669103\n",
      "Epoch 20, Batch 81 Loss 0.674777\n",
      "Epoch 20, Batch 101 Loss 0.673009\n",
      "Epoch 21, Batch 1 Loss 0.720529\n",
      "Epoch 21, Batch 21 Loss 0.686114\n",
      "Epoch 21, Batch 41 Loss 0.675864\n",
      "Epoch 21, Batch 61 Loss 0.672578\n",
      "Epoch 21, Batch 81 Loss 0.675965\n",
      "Epoch 21, Batch 101 Loss 0.677220\n",
      "Epoch 22, Batch 1 Loss 0.742502\n",
      "Epoch 22, Batch 21 Loss 0.673939\n",
      "Epoch 22, Batch 41 Loss 0.673361\n",
      "Epoch 22, Batch 61 Loss 0.671930\n",
      "Epoch 22, Batch 81 Loss 0.669594\n",
      "Epoch 22, Batch 101 Loss 0.672081\n",
      "Epoch 23, Batch 1 Loss 0.613479\n",
      "Epoch 23, Batch 21 Loss 0.672200\n",
      "Epoch 23, Batch 41 Loss 0.669596\n",
      "Epoch 23, Batch 61 Loss 0.669257\n",
      "Epoch 23, Batch 81 Loss 0.672525\n",
      "Epoch 23, Batch 101 Loss 0.671664\n",
      "Epoch 24, Batch 1 Loss 0.643478\n",
      "Epoch 24, Batch 21 Loss 0.658677\n",
      "Epoch 24, Batch 41 Loss 0.660547\n",
      "Epoch 24, Batch 61 Loss 0.665383\n",
      "Epoch 24, Batch 81 Loss 0.670072\n",
      "Epoch 24, Batch 101 Loss 0.668312\n",
      "Epoch 25, Batch 1 Loss 0.750146\n",
      "Epoch 25, Batch 21 Loss 0.676955\n",
      "Epoch 25, Batch 41 Loss 0.671278\n",
      "Epoch 25, Batch 61 Loss 0.670733\n",
      "Epoch 25, Batch 81 Loss 0.669102\n",
      "Epoch 25, Batch 101 Loss 0.670278\n",
      "Epoch 26, Batch 1 Loss 0.669650\n",
      "Epoch 26, Batch 21 Loss 0.664773\n",
      "Epoch 26, Batch 41 Loss 0.665379\n",
      "Epoch 26, Batch 61 Loss 0.664163\n",
      "Epoch 26, Batch 81 Loss 0.664943\n",
      "Epoch 26, Batch 101 Loss 0.665179\n",
      "Epoch 27, Batch 1 Loss 0.689588\n",
      "Epoch 27, Batch 21 Loss 0.659579\n",
      "Epoch 27, Batch 41 Loss 0.659210\n",
      "Epoch 27, Batch 61 Loss 0.665932\n",
      "Epoch 27, Batch 81 Loss 0.664924\n",
      "Epoch 27, Batch 101 Loss 0.669001\n",
      "Epoch 28, Batch 1 Loss 0.664806\n",
      "Epoch 28, Batch 21 Loss 0.675094\n",
      "Epoch 28, Batch 41 Loss 0.663228\n",
      "Epoch 28, Batch 61 Loss 0.667062\n",
      "Epoch 28, Batch 81 Loss 0.668627\n",
      "Epoch 28, Batch 101 Loss 0.666559\n",
      "Epoch 29, Batch 1 Loss 0.764193\n",
      "Epoch 29, Batch 21 Loss 0.663957\n",
      "Epoch 29, Batch 41 Loss 0.659529\n",
      "Epoch 29, Batch 61 Loss 0.671507\n",
      "Epoch 29, Batch 81 Loss 0.671288\n",
      "Epoch 29, Batch 101 Loss 0.666557\n",
      "Epoch 30, Batch 1 Loss 0.624598\n",
      "Epoch 30, Batch 21 Loss 0.664328\n",
      "Epoch 30, Batch 41 Loss 0.663292\n",
      "Epoch 30, Batch 61 Loss 0.660904\n",
      "Epoch 30, Batch 81 Loss 0.660494\n",
      "Epoch 30, Batch 101 Loss 0.664089\n",
      "Epoch 31, Batch 1 Loss 0.604109\n",
      "Epoch 31, Batch 21 Loss 0.655018\n",
      "Epoch 31, Batch 41 Loss 0.658910\n",
      "Epoch 31, Batch 61 Loss 0.663095\n",
      "Epoch 31, Batch 81 Loss 0.662684\n",
      "Epoch 31, Batch 101 Loss 0.662423\n",
      "Epoch 32, Batch 1 Loss 0.644418\n",
      "Epoch 32, Batch 21 Loss 0.669518\n",
      "Epoch 32, Batch 41 Loss 0.658531\n",
      "Epoch 32, Batch 61 Loss 0.657524\n",
      "Epoch 32, Batch 81 Loss 0.660011\n",
      "Epoch 32, Batch 101 Loss 0.661352\n",
      "Epoch 33, Batch 1 Loss 0.740969\n",
      "Epoch 33, Batch 21 Loss 0.662930\n",
      "Epoch 33, Batch 41 Loss 0.656771\n",
      "Epoch 33, Batch 61 Loss 0.663812\n",
      "Epoch 33, Batch 81 Loss 0.660382\n",
      "Epoch 33, Batch 101 Loss 0.658788\n",
      "Epoch 34, Batch 1 Loss 0.761787\n",
      "Epoch 34, Batch 21 Loss 0.657881\n",
      "Epoch 34, Batch 41 Loss 0.652126\n",
      "Epoch 34, Batch 61 Loss 0.656684\n",
      "Epoch 34, Batch 81 Loss 0.655229\n",
      "Epoch 34, Batch 101 Loss 0.657195\n",
      "Epoch 35, Batch 1 Loss 0.597173\n",
      "Epoch 35, Batch 21 Loss 0.659147\n",
      "Epoch 35, Batch 41 Loss 0.653072\n",
      "Epoch 35, Batch 61 Loss 0.656823\n",
      "Epoch 35, Batch 81 Loss 0.655925\n",
      "Epoch 35, Batch 101 Loss 0.653391\n",
      "Epoch 36, Batch 1 Loss 0.762924\n",
      "Epoch 36, Batch 21 Loss 0.659928\n",
      "Epoch 36, Batch 41 Loss 0.659901\n",
      "Epoch 36, Batch 61 Loss 0.663759\n",
      "Epoch 36, Batch 81 Loss 0.658518\n",
      "Epoch 36, Batch 101 Loss 0.658686\n",
      "Epoch 37, Batch 1 Loss 0.599452\n",
      "Epoch 37, Batch 21 Loss 0.635025\n",
      "Epoch 37, Batch 41 Loss 0.652012\n",
      "Epoch 37, Batch 61 Loss 0.656720\n",
      "Epoch 37, Batch 81 Loss 0.653738\n",
      "Epoch 37, Batch 101 Loss 0.654816\n",
      "Epoch 38, Batch 1 Loss 0.620314\n",
      "Epoch 38, Batch 21 Loss 0.672110\n",
      "Epoch 38, Batch 41 Loss 0.662100\n",
      "Epoch 38, Batch 61 Loss 0.652218\n",
      "Epoch 38, Batch 81 Loss 0.656552\n",
      "Epoch 38, Batch 101 Loss 0.655031\n",
      "Epoch 39, Batch 1 Loss 0.700538\n",
      "Epoch 39, Batch 21 Loss 0.657537\n",
      "Epoch 39, Batch 41 Loss 0.644401\n",
      "Epoch 39, Batch 61 Loss 0.652330\n",
      "Epoch 39, Batch 81 Loss 0.652791\n",
      "Epoch 39, Batch 101 Loss 0.650043\n",
      "Epoch 40, Batch 1 Loss 0.584667\n",
      "Epoch 40, Batch 21 Loss 0.619743\n",
      "Epoch 40, Batch 41 Loss 0.640622\n",
      "Epoch 40, Batch 61 Loss 0.644863\n",
      "Epoch 40, Batch 81 Loss 0.641705\n",
      "Epoch 40, Batch 101 Loss 0.645694\n",
      "Epoch 41, Batch 1 Loss 0.728324\n",
      "Epoch 41, Batch 21 Loss 0.623390\n",
      "Epoch 41, Batch 41 Loss 0.637026\n",
      "Epoch 41, Batch 61 Loss 0.643800\n",
      "Epoch 41, Batch 81 Loss 0.643873\n",
      "Epoch 41, Batch 101 Loss 0.647630\n",
      "Epoch 42, Batch 1 Loss 0.583803\n",
      "Epoch 42, Batch 21 Loss 0.636942\n",
      "Epoch 42, Batch 41 Loss 0.636553\n",
      "Epoch 42, Batch 61 Loss 0.643688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42, Batch 81 Loss 0.645701\n",
      "Epoch 42, Batch 101 Loss 0.644455\n",
      "Epoch 43, Batch 1 Loss 0.614976\n",
      "Epoch 43, Batch 21 Loss 0.629384\n",
      "Epoch 43, Batch 41 Loss 0.645390\n",
      "Epoch 43, Batch 61 Loss 0.644328\n",
      "Epoch 43, Batch 81 Loss 0.647051\n",
      "Epoch 43, Batch 101 Loss 0.645483\n",
      "Epoch 44, Batch 1 Loss 0.480838\n",
      "Epoch 44, Batch 21 Loss 0.646055\n",
      "Epoch 44, Batch 41 Loss 0.625246\n",
      "Epoch 44, Batch 61 Loss 0.634952\n",
      "Epoch 44, Batch 81 Loss 0.635735\n",
      "Epoch 44, Batch 101 Loss 0.638113\n",
      "Epoch 45, Batch 1 Loss 0.663670\n",
      "Epoch 45, Batch 21 Loss 0.650093\n",
      "Epoch 45, Batch 41 Loss 0.638925\n",
      "Epoch 45, Batch 61 Loss 0.635027\n",
      "Epoch 45, Batch 81 Loss 0.643266\n",
      "Epoch 45, Batch 101 Loss 0.640858\n",
      "Epoch 46, Batch 1 Loss 0.572070\n",
      "Epoch 46, Batch 21 Loss 0.646685\n",
      "Epoch 46, Batch 41 Loss 0.635748\n",
      "Epoch 46, Batch 61 Loss 0.634514\n",
      "Epoch 46, Batch 81 Loss 0.636604\n",
      "Epoch 46, Batch 101 Loss 0.633857\n",
      "Epoch 47, Batch 1 Loss 0.625921\n",
      "Epoch 47, Batch 21 Loss 0.628945\n",
      "Epoch 47, Batch 41 Loss 0.640913\n",
      "Epoch 47, Batch 61 Loss 0.637245\n",
      "Epoch 47, Batch 81 Loss 0.635940\n",
      "Epoch 47, Batch 101 Loss 0.634216\n",
      "Epoch 48, Batch 1 Loss 0.656191\n",
      "Epoch 48, Batch 21 Loss 0.606442\n",
      "Epoch 48, Batch 41 Loss 0.616525\n",
      "Epoch 48, Batch 61 Loss 0.628488\n",
      "Epoch 48, Batch 81 Loss 0.633508\n",
      "Epoch 48, Batch 101 Loss 0.631002\n",
      "Epoch 49, Batch 1 Loss 0.641867\n",
      "Epoch 49, Batch 21 Loss 0.627358\n",
      "Epoch 49, Batch 41 Loss 0.626133\n",
      "Epoch 49, Batch 61 Loss 0.646713\n",
      "Epoch 49, Batch 81 Loss 0.640645\n",
      "Epoch 49, Batch 101 Loss 0.637207\n",
      "Epoch 50, Batch 1 Loss 0.586526\n",
      "Epoch 50, Batch 21 Loss 0.599672\n",
      "Epoch 50, Batch 41 Loss 0.619046\n",
      "Epoch 50, Batch 61 Loss 0.622563\n",
      "Epoch 50, Batch 81 Loss 0.620059\n",
      "Epoch 50, Batch 101 Loss 0.619697\n",
      "Epoch 51, Batch 1 Loss 0.824308\n",
      "Epoch 51, Batch 21 Loss 0.631479\n",
      "Epoch 51, Batch 41 Loss 0.622467\n",
      "Epoch 51, Batch 61 Loss 0.615557\n",
      "Epoch 51, Batch 81 Loss 0.623510\n",
      "Epoch 51, Batch 101 Loss 0.625704\n",
      "Epoch 52, Batch 1 Loss 0.462711\n",
      "Epoch 52, Batch 21 Loss 0.637180\n",
      "Epoch 52, Batch 41 Loss 0.626485\n",
      "Epoch 52, Batch 61 Loss 0.621355\n",
      "Epoch 52, Batch 81 Loss 0.614061\n",
      "Epoch 52, Batch 101 Loss 0.623468\n",
      "Epoch 53, Batch 1 Loss 0.492598\n",
      "Epoch 53, Batch 21 Loss 0.596269\n",
      "Epoch 53, Batch 41 Loss 0.612627\n",
      "Epoch 53, Batch 61 Loss 0.624204\n",
      "Epoch 53, Batch 81 Loss 0.621507\n",
      "Epoch 53, Batch 101 Loss 0.618766\n",
      "Epoch 54, Batch 1 Loss 0.579893\n",
      "Epoch 54, Batch 21 Loss 0.617511\n",
      "Epoch 54, Batch 41 Loss 0.610365\n",
      "Epoch 54, Batch 61 Loss 0.610280\n",
      "Epoch 54, Batch 81 Loss 0.617758\n",
      "Epoch 54, Batch 101 Loss 0.620465\n",
      "Epoch 55, Batch 1 Loss 0.634885\n",
      "Epoch 55, Batch 21 Loss 0.626790\n",
      "Epoch 55, Batch 41 Loss 0.617102\n",
      "Epoch 55, Batch 61 Loss 0.617624\n",
      "Epoch 55, Batch 81 Loss 0.612419\n",
      "Epoch 55, Batch 101 Loss 0.613460\n",
      "Epoch 56, Batch 1 Loss 0.481901\n",
      "Epoch 56, Batch 21 Loss 0.601393\n",
      "Epoch 56, Batch 41 Loss 0.610797\n",
      "Epoch 56, Batch 61 Loss 0.612311\n",
      "Epoch 56, Batch 81 Loss 0.613063\n",
      "Epoch 56, Batch 101 Loss 0.612719\n",
      "Epoch 57, Batch 1 Loss 0.730906\n",
      "Epoch 57, Batch 21 Loss 0.632452\n",
      "Epoch 57, Batch 41 Loss 0.615499\n",
      "Epoch 57, Batch 61 Loss 0.619148\n",
      "Epoch 57, Batch 81 Loss 0.615446\n",
      "Epoch 57, Batch 101 Loss 0.612251\n",
      "Epoch 58, Batch 1 Loss 0.448393\n",
      "Epoch 58, Batch 21 Loss 0.581828\n",
      "Epoch 58, Batch 41 Loss 0.580979\n",
      "Epoch 58, Batch 61 Loss 0.578595\n",
      "Epoch 58, Batch 81 Loss 0.608076\n",
      "Epoch 58, Batch 101 Loss 0.605905\n",
      "Epoch 59, Batch 1 Loss 0.536174\n",
      "Epoch 59, Batch 21 Loss 0.601586\n",
      "Epoch 59, Batch 41 Loss 0.588588\n",
      "Epoch 59, Batch 61 Loss 0.597096\n",
      "Epoch 59, Batch 81 Loss 0.614221\n",
      "Epoch 59, Batch 101 Loss 0.609134\n",
      "Epoch 60, Batch 1 Loss 0.399484\n",
      "Epoch 60, Batch 21 Loss 0.576387\n",
      "Epoch 60, Batch 41 Loss 0.577493\n",
      "Epoch 60, Batch 61 Loss 0.600099\n",
      "Epoch 60, Batch 81 Loss 0.601124\n",
      "Epoch 60, Batch 101 Loss 0.602286\n",
      "Epoch 61, Batch 1 Loss 0.556679\n",
      "Epoch 61, Batch 21 Loss 0.593969\n",
      "Epoch 61, Batch 41 Loss 0.610899\n",
      "Epoch 61, Batch 61 Loss 0.620950\n",
      "Epoch 61, Batch 81 Loss 0.607743\n",
      "Epoch 61, Batch 101 Loss 0.604904\n",
      "Epoch 62, Batch 1 Loss 0.817092\n",
      "Epoch 62, Batch 21 Loss 0.652996\n",
      "Epoch 62, Batch 41 Loss 0.616997\n",
      "Epoch 62, Batch 61 Loss 0.609978\n",
      "Epoch 62, Batch 81 Loss 0.619946\n",
      "Epoch 62, Batch 101 Loss 0.609870\n",
      "Epoch 63, Batch 1 Loss 0.766100\n",
      "Epoch 63, Batch 21 Loss 0.592652\n",
      "Epoch 63, Batch 41 Loss 0.582947\n",
      "Epoch 63, Batch 61 Loss 0.601991\n",
      "Epoch 63, Batch 81 Loss 0.597900\n",
      "Epoch 63, Batch 101 Loss 0.597813\n",
      "Epoch 64, Batch 1 Loss 0.585718\n",
      "Epoch 64, Batch 21 Loss 0.595863\n",
      "Epoch 64, Batch 41 Loss 0.584888\n",
      "Epoch 64, Batch 61 Loss 0.594406\n",
      "Epoch 64, Batch 81 Loss 0.594084\n",
      "Epoch 64, Batch 101 Loss 0.597308\n",
      "Epoch 65, Batch 1 Loss 0.430906\n",
      "Epoch 65, Batch 21 Loss 0.535875\n",
      "Epoch 65, Batch 41 Loss 0.575579\n",
      "Epoch 65, Batch 61 Loss 0.594602\n",
      "Epoch 65, Batch 81 Loss 0.589123\n",
      "Epoch 65, Batch 101 Loss 0.585446\n",
      "Epoch 66, Batch 1 Loss 0.602135\n",
      "Epoch 66, Batch 21 Loss 0.604735\n",
      "Epoch 66, Batch 41 Loss 0.619408\n",
      "Epoch 66, Batch 61 Loss 0.624627\n",
      "Epoch 66, Batch 81 Loss 0.604354\n",
      "Epoch 66, Batch 101 Loss 0.588535\n",
      "Epoch 67, Batch 1 Loss 0.523745\n",
      "Epoch 67, Batch 21 Loss 0.519763\n",
      "Epoch 67, Batch 41 Loss 0.571470\n",
      "Epoch 67, Batch 61 Loss 0.578261\n",
      "Epoch 67, Batch 81 Loss 0.586139\n",
      "Epoch 67, Batch 101 Loss 0.584034\n",
      "Epoch 68, Batch 1 Loss 0.534294\n",
      "Epoch 68, Batch 21 Loss 0.547217\n",
      "Epoch 68, Batch 41 Loss 0.572888\n",
      "Epoch 68, Batch 61 Loss 0.567480\n",
      "Epoch 68, Batch 81 Loss 0.576767\n",
      "Epoch 68, Batch 101 Loss 0.585340\n",
      "Epoch 69, Batch 1 Loss 0.576977\n",
      "Epoch 69, Batch 21 Loss 0.567204\n",
      "Epoch 69, Batch 41 Loss 0.569777\n",
      "Epoch 69, Batch 61 Loss 0.581899\n",
      "Epoch 69, Batch 81 Loss 0.588675\n",
      "Epoch 69, Batch 101 Loss 0.582092\n",
      "Epoch 70, Batch 1 Loss 0.690785\n",
      "Epoch 70, Batch 21 Loss 0.604918\n",
      "Epoch 70, Batch 41 Loss 0.578254\n",
      "Epoch 70, Batch 61 Loss 0.563774\n",
      "Epoch 70, Batch 81 Loss 0.578186\n",
      "Epoch 70, Batch 101 Loss 0.582040\n",
      "Epoch 71, Batch 1 Loss 0.571338\n",
      "Epoch 71, Batch 21 Loss 0.630720\n",
      "Epoch 71, Batch 41 Loss 0.607681\n",
      "Epoch 71, Batch 61 Loss 0.602834\n",
      "Epoch 71, Batch 81 Loss 0.582897\n",
      "Epoch 71, Batch 101 Loss 0.573753\n",
      "Epoch 72, Batch 1 Loss 0.755435\n",
      "Epoch 72, Batch 21 Loss 0.633779\n",
      "Epoch 72, Batch 41 Loss 0.559304\n",
      "Epoch 72, Batch 61 Loss 0.558778\n",
      "Epoch 72, Batch 81 Loss 0.580783\n",
      "Epoch 72, Batch 101 Loss 0.579307\n",
      "Epoch 73, Batch 1 Loss 0.584084\n",
      "Epoch 73, Batch 21 Loss 0.572532\n",
      "Epoch 73, Batch 41 Loss 0.556899\n",
      "Epoch 73, Batch 61 Loss 0.565632\n",
      "Epoch 73, Batch 81 Loss 0.566743\n",
      "Epoch 73, Batch 101 Loss 0.573457\n",
      "Epoch 74, Batch 1 Loss 0.542765\n",
      "Epoch 74, Batch 21 Loss 0.609170\n",
      "Epoch 74, Batch 41 Loss 0.583640\n",
      "Epoch 74, Batch 61 Loss 0.576697\n",
      "Epoch 74, Batch 81 Loss 0.579375\n",
      "Epoch 74, Batch 101 Loss 0.576380\n",
      "Epoch 75, Batch 1 Loss 0.487629\n",
      "Epoch 75, Batch 21 Loss 0.535214\n",
      "Epoch 75, Batch 41 Loss 0.582869\n",
      "Epoch 75, Batch 61 Loss 0.579288\n",
      "Epoch 75, Batch 81 Loss 0.577475\n",
      "Epoch 75, Batch 101 Loss 0.575768\n",
      "Epoch 76, Batch 1 Loss 0.753056\n",
      "Epoch 76, Batch 21 Loss 0.578995\n",
      "Epoch 76, Batch 41 Loss 0.581073\n",
      "Epoch 76, Batch 61 Loss 0.594162\n",
      "Epoch 76, Batch 81 Loss 0.575309\n",
      "Epoch 76, Batch 101 Loss 0.569085\n",
      "Epoch 77, Batch 1 Loss 0.435491\n",
      "Epoch 77, Batch 21 Loss 0.531483\n",
      "Epoch 77, Batch 41 Loss 0.542310\n",
      "Epoch 77, Batch 61 Loss 0.563927\n",
      "Epoch 77, Batch 81 Loss 0.560980\n",
      "Epoch 77, Batch 101 Loss 0.564598\n",
      "Epoch 78, Batch 1 Loss 0.682542\n",
      "Epoch 78, Batch 21 Loss 0.596266\n",
      "Epoch 78, Batch 41 Loss 0.605044\n",
      "Epoch 78, Batch 61 Loss 0.561478\n",
      "Epoch 78, Batch 81 Loss 0.563634\n",
      "Epoch 78, Batch 101 Loss 0.570049\n",
      "Epoch 79, Batch 1 Loss 0.499820\n",
      "Epoch 79, Batch 21 Loss 0.547145\n",
      "Epoch 79, Batch 41 Loss 0.587726\n",
      "Epoch 79, Batch 61 Loss 0.574950\n",
      "Epoch 79, Batch 81 Loss 0.578710\n",
      "Epoch 79, Batch 101 Loss 0.565768\n",
      "Epoch 80, Batch 1 Loss 0.389013\n",
      "Epoch 80, Batch 21 Loss 0.578498\n",
      "Epoch 80, Batch 41 Loss 0.564791\n",
      "Epoch 80, Batch 61 Loss 0.573261\n",
      "Epoch 80, Batch 81 Loss 0.586760\n",
      "Epoch 80, Batch 101 Loss 0.567525\n",
      "Epoch 81, Batch 1 Loss 0.292183\n",
      "Epoch 81, Batch 21 Loss 0.587189\n",
      "Epoch 81, Batch 41 Loss 0.564910\n",
      "Epoch 81, Batch 61 Loss 0.575626\n",
      "Epoch 81, Batch 81 Loss 0.574487\n",
      "Epoch 81, Batch 101 Loss 0.563896\n",
      "Epoch 82, Batch 1 Loss 0.507910\n",
      "Epoch 82, Batch 21 Loss 0.513953\n",
      "Epoch 82, Batch 41 Loss 0.544841\n",
      "Epoch 82, Batch 61 Loss 0.548713\n",
      "Epoch 82, Batch 81 Loss 0.555741\n",
      "Epoch 82, Batch 101 Loss 0.561672\n",
      "Epoch 83, Batch 1 Loss 0.499181\n",
      "Epoch 83, Batch 21 Loss 0.543748\n",
      "Epoch 83, Batch 41 Loss 0.517960\n",
      "Epoch 83, Batch 61 Loss 0.530195\n",
      "Epoch 83, Batch 81 Loss 0.548395\n",
      "Epoch 83, Batch 101 Loss 0.556661\n",
      "Epoch 84, Batch 1 Loss 0.692889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84, Batch 21 Loss 0.543963\n",
      "Epoch 84, Batch 41 Loss 0.543347\n",
      "Epoch 84, Batch 61 Loss 0.548694\n",
      "Epoch 84, Batch 81 Loss 0.559585\n",
      "Epoch 84, Batch 101 Loss 0.557273\n",
      "Epoch 85, Batch 1 Loss 0.624502\n",
      "Epoch 85, Batch 21 Loss 0.542435\n",
      "Epoch 85, Batch 41 Loss 0.557760\n",
      "Epoch 85, Batch 61 Loss 0.537415\n",
      "Epoch 85, Batch 81 Loss 0.558821\n",
      "Epoch 85, Batch 101 Loss 0.558982\n",
      "Epoch 86, Batch 1 Loss 0.488531\n",
      "Epoch 86, Batch 21 Loss 0.587742\n",
      "Epoch 86, Batch 41 Loss 0.572942\n",
      "Epoch 86, Batch 61 Loss 0.547589\n",
      "Epoch 86, Batch 81 Loss 0.539080\n",
      "Epoch 86, Batch 101 Loss 0.536455\n",
      "Epoch 87, Batch 1 Loss 0.598504\n",
      "Epoch 87, Batch 21 Loss 0.608040\n",
      "Epoch 87, Batch 41 Loss 0.569166\n",
      "Epoch 87, Batch 61 Loss 0.563444\n",
      "Epoch 87, Batch 81 Loss 0.543986\n",
      "Epoch 87, Batch 101 Loss 0.558097\n",
      "Epoch 88, Batch 1 Loss 1.104342\n",
      "Epoch 88, Batch 21 Loss 0.578391\n",
      "Epoch 88, Batch 41 Loss 0.544198\n",
      "Epoch 88, Batch 61 Loss 0.544738\n",
      "Epoch 88, Batch 81 Loss 0.548903\n",
      "Epoch 88, Batch 101 Loss 0.545020\n",
      "Epoch 89, Batch 1 Loss 0.246702\n",
      "Epoch 89, Batch 21 Loss 0.557732\n",
      "Epoch 89, Batch 41 Loss 0.561041\n",
      "Epoch 89, Batch 61 Loss 0.548877\n",
      "Epoch 89, Batch 81 Loss 0.546550\n",
      "Epoch 89, Batch 101 Loss 0.545575\n",
      "Epoch 90, Batch 1 Loss 0.704430\n",
      "Epoch 90, Batch 21 Loss 0.600323\n",
      "Epoch 90, Batch 41 Loss 0.580556\n",
      "Epoch 90, Batch 61 Loss 0.572625\n",
      "Epoch 90, Batch 81 Loss 0.575702\n",
      "Epoch 90, Batch 101 Loss 0.560721\n",
      "Epoch 91, Batch 1 Loss 0.611754\n",
      "Epoch 91, Batch 21 Loss 0.514801\n",
      "Epoch 91, Batch 41 Loss 0.529509\n",
      "Epoch 91, Batch 61 Loss 0.537864\n",
      "Epoch 91, Batch 81 Loss 0.548440\n",
      "Epoch 91, Batch 101 Loss 0.546892\n",
      "Epoch 92, Batch 1 Loss 0.574059\n",
      "Epoch 92, Batch 21 Loss 0.562151\n",
      "Epoch 92, Batch 41 Loss 0.570667\n",
      "Epoch 92, Batch 61 Loss 0.563611\n",
      "Epoch 92, Batch 81 Loss 0.554158\n",
      "Epoch 92, Batch 101 Loss 0.554276\n",
      "Epoch 93, Batch 1 Loss 0.413592\n",
      "Epoch 93, Batch 21 Loss 0.570988\n",
      "Epoch 93, Batch 41 Loss 0.593565\n",
      "Epoch 93, Batch 61 Loss 0.551756\n",
      "Epoch 93, Batch 81 Loss 0.538048\n",
      "Epoch 93, Batch 101 Loss 0.534392\n",
      "Epoch 94, Batch 1 Loss 0.573597\n",
      "Epoch 94, Batch 21 Loss 0.578936\n",
      "Epoch 94, Batch 41 Loss 0.551225\n",
      "Epoch 94, Batch 61 Loss 0.543854\n",
      "Epoch 94, Batch 81 Loss 0.548221\n",
      "Epoch 94, Batch 101 Loss 0.541449\n",
      "Epoch 95, Batch 1 Loss 0.349737\n",
      "Epoch 95, Batch 21 Loss 0.551746\n",
      "Epoch 95, Batch 41 Loss 0.594020\n",
      "Epoch 95, Batch 61 Loss 0.553960\n",
      "Epoch 95, Batch 81 Loss 0.542057\n",
      "Epoch 95, Batch 101 Loss 0.535834\n",
      "Epoch 96, Batch 1 Loss 0.474421\n",
      "Epoch 96, Batch 21 Loss 0.507977\n",
      "Epoch 96, Batch 41 Loss 0.540353\n",
      "Epoch 96, Batch 61 Loss 0.536228\n",
      "Epoch 96, Batch 81 Loss 0.526024\n",
      "Epoch 96, Batch 101 Loss 0.541000\n",
      "Epoch 97, Batch 1 Loss 0.496751\n",
      "Epoch 97, Batch 21 Loss 0.570080\n",
      "Epoch 97, Batch 41 Loss 0.555971\n",
      "Epoch 97, Batch 61 Loss 0.550401\n",
      "Epoch 97, Batch 81 Loss 0.544274\n",
      "Epoch 97, Batch 101 Loss 0.531599\n",
      "Epoch 98, Batch 1 Loss 0.719739\n",
      "Epoch 98, Batch 21 Loss 0.546498\n",
      "Epoch 98, Batch 41 Loss 0.520656\n",
      "Epoch 98, Batch 61 Loss 0.548234\n",
      "Epoch 98, Batch 81 Loss 0.563066\n",
      "Epoch 98, Batch 101 Loss 0.539109\n",
      "Epoch 99, Batch 1 Loss 0.243257\n",
      "Epoch 99, Batch 21 Loss 0.523336\n",
      "Epoch 99, Batch 41 Loss 0.575774\n",
      "Epoch 99, Batch 61 Loss 0.553744\n",
      "Epoch 99, Batch 81 Loss 0.542914\n",
      "Epoch 99, Batch 101 Loss 0.544788\n",
      "Epoch 100, Batch 1 Loss 0.614856\n",
      "Epoch 100, Batch 21 Loss 0.467376\n",
      "Epoch 100, Batch 41 Loss 0.532181\n",
      "Epoch 100, Batch 61 Loss 0.489173\n",
      "Epoch 100, Batch 81 Loss 0.524908\n",
      "Epoch 100, Batch 101 Loss 0.521982\n",
      "Epoch 101, Batch 1 Loss 0.857992\n",
      "Epoch 101, Batch 21 Loss 0.479678\n",
      "Epoch 101, Batch 41 Loss 0.469430\n",
      "Epoch 101, Batch 61 Loss 0.494974\n",
      "Epoch 101, Batch 81 Loss 0.511159\n",
      "Epoch 101, Batch 101 Loss 0.511496\n",
      "Epoch 102, Batch 1 Loss 0.369686\n",
      "Epoch 102, Batch 21 Loss 0.486466\n",
      "Epoch 102, Batch 41 Loss 0.503107\n",
      "Epoch 102, Batch 61 Loss 0.523870\n",
      "Epoch 102, Batch 81 Loss 0.534124\n",
      "Epoch 102, Batch 101 Loss 0.524000\n",
      "Epoch 103, Batch 1 Loss 0.968755\n",
      "Epoch 103, Batch 21 Loss 0.579156\n",
      "Epoch 103, Batch 41 Loss 0.547945\n",
      "Epoch 103, Batch 61 Loss 0.522485\n",
      "Epoch 103, Batch 81 Loss 0.531611\n",
      "Epoch 103, Batch 101 Loss 0.525299\n",
      "Epoch 104, Batch 1 Loss 0.308535\n",
      "Epoch 104, Batch 21 Loss 0.583012\n",
      "Epoch 104, Batch 41 Loss 0.589046\n",
      "Epoch 104, Batch 61 Loss 0.562301\n",
      "Epoch 104, Batch 81 Loss 0.548649\n",
      "Epoch 104, Batch 101 Loss 0.530298\n",
      "Epoch 105, Batch 1 Loss 0.531412\n",
      "Epoch 105, Batch 21 Loss 0.489711\n",
      "Epoch 105, Batch 41 Loss 0.504946\n",
      "Epoch 105, Batch 61 Loss 0.519423\n",
      "Epoch 105, Batch 81 Loss 0.506638\n",
      "Epoch 105, Batch 101 Loss 0.514052\n",
      "Epoch 106, Batch 1 Loss 0.651292\n",
      "Epoch 106, Batch 21 Loss 0.525905\n",
      "Epoch 106, Batch 41 Loss 0.544471\n",
      "Epoch 106, Batch 61 Loss 0.543915\n",
      "Epoch 106, Batch 81 Loss 0.516514\n",
      "Epoch 106, Batch 101 Loss 0.519831\n",
      "Epoch 107, Batch 1 Loss 0.347652\n",
      "Epoch 107, Batch 21 Loss 0.522890\n",
      "Epoch 107, Batch 41 Loss 0.515765\n",
      "Epoch 107, Batch 61 Loss 0.512538\n",
      "Epoch 107, Batch 81 Loss 0.513002\n",
      "Epoch 107, Batch 101 Loss 0.524579\n",
      "Epoch 108, Batch 1 Loss 0.456098\n",
      "Epoch 108, Batch 21 Loss 0.508240\n",
      "Epoch 108, Batch 41 Loss 0.515163\n",
      "Epoch 108, Batch 61 Loss 0.507062\n",
      "Epoch 108, Batch 81 Loss 0.503164\n",
      "Epoch 108, Batch 101 Loss 0.509011\n",
      "Epoch 109, Batch 1 Loss 0.411386\n",
      "Epoch 109, Batch 21 Loss 0.541241\n",
      "Epoch 109, Batch 41 Loss 0.514266\n",
      "Epoch 109, Batch 61 Loss 0.514985\n",
      "Epoch 109, Batch 81 Loss 0.535306\n",
      "Epoch 109, Batch 101 Loss 0.529919\n",
      "Epoch 110, Batch 1 Loss 0.389014\n",
      "Epoch 110, Batch 21 Loss 0.535659\n",
      "Epoch 110, Batch 41 Loss 0.533614\n",
      "Epoch 110, Batch 61 Loss 0.544640\n",
      "Epoch 110, Batch 81 Loss 0.531857\n",
      "Epoch 110, Batch 101 Loss 0.519195\n",
      "Epoch 111, Batch 1 Loss 0.304437\n",
      "Epoch 111, Batch 21 Loss 0.549369\n",
      "Epoch 111, Batch 41 Loss 0.549006\n",
      "Epoch 111, Batch 61 Loss 0.535586\n",
      "Epoch 111, Batch 81 Loss 0.526663\n",
      "Epoch 111, Batch 101 Loss 0.519562\n",
      "Epoch 112, Batch 1 Loss 0.298060\n",
      "Epoch 112, Batch 21 Loss 0.459704\n",
      "Epoch 112, Batch 41 Loss 0.536087\n",
      "Epoch 112, Batch 61 Loss 0.540427\n",
      "Epoch 112, Batch 81 Loss 0.534331\n",
      "Epoch 112, Batch 101 Loss 0.520130\n",
      "Epoch 113, Batch 1 Loss 0.602026\n",
      "Epoch 113, Batch 21 Loss 0.541012\n",
      "Epoch 113, Batch 41 Loss 0.499752\n",
      "Epoch 113, Batch 61 Loss 0.518638\n",
      "Epoch 113, Batch 81 Loss 0.515634\n",
      "Epoch 113, Batch 101 Loss 0.520592\n",
      "Epoch 114, Batch 1 Loss 0.950508\n",
      "Epoch 114, Batch 21 Loss 0.531461\n",
      "Epoch 114, Batch 41 Loss 0.554037\n",
      "Epoch 114, Batch 61 Loss 0.537006\n",
      "Epoch 114, Batch 81 Loss 0.519250\n",
      "Epoch 114, Batch 101 Loss 0.508365\n",
      "Epoch 115, Batch 1 Loss 0.585404\n",
      "Epoch 115, Batch 21 Loss 0.515938\n",
      "Epoch 115, Batch 41 Loss 0.520260\n",
      "Epoch 115, Batch 61 Loss 0.508314\n",
      "Epoch 115, Batch 81 Loss 0.506487\n",
      "Epoch 115, Batch 101 Loss 0.507431\n",
      "Epoch 116, Batch 1 Loss 0.332931\n",
      "Epoch 116, Batch 21 Loss 0.438277\n",
      "Epoch 116, Batch 41 Loss 0.485658\n",
      "Epoch 116, Batch 61 Loss 0.517323\n",
      "Epoch 116, Batch 81 Loss 0.518707\n",
      "Epoch 116, Batch 101 Loss 0.521487\n",
      "Epoch 117, Batch 1 Loss 0.427322\n",
      "Epoch 117, Batch 21 Loss 0.521414\n",
      "Epoch 117, Batch 41 Loss 0.497835\n",
      "Epoch 117, Batch 61 Loss 0.510410\n",
      "Epoch 117, Batch 81 Loss 0.505588\n",
      "Epoch 117, Batch 101 Loss 0.508309\n",
      "Epoch 118, Batch 1 Loss 0.419042\n",
      "Epoch 118, Batch 21 Loss 0.502922\n",
      "Epoch 118, Batch 41 Loss 0.502232\n",
      "Epoch 118, Batch 61 Loss 0.502250\n",
      "Epoch 118, Batch 81 Loss 0.508772\n",
      "Epoch 118, Batch 101 Loss 0.490455\n",
      "Epoch 119, Batch 1 Loss 0.430886\n",
      "Epoch 119, Batch 21 Loss 0.471815\n",
      "Epoch 119, Batch 41 Loss 0.555128\n",
      "Epoch 119, Batch 61 Loss 0.523611\n",
      "Epoch 119, Batch 81 Loss 0.525853\n",
      "Epoch 119, Batch 101 Loss 0.518557\n",
      "Epoch 120, Batch 1 Loss 0.359607\n",
      "Epoch 120, Batch 21 Loss 0.509853\n",
      "Epoch 120, Batch 41 Loss 0.465020\n",
      "Epoch 120, Batch 61 Loss 0.491436\n",
      "Epoch 120, Batch 81 Loss 0.507076\n",
      "Epoch 120, Batch 101 Loss 0.497781\n",
      "Epoch 121, Batch 1 Loss 0.474873\n",
      "Epoch 121, Batch 21 Loss 0.469816\n",
      "Epoch 121, Batch 41 Loss 0.535326\n",
      "Epoch 121, Batch 61 Loss 0.514220\n",
      "Epoch 121, Batch 81 Loss 0.507180\n",
      "Epoch 121, Batch 101 Loss 0.503576\n",
      "Epoch 122, Batch 1 Loss 1.177993\n",
      "Epoch 122, Batch 21 Loss 0.494760\n",
      "Epoch 122, Batch 41 Loss 0.510454\n",
      "Epoch 122, Batch 61 Loss 0.503585\n",
      "Epoch 122, Batch 81 Loss 0.505638\n",
      "Epoch 122, Batch 101 Loss 0.497987\n",
      "Epoch 123, Batch 1 Loss 0.400474\n",
      "Epoch 123, Batch 21 Loss 0.505169\n",
      "Epoch 123, Batch 41 Loss 0.526392\n",
      "Epoch 123, Batch 61 Loss 0.506314\n",
      "Epoch 123, Batch 81 Loss 0.490465\n",
      "Epoch 123, Batch 101 Loss 0.490417\n",
      "Epoch 124, Batch 1 Loss 0.462062\n",
      "Epoch 124, Batch 21 Loss 0.480047\n",
      "Epoch 124, Batch 41 Loss 0.477826\n",
      "Epoch 124, Batch 61 Loss 0.488615\n",
      "Epoch 124, Batch 81 Loss 0.494145\n",
      "Epoch 124, Batch 101 Loss 0.489564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 125, Batch 1 Loss 1.098436\n",
      "Epoch 125, Batch 21 Loss 0.571507\n",
      "Epoch 125, Batch 41 Loss 0.503677\n",
      "Epoch 125, Batch 61 Loss 0.500087\n",
      "Epoch 125, Batch 81 Loss 0.520796\n",
      "Epoch 125, Batch 101 Loss 0.511377\n",
      "Epoch 126, Batch 1 Loss 0.647299\n",
      "Epoch 126, Batch 21 Loss 0.617444\n",
      "Epoch 126, Batch 41 Loss 0.521182\n",
      "Epoch 126, Batch 61 Loss 0.510287\n",
      "Epoch 126, Batch 81 Loss 0.511193\n",
      "Epoch 126, Batch 101 Loss 0.499713\n",
      "Epoch 127, Batch 1 Loss 1.294948\n",
      "Epoch 127, Batch 21 Loss 0.487475\n",
      "Epoch 127, Batch 41 Loss 0.486822\n",
      "Epoch 127, Batch 61 Loss 0.473134\n",
      "Epoch 127, Batch 81 Loss 0.483176\n",
      "Epoch 127, Batch 101 Loss 0.482579\n",
      "Epoch 128, Batch 1 Loss 0.167551\n",
      "Epoch 128, Batch 21 Loss 0.514376\n",
      "Epoch 128, Batch 41 Loss 0.525089\n",
      "Epoch 128, Batch 61 Loss 0.552380\n",
      "Epoch 128, Batch 81 Loss 0.524001\n",
      "Epoch 128, Batch 101 Loss 0.503069\n",
      "Epoch 129, Batch 1 Loss 0.307353\n",
      "Epoch 129, Batch 21 Loss 0.495351\n",
      "Epoch 129, Batch 41 Loss 0.469860\n",
      "Epoch 129, Batch 61 Loss 0.458770\n",
      "Epoch 129, Batch 81 Loss 0.470140\n",
      "Epoch 129, Batch 101 Loss 0.488440\n",
      "Epoch 130, Batch 1 Loss 0.512315\n",
      "Epoch 130, Batch 21 Loss 0.469940\n",
      "Epoch 130, Batch 41 Loss 0.513562\n",
      "Epoch 130, Batch 61 Loss 0.481514\n",
      "Epoch 130, Batch 81 Loss 0.492385\n",
      "Epoch 130, Batch 101 Loss 0.493693\n",
      "Epoch 131, Batch 1 Loss 0.152319\n",
      "Epoch 131, Batch 21 Loss 0.427517\n",
      "Epoch 131, Batch 41 Loss 0.448129\n",
      "Epoch 131, Batch 61 Loss 0.469361\n",
      "Epoch 131, Batch 81 Loss 0.468995\n",
      "Epoch 131, Batch 101 Loss 0.479625\n",
      "Epoch 132, Batch 1 Loss 0.330693\n",
      "Epoch 132, Batch 21 Loss 0.454278\n",
      "Epoch 132, Batch 41 Loss 0.450873\n",
      "Epoch 132, Batch 61 Loss 0.483686\n",
      "Epoch 132, Batch 81 Loss 0.495243\n",
      "Epoch 132, Batch 101 Loss 0.480927\n",
      "Epoch 133, Batch 1 Loss 0.158999\n",
      "Epoch 133, Batch 21 Loss 0.450345\n",
      "Epoch 133, Batch 41 Loss 0.477762\n",
      "Epoch 133, Batch 61 Loss 0.480338\n",
      "Epoch 133, Batch 81 Loss 0.465282\n",
      "Epoch 133, Batch 101 Loss 0.480846\n",
      "Epoch 134, Batch 1 Loss 0.443031\n",
      "Epoch 134, Batch 21 Loss 0.512330\n",
      "Epoch 134, Batch 41 Loss 0.518144\n",
      "Epoch 134, Batch 61 Loss 0.498300\n",
      "Epoch 134, Batch 81 Loss 0.468189\n",
      "Epoch 134, Batch 101 Loss 0.477425\n",
      "Epoch 135, Batch 1 Loss 0.165108\n",
      "Epoch 135, Batch 21 Loss 0.459636\n",
      "Epoch 135, Batch 41 Loss 0.472582\n",
      "Epoch 135, Batch 61 Loss 0.499034\n",
      "Epoch 135, Batch 81 Loss 0.496267\n",
      "Epoch 135, Batch 101 Loss 0.487043\n",
      "Epoch 136, Batch 1 Loss 1.031017\n",
      "Epoch 136, Batch 21 Loss 0.437717\n",
      "Epoch 136, Batch 41 Loss 0.454940\n",
      "Epoch 136, Batch 61 Loss 0.486154\n",
      "Epoch 136, Batch 81 Loss 0.497943\n",
      "Epoch 136, Batch 101 Loss 0.477561\n",
      "Epoch 137, Batch 1 Loss 0.772832\n",
      "Epoch 137, Batch 21 Loss 0.514641\n",
      "Epoch 137, Batch 41 Loss 0.502862\n",
      "Epoch 137, Batch 61 Loss 0.479571\n",
      "Epoch 137, Batch 81 Loss 0.483831\n",
      "Epoch 137, Batch 101 Loss 0.466208\n",
      "Epoch 138, Batch 1 Loss 0.495403\n",
      "Epoch 138, Batch 21 Loss 0.446298\n",
      "Epoch 138, Batch 41 Loss 0.463452\n",
      "Epoch 138, Batch 61 Loss 0.477670\n",
      "Epoch 138, Batch 81 Loss 0.481947\n",
      "Epoch 138, Batch 101 Loss 0.471897\n",
      "Epoch 139, Batch 1 Loss 0.607954\n",
      "Epoch 139, Batch 21 Loss 0.490291\n",
      "Epoch 139, Batch 41 Loss 0.466897\n",
      "Epoch 139, Batch 61 Loss 0.457903\n",
      "Epoch 139, Batch 81 Loss 0.463221\n",
      "Epoch 139, Batch 101 Loss 0.472985\n",
      "Epoch 140, Batch 1 Loss 1.084631\n",
      "Epoch 140, Batch 21 Loss 0.491368\n",
      "Epoch 140, Batch 41 Loss 0.448720\n",
      "Epoch 140, Batch 61 Loss 0.471034\n",
      "Epoch 140, Batch 81 Loss 0.457955\n",
      "Epoch 140, Batch 101 Loss 0.464750\n",
      "Epoch 141, Batch 1 Loss 0.377226\n",
      "Epoch 141, Batch 21 Loss 0.420542\n",
      "Epoch 141, Batch 41 Loss 0.452422\n",
      "Epoch 141, Batch 61 Loss 0.453792\n",
      "Epoch 141, Batch 81 Loss 0.455512\n",
      "Epoch 141, Batch 101 Loss 0.459081\n",
      "Epoch 142, Batch 1 Loss 0.495283\n",
      "Epoch 142, Batch 21 Loss 0.467110\n",
      "Epoch 142, Batch 41 Loss 0.461744\n",
      "Epoch 142, Batch 61 Loss 0.467347\n",
      "Epoch 142, Batch 81 Loss 0.480984\n",
      "Epoch 142, Batch 101 Loss 0.475893\n",
      "Epoch 143, Batch 1 Loss 0.575692\n",
      "Epoch 143, Batch 21 Loss 0.483278\n",
      "Epoch 143, Batch 41 Loss 0.443007\n",
      "Epoch 143, Batch 61 Loss 0.451629\n",
      "Epoch 143, Batch 81 Loss 0.474651\n",
      "Epoch 143, Batch 101 Loss 0.468568\n",
      "Epoch 144, Batch 1 Loss 0.254953\n",
      "Epoch 144, Batch 21 Loss 0.366802\n",
      "Epoch 144, Batch 41 Loss 0.431398\n",
      "Epoch 144, Batch 61 Loss 0.421543\n",
      "Epoch 144, Batch 81 Loss 0.431460\n",
      "Epoch 144, Batch 101 Loss 0.442763\n",
      "Epoch 145, Batch 1 Loss 0.613107\n",
      "Epoch 145, Batch 21 Loss 0.392403\n",
      "Epoch 145, Batch 41 Loss 0.442893\n",
      "Epoch 145, Batch 61 Loss 0.439273\n",
      "Epoch 145, Batch 81 Loss 0.439979\n",
      "Epoch 145, Batch 101 Loss 0.455060\n",
      "Epoch 146, Batch 1 Loss 0.628028\n",
      "Epoch 146, Batch 21 Loss 0.491067\n",
      "Epoch 146, Batch 41 Loss 0.474892\n",
      "Epoch 146, Batch 61 Loss 0.495141\n",
      "Epoch 146, Batch 81 Loss 0.495805\n",
      "Epoch 146, Batch 101 Loss 0.471559\n",
      "Epoch 147, Batch 1 Loss 0.404327\n",
      "Epoch 147, Batch 21 Loss 0.445962\n",
      "Epoch 147, Batch 41 Loss 0.486374\n",
      "Epoch 147, Batch 61 Loss 0.480386\n",
      "Epoch 147, Batch 81 Loss 0.483967\n",
      "Epoch 147, Batch 101 Loss 0.459087\n",
      "Epoch 148, Batch 1 Loss 0.702003\n",
      "Epoch 148, Batch 21 Loss 0.497814\n",
      "Epoch 148, Batch 41 Loss 0.455938\n",
      "Epoch 148, Batch 61 Loss 0.444126\n",
      "Epoch 148, Batch 81 Loss 0.460809\n",
      "Epoch 148, Batch 101 Loss 0.454995\n",
      "Epoch 149, Batch 1 Loss 0.472913\n",
      "Epoch 149, Batch 21 Loss 0.442554\n",
      "Epoch 149, Batch 41 Loss 0.460661\n",
      "Epoch 149, Batch 61 Loss 0.464444\n",
      "Epoch 149, Batch 81 Loss 0.469435\n",
      "Epoch 149, Batch 101 Loss 0.465767\n",
      "Epoch 150, Batch 1 Loss 0.672257\n",
      "Epoch 150, Batch 21 Loss 0.391683\n",
      "Epoch 150, Batch 41 Loss 0.443813\n",
      "Epoch 150, Batch 61 Loss 0.442787\n",
      "Epoch 150, Batch 81 Loss 0.438067\n",
      "Epoch 150, Batch 101 Loss 0.438212\n",
      "Epoch 151, Batch 1 Loss 0.256901\n",
      "Epoch 151, Batch 21 Loss 0.460483\n",
      "Epoch 151, Batch 41 Loss 0.483494\n",
      "Epoch 151, Batch 61 Loss 0.459296\n",
      "Epoch 151, Batch 81 Loss 0.441826\n",
      "Epoch 151, Batch 101 Loss 0.443293\n",
      "Epoch 152, Batch 1 Loss 0.369631\n",
      "Epoch 152, Batch 21 Loss 0.447992\n",
      "Epoch 152, Batch 41 Loss 0.449511\n",
      "Epoch 152, Batch 61 Loss 0.458685\n",
      "Epoch 152, Batch 81 Loss 0.434947\n",
      "Epoch 152, Batch 101 Loss 0.439471\n",
      "Epoch 153, Batch 1 Loss 0.302628\n",
      "Epoch 153, Batch 21 Loss 0.459785\n",
      "Epoch 153, Batch 41 Loss 0.462210\n",
      "Epoch 153, Batch 61 Loss 0.454003\n",
      "Epoch 153, Batch 81 Loss 0.458941\n",
      "Epoch 153, Batch 101 Loss 0.447664\n",
      "Epoch 154, Batch 1 Loss 0.280696\n",
      "Epoch 154, Batch 21 Loss 0.410232\n",
      "Epoch 154, Batch 41 Loss 0.473004\n",
      "Epoch 154, Batch 61 Loss 0.451415\n",
      "Epoch 154, Batch 81 Loss 0.431091\n",
      "Epoch 154, Batch 101 Loss 0.440436\n",
      "Epoch 155, Batch 1 Loss 0.564122\n",
      "Epoch 155, Batch 21 Loss 0.498747\n",
      "Epoch 155, Batch 41 Loss 0.443485\n",
      "Epoch 155, Batch 61 Loss 0.455441\n",
      "Epoch 155, Batch 81 Loss 0.452354\n",
      "Epoch 155, Batch 101 Loss 0.454611\n",
      "Epoch 156, Batch 1 Loss 0.599064\n",
      "Epoch 156, Batch 21 Loss 0.401486\n",
      "Epoch 156, Batch 41 Loss 0.444611\n",
      "Epoch 156, Batch 61 Loss 0.442739\n",
      "Epoch 156, Batch 81 Loss 0.454428\n",
      "Epoch 156, Batch 101 Loss 0.454765\n",
      "Epoch 157, Batch 1 Loss 0.205938\n",
      "Epoch 157, Batch 21 Loss 0.333688\n",
      "Epoch 157, Batch 41 Loss 0.377248\n",
      "Epoch 157, Batch 61 Loss 0.396611\n",
      "Epoch 157, Batch 81 Loss 0.413311\n",
      "Epoch 157, Batch 101 Loss 0.436432\n",
      "Epoch 158, Batch 1 Loss 0.181247\n",
      "Epoch 158, Batch 21 Loss 0.444051\n",
      "Epoch 158, Batch 41 Loss 0.411722\n",
      "Epoch 158, Batch 61 Loss 0.423151\n",
      "Epoch 158, Batch 81 Loss 0.412271\n",
      "Epoch 158, Batch 101 Loss 0.421552\n",
      "Epoch 159, Batch 1 Loss 0.481181\n",
      "Epoch 159, Batch 21 Loss 0.431573\n",
      "Epoch 159, Batch 41 Loss 0.404621\n",
      "Epoch 159, Batch 61 Loss 0.413874\n",
      "Epoch 159, Batch 81 Loss 0.414296\n",
      "Epoch 159, Batch 101 Loss 0.425672\n",
      "Epoch 160, Batch 1 Loss 0.600783\n",
      "Epoch 160, Batch 21 Loss 0.451963\n",
      "Epoch 160, Batch 41 Loss 0.429722\n",
      "Epoch 160, Batch 61 Loss 0.426643\n",
      "Epoch 160, Batch 81 Loss 0.434513\n",
      "Epoch 160, Batch 101 Loss 0.431331\n",
      "Epoch 161, Batch 1 Loss 0.231247\n",
      "Epoch 161, Batch 21 Loss 0.464296\n",
      "Epoch 161, Batch 41 Loss 0.448340\n",
      "Epoch 161, Batch 61 Loss 0.451910\n",
      "Epoch 161, Batch 81 Loss 0.441166\n",
      "Epoch 161, Batch 101 Loss 0.435596\n",
      "Epoch 162, Batch 1 Loss 0.975959\n",
      "Epoch 162, Batch 21 Loss 0.468024\n",
      "Epoch 162, Batch 41 Loss 0.411868\n",
      "Epoch 162, Batch 61 Loss 0.425411\n",
      "Epoch 162, Batch 81 Loss 0.433860\n",
      "Epoch 162, Batch 101 Loss 0.432599\n",
      "Epoch 163, Batch 1 Loss 0.390235\n",
      "Epoch 163, Batch 21 Loss 0.498730\n",
      "Epoch 163, Batch 41 Loss 0.429574\n",
      "Epoch 163, Batch 61 Loss 0.444125\n",
      "Epoch 163, Batch 81 Loss 0.438747\n",
      "Epoch 163, Batch 101 Loss 0.436861\n",
      "Epoch 164, Batch 1 Loss 0.249750\n",
      "Epoch 164, Batch 21 Loss 0.441126\n",
      "Epoch 164, Batch 41 Loss 0.450498\n",
      "Epoch 164, Batch 61 Loss 0.459415\n",
      "Epoch 164, Batch 81 Loss 0.440387\n",
      "Epoch 164, Batch 101 Loss 0.434801\n",
      "Epoch 165, Batch 1 Loss 0.340700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 165, Batch 21 Loss 0.381031\n",
      "Epoch 165, Batch 41 Loss 0.403324\n",
      "Epoch 165, Batch 61 Loss 0.420352\n",
      "Epoch 165, Batch 81 Loss 0.408552\n",
      "Epoch 165, Batch 101 Loss 0.416907\n",
      "Epoch 166, Batch 1 Loss 0.787279\n",
      "Epoch 166, Batch 21 Loss 0.365914\n",
      "Epoch 166, Batch 41 Loss 0.383552\n",
      "Epoch 166, Batch 61 Loss 0.441046\n",
      "Epoch 166, Batch 81 Loss 0.427785\n",
      "Epoch 166, Batch 101 Loss 0.428736\n",
      "Epoch 167, Batch 1 Loss 0.348670\n",
      "Epoch 167, Batch 21 Loss 0.484251\n",
      "Epoch 167, Batch 41 Loss 0.463645\n",
      "Epoch 167, Batch 61 Loss 0.458979\n",
      "Epoch 167, Batch 81 Loss 0.451365\n",
      "Epoch 167, Batch 101 Loss 0.441387\n",
      "Epoch 168, Batch 1 Loss 0.333101\n",
      "Epoch 168, Batch 21 Loss 0.464985\n",
      "Epoch 168, Batch 41 Loss 0.430875\n",
      "Epoch 168, Batch 61 Loss 0.409984\n",
      "Epoch 168, Batch 81 Loss 0.424432\n",
      "Epoch 168, Batch 101 Loss 0.419125\n",
      "Epoch 169, Batch 1 Loss 0.118342\n",
      "Epoch 169, Batch 21 Loss 0.412518\n",
      "Epoch 169, Batch 41 Loss 0.417471\n",
      "Epoch 169, Batch 61 Loss 0.431836\n",
      "Epoch 169, Batch 81 Loss 0.407606\n",
      "Epoch 169, Batch 101 Loss 0.422759\n",
      "Epoch 170, Batch 1 Loss 0.811429\n",
      "Epoch 170, Batch 21 Loss 0.510142\n",
      "Epoch 170, Batch 41 Loss 0.441166\n",
      "Epoch 170, Batch 61 Loss 0.442707\n",
      "Epoch 170, Batch 81 Loss 0.418521\n",
      "Epoch 170, Batch 101 Loss 0.416014\n",
      "Epoch 171, Batch 1 Loss 0.475029\n",
      "Epoch 171, Batch 21 Loss 0.436885\n",
      "Epoch 171, Batch 41 Loss 0.414442\n",
      "Epoch 171, Batch 61 Loss 0.428137\n",
      "Epoch 171, Batch 81 Loss 0.411110\n",
      "Epoch 171, Batch 101 Loss 0.411860\n",
      "Epoch 172, Batch 1 Loss 0.092235\n",
      "Epoch 172, Batch 21 Loss 0.363741\n",
      "Epoch 172, Batch 41 Loss 0.357137\n",
      "Epoch 172, Batch 61 Loss 0.374926\n",
      "Epoch 172, Batch 81 Loss 0.401766\n",
      "Epoch 172, Batch 101 Loss 0.399728\n",
      "Epoch 173, Batch 1 Loss 0.241385\n",
      "Epoch 173, Batch 21 Loss 0.382878\n",
      "Epoch 173, Batch 41 Loss 0.424710\n",
      "Epoch 173, Batch 61 Loss 0.391973\n",
      "Epoch 173, Batch 81 Loss 0.388612\n",
      "Epoch 173, Batch 101 Loss 0.407988\n",
      "Epoch 174, Batch 1 Loss 0.152049\n",
      "Epoch 174, Batch 21 Loss 0.420801\n",
      "Epoch 174, Batch 41 Loss 0.434569\n",
      "Epoch 174, Batch 61 Loss 0.405490\n",
      "Epoch 174, Batch 81 Loss 0.404500\n",
      "Epoch 174, Batch 101 Loss 0.400546\n",
      "Epoch 175, Batch 1 Loss 0.185762\n",
      "Epoch 175, Batch 21 Loss 0.339214\n",
      "Epoch 175, Batch 41 Loss 0.395524\n",
      "Epoch 175, Batch 61 Loss 0.401865\n",
      "Epoch 175, Batch 81 Loss 0.404023\n",
      "Epoch 175, Batch 101 Loss 0.410010\n",
      "Epoch 176, Batch 1 Loss 0.419878\n",
      "Epoch 176, Batch 21 Loss 0.403626\n",
      "Epoch 176, Batch 41 Loss 0.372244\n",
      "Epoch 176, Batch 61 Loss 0.379847\n",
      "Epoch 176, Batch 81 Loss 0.395579\n",
      "Epoch 176, Batch 101 Loss 0.399941\n",
      "Epoch 177, Batch 1 Loss 0.184839\n",
      "Epoch 177, Batch 21 Loss 0.356081\n",
      "Epoch 177, Batch 41 Loss 0.335171\n",
      "Epoch 177, Batch 61 Loss 0.377566\n",
      "Epoch 177, Batch 81 Loss 0.375836\n",
      "Epoch 177, Batch 101 Loss 0.400733\n",
      "Epoch 178, Batch 1 Loss 0.436410\n",
      "Epoch 178, Batch 21 Loss 0.452677\n",
      "Epoch 178, Batch 41 Loss 0.431411\n",
      "Epoch 178, Batch 61 Loss 0.395908\n",
      "Epoch 178, Batch 81 Loss 0.397754\n",
      "Epoch 178, Batch 101 Loss 0.391233\n",
      "Epoch 179, Batch 1 Loss 0.149496\n",
      "Epoch 179, Batch 21 Loss 0.349283\n",
      "Epoch 179, Batch 41 Loss 0.342509\n",
      "Epoch 179, Batch 61 Loss 0.350583\n",
      "Epoch 179, Batch 81 Loss 0.379202\n",
      "Epoch 179, Batch 101 Loss 0.390189\n",
      "Epoch 180, Batch 1 Loss 0.254324\n",
      "Epoch 180, Batch 21 Loss 0.391340\n",
      "Epoch 180, Batch 41 Loss 0.408422\n",
      "Epoch 180, Batch 61 Loss 0.376044\n",
      "Epoch 180, Batch 81 Loss 0.383090\n",
      "Epoch 180, Batch 101 Loss 0.390545\n",
      "Epoch 181, Batch 1 Loss 0.322573\n",
      "Epoch 181, Batch 21 Loss 0.397677\n",
      "Epoch 181, Batch 41 Loss 0.433476\n",
      "Epoch 181, Batch 61 Loss 0.414350\n",
      "Epoch 181, Batch 81 Loss 0.394155\n",
      "Epoch 181, Batch 101 Loss 0.391385\n",
      "Epoch 182, Batch 1 Loss 0.276786\n",
      "Epoch 182, Batch 21 Loss 0.404786\n",
      "Epoch 182, Batch 41 Loss 0.435042\n",
      "Epoch 182, Batch 61 Loss 0.417064\n",
      "Epoch 182, Batch 81 Loss 0.396304\n",
      "Epoch 182, Batch 101 Loss 0.393322\n",
      "Epoch 183, Batch 1 Loss 0.251095\n",
      "Epoch 183, Batch 21 Loss 0.380441\n",
      "Epoch 183, Batch 41 Loss 0.427327\n",
      "Epoch 183, Batch 61 Loss 0.413519\n",
      "Epoch 183, Batch 81 Loss 0.403922\n",
      "Epoch 183, Batch 101 Loss 0.400564\n",
      "Epoch 184, Batch 1 Loss 0.073962\n",
      "Epoch 184, Batch 21 Loss 0.375795\n",
      "Epoch 184, Batch 41 Loss 0.377053\n",
      "Epoch 184, Batch 61 Loss 0.382901\n",
      "Epoch 184, Batch 81 Loss 0.353935\n",
      "Epoch 184, Batch 101 Loss 0.380247\n",
      "Epoch 185, Batch 1 Loss 0.232009\n",
      "Epoch 185, Batch 21 Loss 0.349800\n",
      "Epoch 185, Batch 41 Loss 0.284522\n",
      "Epoch 185, Batch 61 Loss 0.365838\n",
      "Epoch 185, Batch 81 Loss 0.402533\n",
      "Epoch 185, Batch 101 Loss 0.391844\n",
      "Epoch 186, Batch 1 Loss 0.195390\n",
      "Epoch 186, Batch 21 Loss 0.354928\n",
      "Epoch 186, Batch 41 Loss 0.328866\n",
      "Epoch 186, Batch 61 Loss 0.351178\n",
      "Epoch 186, Batch 81 Loss 0.352734\n",
      "Epoch 186, Batch 101 Loss 0.371945\n",
      "Epoch 187, Batch 1 Loss 0.196562\n",
      "Epoch 187, Batch 21 Loss 0.367660\n",
      "Epoch 187, Batch 41 Loss 0.351488\n",
      "Epoch 187, Batch 61 Loss 0.367956\n",
      "Epoch 187, Batch 81 Loss 0.366849\n",
      "Epoch 187, Batch 101 Loss 0.377200\n",
      "Epoch 188, Batch 1 Loss 0.431107\n",
      "Epoch 188, Batch 21 Loss 0.421082\n",
      "Epoch 188, Batch 41 Loss 0.420375\n",
      "Epoch 188, Batch 61 Loss 0.398868\n",
      "Epoch 188, Batch 81 Loss 0.376193\n",
      "Epoch 188, Batch 101 Loss 0.368795\n",
      "Epoch 189, Batch 1 Loss 0.231703\n",
      "Epoch 189, Batch 21 Loss 0.401478\n",
      "Epoch 189, Batch 41 Loss 0.364680\n",
      "Epoch 189, Batch 61 Loss 0.347274\n",
      "Epoch 189, Batch 81 Loss 0.380070\n",
      "Epoch 189, Batch 101 Loss 0.369260\n",
      "Epoch 190, Batch 1 Loss 0.161483\n",
      "Epoch 190, Batch 21 Loss 0.341753\n",
      "Epoch 190, Batch 41 Loss 0.328287\n",
      "Epoch 190, Batch 61 Loss 0.344395\n",
      "Epoch 190, Batch 81 Loss 0.350537\n",
      "Epoch 190, Batch 101 Loss 0.359274\n",
      "Epoch 191, Batch 1 Loss 0.218574\n",
      "Epoch 191, Batch 21 Loss 0.383007\n",
      "Epoch 191, Batch 41 Loss 0.358078\n",
      "Epoch 191, Batch 61 Loss 0.332389\n",
      "Epoch 191, Batch 81 Loss 0.332162\n",
      "Epoch 191, Batch 101 Loss 0.335236\n",
      "Epoch 192, Batch 1 Loss 0.459400\n",
      "Epoch 192, Batch 21 Loss 0.320238\n",
      "Epoch 192, Batch 41 Loss 0.333310\n",
      "Epoch 192, Batch 61 Loss 0.357608\n",
      "Epoch 192, Batch 81 Loss 0.353475\n",
      "Epoch 192, Batch 101 Loss 0.369935\n",
      "Epoch 193, Batch 1 Loss 0.208841\n",
      "Epoch 193, Batch 21 Loss 0.378883\n",
      "Epoch 193, Batch 41 Loss 0.365126\n",
      "Epoch 193, Batch 61 Loss 0.371134\n",
      "Epoch 193, Batch 81 Loss 0.356087\n",
      "Epoch 193, Batch 101 Loss 0.355156\n",
      "Epoch 194, Batch 1 Loss 0.629241\n",
      "Epoch 194, Batch 21 Loss 0.362631\n",
      "Epoch 194, Batch 41 Loss 0.348060\n",
      "Epoch 194, Batch 61 Loss 0.352309\n",
      "Epoch 194, Batch 81 Loss 0.369310\n",
      "Epoch 194, Batch 101 Loss 0.361332\n",
      "Epoch 195, Batch 1 Loss 0.184095\n",
      "Epoch 195, Batch 21 Loss 0.356224\n",
      "Epoch 195, Batch 41 Loss 0.386337\n",
      "Epoch 195, Batch 61 Loss 0.384513\n",
      "Epoch 195, Batch 81 Loss 0.376878\n",
      "Epoch 195, Batch 101 Loss 0.361771\n",
      "Epoch 196, Batch 1 Loss 0.185572\n",
      "Epoch 196, Batch 21 Loss 0.333755\n",
      "Epoch 196, Batch 41 Loss 0.372352\n",
      "Epoch 196, Batch 61 Loss 0.351376\n",
      "Epoch 196, Batch 81 Loss 0.345065\n",
      "Epoch 196, Batch 101 Loss 0.352565\n",
      "Epoch 197, Batch 1 Loss 0.201509\n",
      "Epoch 197, Batch 21 Loss 0.388387\n",
      "Epoch 197, Batch 41 Loss 0.392890\n",
      "Epoch 197, Batch 61 Loss 0.386566\n",
      "Epoch 197, Batch 81 Loss 0.356936\n",
      "Epoch 197, Batch 101 Loss 0.354025\n",
      "Epoch 198, Batch 1 Loss 0.308668\n",
      "Epoch 198, Batch 21 Loss 0.303173\n",
      "Epoch 198, Batch 41 Loss 0.342803\n",
      "Epoch 198, Batch 61 Loss 0.348195\n",
      "Epoch 198, Batch 81 Loss 0.345696\n",
      "Epoch 198, Batch 101 Loss 0.341952\n",
      "Epoch 199, Batch 1 Loss 0.230448\n",
      "Epoch 199, Batch 21 Loss 0.326153\n",
      "Epoch 199, Batch 41 Loss 0.319586\n",
      "Epoch 199, Batch 61 Loss 0.340906\n",
      "Epoch 199, Batch 81 Loss 0.326948\n",
      "Epoch 199, Batch 101 Loss 0.335609\n",
      "Epoch 200, Batch 1 Loss 0.233378\n",
      "Epoch 200, Batch 21 Loss 0.290997\n",
      "Epoch 200, Batch 41 Loss 0.295009\n",
      "Epoch 200, Batch 61 Loss 0.307607\n",
      "Epoch 200, Batch 81 Loss 0.311147\n",
      "Epoch 200, Batch 101 Loss 0.318159\n"
     ]
    }
   ],
   "source": [
    "model = train(200, trainloader, model, optimizer, criterion,\n",
    "                      use_cuda, 'model_nn1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(loader, model, criterion, use_cuda, num_classes = 2):\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "    # monitor test loss and accuracy\n",
    "    test_loss = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "    model.eval()\n",
    "    load_iter = iter(loader)\n",
    "    for i in range(len(loader)):\n",
    "        data, target = next(load_iter)\n",
    "        # move to GPU\n",
    "        if use_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        \n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # update average test loss \n",
    "        test_loss = test_loss + ((1 / (i + 1)) * (loss.data - test_loss))\n",
    "        # compare predictions to true label\n",
    "        for j, tensor in enumerate(output):\n",
    "            if (tensor.item() > .5 and target[j] == 1) or (tensor.item() <= .5 and target[j] == 0):\n",
    "                correct += 1\n",
    "        total += data.size(0)\n",
    "       \n",
    "    print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "    print('\\nTest Accuracy: %2d%% (%2d/%2d)' % (\n",
    "        100. * correct / total, correct, total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.446631\n",
      "\n",
      "\n",
      "Test Accuracy: 79% (71/89)\n"
     ]
    }
   ],
   "source": [
    "test(testloader, model, criterion, use_cuda)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
